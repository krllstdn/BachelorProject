{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [ ] To add diseases tests\n",
    "    - add to nan handling\n",
    "    - make sure that it does not destroy the dataset\n",
    "    - make sure that it is in the dataset\n",
    "    - train the best rsf with the updated dataset and compare results\n",
    "- Look for other features of interest for deceased donors (cold ischemia time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/lustre/helios-home/stadnkyr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/mnt/lustre/helios-home/stadnkyr')\n",
    "print(os.getcwd())\n",
    "\n",
    "import logging\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sksurv.column import encode_categorical\n",
    "from sksurv.column import standardize\n",
    "from sksurv.util import Surv\n",
    "\n",
    "# from columns import COLUMNS\n",
    "from surv_data_pipeline.columns import COLUMNS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sksurv.preprocessing import OneHotEncoder as SurvOneHotEncoder\n",
    "from sksurv.util import Surv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScikitSurvivalDataLoader:\n",
    "    df = None\n",
    "    target = None\n",
    "\n",
    "    ''' ideas\n",
    "     - return already smaller dataset with df = df.sample(frac=1).reset_index(drop=True) (draws sample percentage of the dataset)\n",
    "     - return another variable - validation set (or do that in code itself)\n",
    "     - add self.validation_set - without filling nan values with median\n",
    "     - try to train only on newer transplantations - today the chances of survival are much higher than it was in 80s.\n",
    "     - try to add kidney compatibility indices\n",
    "     - make defining of the features only here and not to use columns.py\n",
    "    '''\n",
    "    \n",
    "    # target_columns = [\"PTIME\", \"PSTATUS\"]\n",
    "\n",
    "    yes_categorical=['ON_DIALYSIS',\"PRE_TX_TXFUS\",  \"GENDER\", \"ETHCAT\", \n",
    "                     \"DIABETES_DON\",\n",
    "                     'DIAB',\n",
    "                    'HCV_SEROSTATUS', \n",
    "                    \"AGE_GROUP\", \"DON_TY\", \"COD_KI\", #\"CONTIN_CIG_DON\" #\"DIET_DON\",\n",
    "                    \n",
    "                ]\n",
    "    \n",
    "    yes_numerical = [\"PTIME\",\"PSTATUS\", 'AGE', \"BMI_CALC\", \n",
    "                     \"AGE_DON\", \n",
    "                     \"CREAT_TRR\",\n",
    "                     \"NPKID\",\n",
    "                    'COLD_ISCH_KI', \n",
    "                    \"CREAT_DON\",\n",
    "                    \"KDPI\",\n",
    "                    # \"GFR\",\n",
    "                    \"TX_DATE\", \"DIALYSIS_DATE\",\"KDRI_RAO\"\n",
    "                    ]\n",
    "\n",
    "    def __init__(self, patient_survival: bool = True) -> None:\n",
    "        self.target = [\"PTIME\", \"PSTATUS\"]\n",
    "\n",
    "        self.yes_numerical.insert(0,self.target[1])\n",
    "        self.yes_numerical.insert(0,self.target[0])\n",
    "\n",
    "    def load(self, n_samples_to_load=None, fill_na_with_median:bool = True) -> Tuple[pd.DataFrame, any]:\n",
    "        self._load_pd_df()\n",
    "        self._apply_exclusion_criteria()\n",
    "\n",
    "        if n_samples_to_load is not None:\n",
    "            self.df = self.df.sample(n_samples_to_load, random_state=42)\n",
    "\n",
    "        # self._divide_train_test_validation()\n",
    "\n",
    "        self.df = self._handle_nan(fill_na_with_median)\n",
    "        # self.test = self._handle_nan(fill_na_with_median, self.test)\n",
    "        # self.validate = self._handle_nan(fill_na_with_median, self.validate)\n",
    "\n",
    "        return self._get_X_y()\n",
    "    \n",
    "    def _load_pd_df(self, columns = COLUMNS):\n",
    "        logger.info(\"Loading data into pandas DataFrame...\")\n",
    "        self.df = pd.read_parquet(\n",
    "            \"/mnt/lustre/helios-home/stadnkyr/Kidney_transplants.parquet\", \n",
    "            engine='auto', columns=list(set(self.yes_categorical+self.yes_numerical)))\n",
    "\n",
    "        logger.info(f\"Done! Loaded df of shape {self.df.shape}\")\n",
    "    \n",
    "    def get_test_X_y(self):\n",
    "        return self._get_X_y(self.test)\n",
    "\n",
    "    def get_validate_X_y(self):\n",
    "        return self._get_X_y(self.validate)\n",
    "    \n",
    "    def _divide_train_test_validation(self):\n",
    "        self.test = None\n",
    "        self.validate = None\n",
    "\n",
    "        logger.info(\"Dividing the dataset into train, test and validation sets...\")\n",
    "\n",
    "        self.df, test_validation_df = train_test_split(self.df, test_size=0.3, random_state=42)\n",
    "\n",
    "        self.test, self.validate = train_test_split(test_validation_df, test_size=0.5, random_state=42)\n",
    "\n",
    "        logger.info(\"Done!\")\n",
    "\n",
    "    \n",
    "    def _apply_exclusion_criteria(self):\n",
    "        '''\n",
    "        ideas: try to censor unrelated reasons\n",
    "        '''\n",
    "        UNRELATED_COD = {998,999, 2801,2803,8065,8064,8063,8062,8050,7237, 7226, \n",
    "                         7227, 6853,5808,3899, 3800}\n",
    "        \n",
    "        self.df = self.df[self.df['PSTATUS'].notnull()]\n",
    "        self.df = self.df[self.df['AGE_GROUP'] == \"A\"]\n",
    "        self.df = self.df[self.df['DON_TY'] != \"F\"]\n",
    "        self.df = self.df[self.df['DIABETES_DON'] != \"U\"]\n",
    "        self.df = self.df[self.df['PRE_TX_TXFUS'] != \"U\"]\n",
    "        self.df = self.df[self.df['HCV_SEROSTATUS'] != \"U\"]\n",
    "        self.df = self.df[self.df['ETHCAT'] != \"998\"]\n",
    "        self.df = self.df[self.df['DIAB'] != \"998\"]\n",
    "        self.df = self.df[self.df['ETHCAT'] != \"9\"]\n",
    "\n",
    "        self.df = self.df[self.df['DON_TY'] == \"C\"] # deceased donor specification\n",
    "\n",
    "        # remove death of unrelated  reasons\n",
    "        mask = ~self.df['COD_KI'].isin(UNRELATED_COD)\n",
    "        self.df = self.df[mask]\n",
    "        self.df= self.df.drop('COD_KI', axis=1)\n",
    "\n",
    "        self.df = self.df.drop('DON_TY', axis=1)\n",
    "        self.df = self.df.drop('AGE_GROUP', axis=1)\n",
    "\n",
    "        self.yes_categorical = [x for x in self.yes_categorical if x != 'DON_TY' and x != 'AGE_GROUP']\n",
    "\n",
    "        self.df['KDPI'] = pd.to_numeric(self.df['KDPI'].str.replace('%', ''))\n",
    "\n",
    "        # self.df.dropna(subset=['DIALYSIS_DATE', 'TX_DATE'], inplace=True)\n",
    "        # self.df['DIALYSIS_TIME'] = self.df.apply(lambda row: self._get_difference_in_days(row['DIALYSIS_DATE'], row['TX_DATE']), axis=1)\n",
    "        self.df['DIALYSIS_TIME'] = self.df.apply(lambda row: \n",
    "                                                 self._get_difference_in_days(row['DIALYSIS_DATE'], row['TX_DATE']) \n",
    "                                                 if row['DIALYSIS_DATE'] is not None and row['TX_DATE'] \n",
    "                                                 is not None else 0, axis=1)\n",
    "\n",
    "        self.yes_numerical = [x for x in self.yes_numerical if x != 'DIALYSIS_DATE' and x != \"TX_DATE\"]\n",
    "        self.yes_numerical.append('DIALYSIS_TIME')\n",
    "\n",
    "        logger.info(f\"{self.df.shape}\")\n",
    "\n",
    "    def _handle_nan(self, fill_na_with_median:bool, dataset = None):\n",
    "        '''\n",
    "        evaluation set should not have values that were calculated\n",
    "        '''\n",
    "        if dataset is None:\n",
    "            dataset = self.df\n",
    "            fill_na_with_median=False\n",
    "        else:\n",
    "            fill_na_with_median=False\n",
    "\n",
    "        if fill_na_with_median:\n",
    "            logger.info(\"Handling nan values...\")\n",
    "        else:\n",
    "            logger.info(\"Dropping nan values...\")\n",
    "        \n",
    "        self.yes_categorical = [item for item in self.yes_categorical if item !=\"COD_KI\"]\n",
    "        dataset.dropna(subset=list(set(self.yes_categorical+self.yes_numerical)), inplace=True)\n",
    "\n",
    "        dataset.dropna(subset=self.yes_numerical, inplace=True)\n",
    "        \n",
    "        # print(dataset.shape)\n",
    "        logger.info(\"Done!\")\n",
    "        return dataset\n",
    "\n",
    "    def _get_X_y(self, dataset = None) -> Tuple[pd.DataFrame, any]:\n",
    "        if dataset is None:\n",
    "            dataset = self.df\n",
    "            \n",
    "        logger.info(\"Dividing data into X and y...\")\n",
    "\n",
    "        numeric_features = [x for x in self.yes_numerical if x != \"PTIME\" and x != \"PSTATUS\"]\n",
    "        categorical_features = [x for x in self.yes_categorical if x != \"PSTATUS\"]\n",
    "\n",
    "        dataset[numeric_features] = dataset[numeric_features].astype('float64')\n",
    "\n",
    "        with open('pickle/DATA_DECEASED.pkl', 'wb') as file:\n",
    "            pickle.dump(dataset, file)\n",
    "\n",
    "        # Define transformations for numeric and categorical features\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        # Combine transformations for all features\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Set up the final pipeline\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor)\n",
    "        ])\n",
    "\n",
    "        # Apply preprocessing to X\n",
    "        X = pipeline.fit_transform(dataset[numeric_features + categorical_features])\n",
    "        # print(dataset[numeric_features + categorical_features].columns)\n",
    "\n",
    "        categorical_x = encode_categorical(dataset[categorical_features])\n",
    "        numerical_x = standardize(dataset[numeric_features])\n",
    "        X = pd.concat([numerical_x, categorical_x], axis=1)\n",
    "        \n",
    "        survival_time = dataset[self.target[0]].astype(np.float64)\n",
    "        event = dataset[self.target[1]].astype(float).astype(np.bool)\n",
    "\n",
    "        y = Surv.from_arrays(event, survival_time, \"Status\", \"Days\")\n",
    "\n",
    "        self.df = None\n",
    "\n",
    "        logger.info(\"Done!\")\n",
    "\n",
    "        with open('pickle/trained_pipeline.pkl', 'wb') as file:\n",
    "            pickle.dump(pipeline, file)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def _calculate_egfr(self, creatinine, age, gender, race):\n",
    "        pass\n",
    "    \n",
    "    def _get_difference_in_days(self, date1, date2):\n",
    "        date1_dict = eval(str(date1))\n",
    "        date2_dict = eval(str(date2))\n",
    "        \n",
    "        date1 = date1_dict['$date']\n",
    "        date2 = date2_dict['$date']\n",
    "\n",
    "        if isinstance(date1, str) and (date2, str):\n",
    "            date1_object = datetime.datetime.strptime(date1, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            date2_object = datetime.datetime.strptime(date2, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "            difference = date2_object - date1_object\n",
    "            return difference.days\n",
    "        else:\n",
    "            return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-28 12:32:18,119 - Loading data into pandas DataFrame...\n",
      "2024-02-28 12:32:40,815 - Done! Loaded df of shape (993806, 23)\n",
      "2024-02-28 12:33:03,661 - (314517, 21)\n",
      "2024-02-28 12:33:03,663 - Dropping nan values...\n",
      "2024-02-28 12:33:04,262 - Done!\n",
      "2024-02-28 12:33:04,264 - Dividing data into X and y...\n",
      "2024-02-28 12:33:05,134 - Done!\n"
     ]
    }
   ],
   "source": [
    "loader = ScikitSurvivalDataLoader()\n",
    "\n",
    "X, y = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117536, 26)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[( True, 3501.) ( True, 6479.) (False, 5460.) ... ( True,  739.)\n",
      " ( True, 4973.) ( True, 1239.)]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify = y[\"event\"].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y[\"Status\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    ")\n",
    "\n",
    "def evaluate_model(model, test_X, test_y, train_y, times):\n",
    "    pred = model.predict(test_X)\n",
    "    surv_fn = model.predict_survival_function(test_X, return_array=False)\n",
    "    surv_prob = np.row_stack([fn(times) for fn in surv_fn])\n",
    "\n",
    "    uno_concordance = concordance_index_ipcw(train_y, test_y, pred, tau=times[-1])\n",
    "    ibs = integrated_brier_score(train_y, test_y, surv_prob, times)\n",
    "    auc, mean_auc = cumulative_dynamic_auc(train_y, test_y, pred, times)\n",
    "\n",
    "    print(f\"Concordance Uno: {round(uno_concordance[0], 3)}\")\n",
    "    print(f\"IBS: {round(ibs, 3)}\")\n",
    "    print(f\"Mean AUC: {round(mean_auc,3)}\")\n",
    "\n",
    "    return uno_concordance, ibs, mean_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the best model's time dependent-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "lower, upper = np.percentile(y[\"Days\"], [10, 90])\n",
    "times = np.arange(lower, upper + 1)\n",
    "\n",
    "with open('pickle/models/RSF_DECEASED_0.69.pkl', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n",
    "\n",
    "# _,_,_ = evaluate_model(best_model, X_test, y_test, y_train, times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sksurv.metrics import brier_score\n",
    "\n",
    "surv_fn = best_model.predict_survival_function(X_test, return_array=False)\n",
    "\n",
    "surv_prob = np.row_stack([fn(times) for fn in surv_fn])\n",
    "\n",
    "bs = brier_score(y_train, y_test, surv_prob, times)\n",
    "\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(bs[0], bs[1], marker=\",\")\n",
    "# # plt.axhline(ibs, linestyle=\"--\")\n",
    "# # plt.text(5, 0, \"{model}\", fontsize=12)\n",
    "# plt.title(\"Time-dependent Brier Score for the Random Survival Forest\")\n",
    "# plt.xlabel(\"days\")\n",
    "# plt.ylabel(\"time-dependent Brier Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pickle/models/RSF_DECEASED_FINAL_BRIER_SCORE.pkl', 'wb') as file:\n",
    "#     pickle.dump(bs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower, upper = np.percentile(y_train[\"Days\"], [10, 90])\n",
    "times = np.arange(lower, upper + 1)\n",
    "cph_risk_scores = best_model.predict(X_test)\n",
    "auc, mean_auc = cumulative_dynamic_auc(y_train, y_test, cph_risk_scores, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('pickle/models/AUC_RSF_DECEASED_FINAL.pickle', 'wb') as f:\n",
    "#     pickle.dump((times, auc, mean_auc), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomSurvivalForest(n_estimators=3, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsf = RandomSurvivalForest(n_estimators=3, n_jobs=-1, random_state=42)\n",
    "rsf.fit(X_train[1000:], y_train[1000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# uncoment if you want to calculate permutation importance (data must not be processed by pipeline)\n",
    "result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=0, n_jobs=1)\n",
    "# result = permutation_importance(rsf, X_test, y_test, n_repeats=10, random_state=0, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Importance\n",
      "AGE                  0.100619\n",
      "DIAB=5.0             0.018903\n",
      "DIAB=3.0             0.017945\n",
      "DIALYSIS_TIME        0.005100\n",
      "KDRI_RAO             0.004415\n",
      "DIAB=2.0             0.004288\n",
      "HCV_SEROSTATUS=P     0.003187\n",
      "ON_DIALYSIS=Y        0.002347\n",
      "CREAT_TRR            0.002001\n",
      "GENDER=M             0.000861\n",
      "ETHCAT=4             0.000860\n",
      "BMI_CALC             0.000802\n",
      "ETHCAT=5             0.000795\n",
      "AGE_DON              0.000680\n",
      "PRE_TX_TXFUS=Y       0.000590\n",
      "NPKID                0.000380\n",
      "ETHCAT=2             0.000280\n",
      "COLD_ISCH_KI         0.000219\n",
      "KDPI                 0.000021\n",
      "DIAB=4.0             0.000013\n",
      "CREAT_DON            0.000011\n",
      "DIAB=998.0           0.000011\n",
      "HCV_SEROSTATUS=ND    0.000008\n",
      "ETHCAT=6             0.000008\n",
      "ETHCAT=7            -0.000001\n",
      "DIABETES_DON=Y      -0.000023\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# columns = numeric_features + categorical_features\n",
    "\n",
    "importances_df = pd.DataFrame(result.importances_mean, index=X_train.columns)\n",
    "importances_df.columns = ['Importance']\n",
    "importances_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "# Print out feature importances\n",
    "print(importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_importances =importances_df.sort_values(by='Importance', ascending=True)\n",
    "plt_importances.plot.barh(color='blue', legend=False, title='RSF Feature Permutation Importance Living', grid=True, figsize=(8, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                     Importance\n",
    "AGE                7.603468e-02\n",
    "DIAB=5.0           1.755682e-02\n",
    "DIAB=3.0           1.636021e-02\n",
    "CREAT_TRR          6.719912e-03\n",
    "ON_DIALYSIS=Y      6.290068e-03\n",
    "AGE_DON            4.584326e-03\n",
    "KDPI               4.224856e-03\n",
    "DIALYSIS_TIME      3.907660e-03\n",
    "KDRI_RAO           3.656396e-03\n",
    "HCV_SEROSTATUS=P   3.371945e-03\n",
    "DIAB=2.0           3.240055e-03\n",
    "ETHCAT=4           2.897624e-03\n",
    "BMI_CALC           2.440851e-03\n",
    "ETHCAT=5           2.209945e-03\n",
    "PRE_TX_TXFUS=Y     1.797013e-03\n",
    "ETHCAT=2           1.152406e-03\n",
    "COLD_ISCH_KI       1.093853e-03\n",
    "NPKID              4.144752e-04\n",
    "HCV_SEROSTATUS=ND  2.775765e-04\n",
    "GENDER=M           1.363486e-04\n",
    "ETHCAT=6           6.727253e-05\n",
    "CREAT_DON          1.390463e-05\n",
    "ETHCAT=7           1.339929e-07\n",
    "DIAB=4.0          -3.417864e-05\n",
    "DIAB=998.0        -3.806376e-05\n",
    "DIABETES_DON=Y    -1.558873e-04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSF train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsf = RandomSurvivalForest(n_estimators=50, n_jobs=-1, max_depth=12, min_samples_split=16, max_features=None, random_state=42)\n",
    "rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rsf, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6908782202813217"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw\n",
    "\n",
    "lower, upper = np.percentile(y[\"Days\"], [10, 90])\n",
    "times = np.arange(lower, upper + 1)\n",
    "\n",
    "pred = rsf.predict(X_test)\n",
    "uno_concordance = concordance_index_ipcw(y_train[1000:], y_test, pred, tau=times[-1])\n",
    "uno = float(uno_concordance[0])\n",
    "uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('pickle/models/RSF_DECEASED_0.69.pkl', 'wb') as file:\n",
    "#             pickle.dump(rsf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSF fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from surv_data_pipeline.estimator_evaluation import SurvivalEstimatorEvaluation\n",
    "\n",
    "from sksurv.metrics import (\n",
    "    concordance_index_censored,\n",
    "    concordance_index_ipcw,\n",
    "    cumulative_dynamic_auc,\n",
    "    integrated_brier_score,\n",
    "    brier_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train_model(model, x, y):\n",
    "    lower, upper = np.percentile(y[\"Days\"], [10, 90])\n",
    "    times = np.arange(lower, upper + 1)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    # model.fit(X_train, y_train)\n",
    "\n",
    "    # score = model.score(X_test, y_test)\n",
    "    # print(\"Model's score: \", round(score, 3))\n",
    "\n",
    "    uno_score = SurvivalEstimatorEvaluation.evaluate_model_uno_c(model, X_test, y_test, y_train, times)\n",
    "    # print(\"Uno's score: \", round(uno_score[0], 3))\n",
    "    # auc_score, mean_auc = SurvivalEstimatorEvaluation.evaluate_model_auc(model, X_test.iloc[:5000], y_test[:5000], y, times)\n",
    "    # print(\"Mean AUC: \", round(mean_auc, 3))\n",
    "\n",
    "    # surv = model.predict_survival_function(X_test)#, return_array=False)\n",
    "    # surv_prob = np.row_stack([fn(times) for fn in surv])\n",
    "\n",
    "    # ibs = integrated_brier_score(y_train, y_test, surv_prob, times)#best_rsf.event_times_)\n",
    "    # print('Integrated Brier Score:', round(ibs,3))\n",
    "\n",
    "    return model, uno_score#, ibs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from tqdm import tqdm\n",
    "\n",
    "# n_estimators = [70, 80, 90]\n",
    "# max_depth = [5,7]\n",
    "# min_samples_split = [10, 12, 14]\n",
    "# max_features = [None]\n",
    "\n",
    "n_estimators = [100, 200]\n",
    "max_depth = [8, 12, None]\n",
    "min_samples_split = [12]\n",
    "max_features = [ None]\n",
    "\n",
    "best_params = None\n",
    "# lowest_ibs = 1\n",
    "highest_cindex = 0\n",
    "best_rsf_model = None\n",
    "\n",
    "\n",
    "\n",
    "rsf_gr = RandomSurvivalForest(n_jobs=-1)\n",
    "pbar = tqdm(total = len(n_estimators)*len(max_depth)*len(min_samples_split)\n",
    "            *len(max_features), desc='Hyperparameter Tuning')\n",
    "\n",
    "for n in n_estimators:\n",
    "    for depth in max_depth:\n",
    "        for min_split in min_samples_split:\n",
    "            for max_feat in max_features:\n",
    "                rsf_gr.set_params(n_estimators=n, \n",
    "                                  max_depth=depth, \n",
    "                                  min_samples_split=min_split, \n",
    "                                  max_features=max_feat)\n",
    "                rsf_gr, uno = train_model(rsf_gr, \n",
    "                                          X.iloc[100:], y[100:])\n",
    "                                        #   X, y)\n",
    "\n",
    "                uno = float(uno[0])\n",
    "                if uno > highest_cindex:\n",
    "                    highest_cindex = uno\n",
    "                    best_params = (n, depth, min_split, max_feat)\n",
    "                    best_rsf_model = rsf_gr\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update()\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close() \n",
    "\n",
    "print(best_params, highest_cindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rsf = RandomSurvivalForest(n_jobs=-1,\n",
    "                              n_estimators=100,\n",
    "                              max_depth=None,\n",
    "                              min_samples_split=15,\n",
    "                              max_features='sqrt',\n",
    "                               verbose=1\n",
    "                              )\n",
    "best_rsf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvivalEstimatorEvaluation.evaluate_model_uno_c(best_rsf, X_test.iloc[1000:], \n",
    "                                                 y_test[1000:], y_train, times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
