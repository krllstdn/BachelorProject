#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\begin_local_layout
Format 49

Float

Type listing

GuiName "Code listing"

Placement tbp

Extension lol

NumberWithin chapter

Style ruled

ListName "List of code listings"

LaTeXBuiltin false

End
\end_local_layout
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{August 2, 2023}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods 
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Bachelor's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author: 
\series bold
Kyrylo Stadniuk
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor: 
\series bold
Ing.
 Tomáš Kouřim
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Consultant: 
\series bold
Ing.
 Pavel Strachota, Ph.D.
\end_layout

\begin_layout Labeling
\labelwidthstring MMMMMMMMM
Language
\begin_inset space ~
\end_inset

advisor: 
\series bold
PaedDr.
 Eliška Rafajová
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2022/2023
\end_layout

\begin_layout Standard
\begin_inset External
	template PDFPages
	filename zadani_cele.pdf
	extra LaTeX "pages={1,2}"

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I am grateful to Ing.
 Tomáš Kouřim for his expert guidance and to Dr.
 Pavel Strachota for his invaluable support and insightful feedback throughout
 this project.
 I would also like to extend my sincerest appreciation to PaedDr Eliška
 Rafajová for her language assistance.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Bachelor's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
documentdate
\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset

Kyrylo Stadniuk
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Autor:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Obor:
\emph default
 Aplikovaná Informatika
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Druh práce:
\emph default
 Bakalářská práce
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Vedoucí práce:
\emph default
 Ing.
 Tomás Kourim Mild Blue, s.r.o., Plzenská 27, Praha 5
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Konzultant: 
\emph default
Ing.
 Pavel Strachota, Ph.D.
 Katedra matematiky, Fakulta jaderna a fyzikálne inzenyrska, Ceské vysoké
 udeni technické v Praze, Trojanova 13, 120 00 Praha 2
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Abstrakt:
\emph default
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Klíčová slova:
\emph default
 klíčová slova (nebo výrazy) seřazená podle abecedy a oddělená čárkou
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
Title:
\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Author:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Abstract:
\emph default
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Key words:
\emph default
 keywords in alphabetical order separated by commas
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The goal of this paper is to explore fields of kidney transplantation and
 machine learning, create and apply machine learning model in real-world
 application.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
During kidney transplantation, the donor-recipient kompatibility is maximized
 in hopes of better outcomes.
 However there is little to no instruments to estimate the outcomes.
 In this work we will try to address that.
 With trained machine learning models we created an instrument to estimate
 recipient's survival in form of survival function.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Medical Background
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Kidney — An Overview
\end_layout

\begin_layout Standard
The kidneys are reddish-brown, bean-shaped organs located on both sides
 of the spine beneath the lower ribs.
 They serve several critical functions, such as filtering blood, regulating
 blood pressure, aiding red blood cell production, and maintaining the fluid
 balance.
 Structurally, a kidney consists of about 1 million 
\emph on
nephrons
\emph default
, each consisting of a small filter called a 
\emph on
glomerulus
\emph default
, attached to a tubule, responsible for filtering waste products from the
 blood and excreting them with small amounts of fluid as urine 
\begin_inset CommandInset citation
LatexCommand cite
key "key-33"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Each kidney performs 50% of the normal kidney function.
 However, if one kidney is lost, the remaining kidney can adapt, increasing
 its capacity to as much as 75% of normal function.
 Kidney function is measured with 
\emph on
glomerular filtration rate (GFR)
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-33"
literal "false"

\end_inset

.
 There are two types of GFR: 
\emph on
measured GFR (mGFR)
\emph default
 and 
\emph on
estimated GFR (eGFR)
\emph default
.
 Measuring mGFR is a problematic and lengthy process, so healthcare practitioner
s usually use a formula to calculate eGFR.
 An eGFR of 90 and above is the normal range.
 An eGFR of 15-89 is a range for different stages of chronic kidney disease.
 Values below 15 might indicate renal failure 
\begin_inset CommandInset citation
LatexCommand cite
key "key-38"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Impairment in kidney function can have serious health consequences.
 There are many reasons why kidneys may become dysfunctional, such as end-stage
 chronic kidney disease, heavy metal poisoning, polycystic kidney disease,
 an infection, nephrotoxic drugs, lupus, physical injury, etc 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36,key-37"
literal "false"

\end_inset

.
 Among these conditions, chronic kidney disease is the most prevalent —
 about 10% of the population worldwide is affected by some form of chronic
 kidney disease 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
 
\emph on
Chronic kidney disease (CKD)
\emph default
 is an incurable condition characterized by a gradual reduction in kidney
 function over time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34,key-35"
literal "false"

\end_inset

.
 Diabetes and high blood pressure are responsible for the majority of cases
 of chronic kidney disease, with high blood sugar damaging the glomeruli
 and high blood pressure damaging the kidney blood vessels 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34,key-37"
literal "false"

\end_inset

.
 The latter interaction is associated with a negative feedback loop, where
 it is unclear whether kidney damage causes high blood pressure or vice
 versa 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
CKD progresses in 5 stages, ultimately culminating in 
\emph on
end-stage renal disease (ESRD)
\emph default
, where kidneys fail.
 Treatment options include 
\emph on
dialysis
\emph default
, a mechanical process that substitutes kidney function but cannot fully
 replicate it.
 It leads to a reduced life expectancy of, on average, 5 to 10 years.
 Alternatively, 
\emph on
kidney transplantation
\emph default
 offers a more permanent solution but requires a compatible donor, waiting
 for which can take many months or years, and the use of immunosuppressants
 after the transplantation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In conclusion, the kidneys play an essential role in maintaining overall
 health with their ability to filter waste, regulate blood pressure, and
 balance fluids.
 Kidney failure has serious health implications and there are multiple reasons
 for it.
 Chronic kidney disease, resulting from various causes such as diabetes
 and hypertension, progressively impairs kidney function, leading to end-stage
 renal disease.
 While dialysis offers temporary relief, it cannot fully substitute for
 a healthy kidney, which brings us to the crucial role of kidney transplantation.
 Kidney transplantation offers a more conclusive and sustainable solution
 for patients with ESRD, albeit accompanied by challenges related to donor
 compatibility and life-long immunosuppression.
 Both of which will be discussed in the subsequent section.
\end_layout

\begin_layout Section
Clinical Aspects of Kidney Transplantation 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
The citation at the end of a paragraph indicates that the whole paragraph
 has been taken from the source.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Kidney transplantation stands as the most commonly performed organ transplant
 worldwide.
 This prevalence is partly due to the high incidence of diseases such as
 diabetes and chronic kidney disease, which often lead to chronic renal
 failure.
 Unlike organs such as the liver and heart, kidney transplantation is relatively
 straightforward, contributing to its frequency of transplantation.
 Kidneys can be obtained from both deceased and living donors - relatives
 or altruistic volunteers.
 Living donors can donate one kidney and continue to lead normal and healthy
 lives, significantly expanding the potential donor pool 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
ABO blood group and histocompatibility (defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

) matchings for kidney transplantation are of utmost importance.
 Proper matching substantially reduces the risk of rejection and improves
 transplant outcomes.
 Compared to organs such as the liver or bone marrow, kidneys do not present
 additional challenges such as graft versus host disease (GVHD), simplifying
 the transplantation process 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
However, kidney transplantation, despite its advances, faces significant
 challenges.
 Two critical issues confront potential recipients: the scarcity of available
 organs and the risk of sensitization following a failed first transplant,
 decreasing the pool of available donors even further.
 In such cases, the immune system develops antibodies to the graft alloantigens,
 and any further transplantation of organs with similar antigens will result
 in hyperacute rejection (many of these termes are explained in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 This fact often leaves many patients unable to find a compatible donor
 after one or two rejection episodes 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Recipients of kidney transplants typically require life-long immunosuppression.
 While immunosuppressants are essential to prevent organ rejection, they
 are associated with severe side effects, such as increased risk of cancer,
 hypertension, and metabolic bone disease 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Given the high incidence of renal failure in diabetic patients - affecting
 roughly 30% of individuals with advanced diabetes - simultaneous kidney
 and pancreas transplants are sometimes performed.
 This approach addresses renal failure and underlying diabetes, offering
 a comprehensive solution for these patients 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In summary, kidney transplantation has become a vital and frequent medical
 procedure, offering improved quality of life to many suffering from kidney
 failure.
 The procedure, while simpler than other organ transplants, has its own
 set of challenges.
 The following section will delve into the history of kidney transplantation,
 showing its path from the initial experiments to the most frequently performed
 organ transplantation today.
 The historical perspective will provide insights into the advancements
 in surgical techniques, immunosuppressive therapy, and donor-recipient
 matching that have shaped the current state of kidney transplantation.
\end_layout

\begin_layout Section
The History of Kidney Transplantation 
\begin_inset CommandInset label
LatexCommand label
name "sec:history-of-kidney-transplantation"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Write intro 
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Early Animal Experiments
\end_layout

\begin_layout Standard
Advancements in surgical methods at the beginning of the 20th century eventually
 led to experiments with organ transplantation.
 One of the first recorded transplantations was an 
\emph on
autograft
\emph default
, where the donor and recipient are the same individual, performed by Emerich
 Ullmann on March 1, 1902, at the Vienna Medical School.
 Ullmann successfully connected the dog's kidney to the vessels of its neck,
 which resulted in the urine production.
 The success of this experiment was notable enough to be presented to the
 Vienna Medical Society, sparking considerable interest.
\end_layout

\begin_layout Standard
That year, other experiments followed.
 Alfred von Decastello performed a dog-to-dog kidney 
\emph on
allograft
\emph default
, a transplant between two individuals of the same species, at the Institute
 of Experimental Pathology in Vienna.
 Although initially, the transplanted kidney produced urine, it eventually
 ceased.
 Later, Ullman performed a dog-to-goat kidney 
\emph on
xenograft
\emph default
, a transplantation between individuals of different species, which resulted
 in brief function time as well.
\end_layout

\begin_layout Standard
At the same time, in Lyon, Alexis Carrel and his colleagues were working
 on vascular suturing methods.
 Carrel's technique, known as Carrel's seam, was a considerable improvement
 over existing methods, addressing the common issues of thrombosis, hemorrhage,
 stricture, and embolism 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 His consecutive move to The Rockefeller Institute for Medical Research
 in the United States led to further refinements of his method and the documenta
tion of organ rejection.
 For his contributions, Carrel received the Nobel Prize in Medicine in 1912
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
These early experiments, although varied in their outcomes, were defining
 in shaping the future of the organ transplantation.
 They not only illustrated the possibility of organ transplantation, but
 also highlighted the challenges, such as rejection, that would direct the
 course of future research.
 The insights gained in animal experiments laid the groundwork for the next
 big milestone in transplantation medicine: the advent of human organ transplant
ation.
 This transition from animals to humans marked the beginning of a new era
 in medical history.
\end_layout

\begin_layout Subsection*
Early Human Transplantation
\end_layout

\begin_layout Standard
Two first recorded human renal 
\emph on
xenografts
\emph default
 are credited to Mathieu Jaboulay in 1906.
 It involved a pig and a goat as donor animals.
 One kidney was transplanted to the arm and the second to the thigh.
 Although each kidney functioned only for an hour, these efforts marked
 the beginning of human transplantation attempts 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1,key-4"
literal "false"

\end_inset

.
 Ernst Unger's xenografts in 1909 gained more attention.
 His first attempt, involving the transplantation of a kidney from a stillborn
 baby to a baboon, resulted in a lack of kidney function despite a successful
 connection of vessels.
 Inspired by a successful surgery, Unger attempted a monkey-to-human xenograft,
 which also resulted in failure.
\end_layout

\begin_layout Standard
These experiments demonstrated the technical feasibility of kidney transplantati
on but also exposed the challenge of graft rejection.
 Alexis Carrel, in a 1914 lecture, mentioned J.
 B.
 Murphy's work on irradiation and benzol treatment, suggesting their potential
 to improve graft survival 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 Inspired by these findings, Carrel conducted his own experiments with irradiati
on, achieving prolonged graft survival.
 However, these findings were never formally published 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The period of the 1930s and 1940s was stagnant compared to the beginning
 of the century.
 European surgical centers that studied transplantology before were in decline.
 Meanwhile, the Mayo Clinic in the US was conducting some cautious experiments
 without considering Carrol's works and attempts at immunosuppression 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
However, it was during that period that a significant milestone was achieved
 by Yurii Voronyi.
 On March 3, 1933, in Kherson, Ukraine, Voronyi performed the first human
 kidney allograft on a woman suffering from acute renal failure due to mercury
 chloride poisoning.
 Given the ethical concerns surrounding living donors and previous failures
 of xenografts, Voronyi considered a cadaveric transplant the only viable
 option.
 Although there was initial urine production, the transplant failed 48 hours
 post-surgery due to blood group incompatibility and prolonged warm ischemia,
 triggering an immune reaction.
 Despite this setback, Voronyi continued to perform similar transplantations.
 He viewed these transplants as temporary measures to bridge the gap until
 the recipient's own kidneys could recover.
 Out of the six transplants he performed, two patients experienced a complete
 recovery, regaining normal kidney function 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The pioneering work of Jaboulay, Unger, and Voronyi not only demonstrated
 the technical feasibility of human organ transplantation but also highlighted
 its main challenge of graft rejection.
 Despite the progress, the field has yet to witness success, largely due
 to the lack of effective immunosuppression.
 In the next section, we will explore important moments of 1950s that transforme
d human renal transplantation from a daring experiment to feasible medical
 procedure.
\end_layout

\begin_layout Subsection*
First Successes
\end_layout

\begin_layout Standard
In 1946, at the Peter Bent Brigham Hospital in Boston, a group of surgeons:
 Hufnagel, Hume, and Landsteiner, performed kidney transplantation under
 local anesthetic on the arm vessels.
 The short period of kidney functioning may have helped the patient recover
 from acute renal failure, igniting the hospital's interest in renal transplanta
tion.
\end_layout

\begin_layout Standard
Meanwhile, European surgeons were making significant advancements.
 Notably, Simonsen in Denmark, Dempster in London, and Küss in Paris concluded
 that it is preferable to place the kidney in the pelvis.
 Furthermore, both Simonsen and Dempster deduced that the immune response
 was responsible for graft failure and hypothesized that the humoral mechanism
 of rejection was probable.
\end_layout

\begin_layout Standard
The early 1950s marked a period of active experimentation.
 In Paris, Jean Hamburger reported the first live-related kidney transplant
 between a mother and her child, achieving the immediate function of the
 transplanted kidney for 22 days until it was rejected.
 Meanwhile, in Boston, a series of nine transplantations with the thigh
 position of the allograft was closely studied.
 Moreover, in 1953, David Hume introduced the pre-transplant use of hemodialysis.
 Although some success was achieved with the administration of the adrenocortico
tropic hormone (more known as cortisone), it was deemed clinically insignificant.
 Hume's further research suggested the potential benefits of prior blood
 transfusions, blood group compatibility, and removal of host's both kidneys
 for transplant success - insights later validated with further research.
 On December 23, 1954, in Boston, Joseph Murray performed kidney allograft
 from one identical twin to another, bypassing the rejection barrier.
 From that time, many similar surgeries were performed in Boston 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1,key-3"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
To conclude, the early successes during the 1950s marked a transformative
 period in the history of kidney transplantation.
 Works of Hume, Murray, and others underlined the critical role of immune
 response in graft survival, starting the quest for effective immunosuppression,
 to which the subsequent section is dedicated.
 
\end_layout

\begin_layout Subsection*
Attempts in Immunosuppression
\end_layout

\begin_layout Standard
The exploration of immunosuppression in transplantation began as early as
 the late 1940s.
 At the Mayo Clinic in 1948, patients with rheumatoid arthritis were administere
d cortisone, an adrenal cortical hormone with mild immunosuppressive properties,
 which provided temporary relief.
 Although initially praised, its effects were deemed clinically insignificant
 for transplantation purposes.
 It led researchers to revisit earlier experiments with irradiation.
 Experiments on mice by Joan Main and Richmond Prehn showed promising results,
 inspiring human trials in Boston and Paris 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In 1958, Murray's team in Boston employed radiation in human transplantation,
 achieving a significant breakthrough with a kidney transplant between non-ident
ical twins that lasted 20 years.
 Similarly, in Paris, Jean Hamburger's team accomplished a 26-year functioning
 transplant using radiation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The quest for safer immunosuppressive methods led to the anticancer drug
 6-mercaptopurine (6-MP).
 In 1959, Schwarz and Damesehek published a paper that described how 6-MP
 lowered immune response to foreign proteins in rabbits.
 Inspired by their work, Roy Calne performed his own dog experiments, showing
 promising results backed by Charles Zukoski and David Hume.
 Despite initial setbacks, Küss and others reported prolonged graft survival
 from non-related donors using total body irradiation (TBI) complemented
 by 6-MP.
 The introduction of azathioprine in 1959, a derivative of 6-MP, by Gertrude
 Elion and George Hitchings, further improved results.
 Their groundbreaking work earned them the Nobel Prize, and by 1961, azathioprin
e was available for human use 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
In 1963, a conference held by the National Research Council revealed a bleak
 outlook for kidney transplantation, with less than 10% survival beyond
 three months.
 It changed when Tom Starzl presented a protocol combining 6-MP with prednisone,
 leading to over one-year graft survival in 70% of cases.
 His results revolutionized the field, resulting in 50 new US transplantation
 programs and setting a 20-year standard in post-transplant immunosuppression
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In summary, the late 1940s to the early 1960s was a period of discoveries
 and experimentation.
 The use of cortisone in 1948, the administration of 6-MP for immunosuppression,
 and the invention of azathioprine in 1959 led to better immunosuppression,
 culminating in Tom Starzl's protocol, which became the standard for the
 next two decades.
 The next section is dedicated to a phase of steady developments, in literature
 referred to as plateau, that would solidify the practice of kidney transplantat
ion.
\end_layout

\begin_layout Subsection*
Plateau
\end_layout

\begin_layout Standard
From 1964 to 1980, kidney transplantation saw gradual progress.
 Dialysis, developed during WWII, finally became available for chronic renal
 failure as a result of the invention of Teflon arteriovenous conduits in
 the 1960s.
 Acceptance of brain death expanded donor pools.
 Organ preservation techniques also advanced: total body hypothermia was
 replaced by targeted cold solutions infusions that preserved organs better.
 By the mid-60s, longer preservation times allowed organ exchanges between
 centers.
 Concerns about equitable distribution of organs led to the National Transplant
 Act in 1984, establishing the United Network of Organ Sharing (UNOS) for
 oversight 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
As techniques were improving and donor pools expanding in the period from
 1964 to 1980, so was the understanding of the nuances of immune compatibility.
 The following section describes the developments in tissue typing that
 would further expand the field of kidney transplantation.
\end_layout

\begin_layout Subsection*
Tissue Typing
\end_layout

\begin_layout Standard
The concept of tissue typing, suggested by Alexis Carrel in the early 20th
 century, remained unproven until Jean Dausset discovered the first human
 leukocyte antigen (HLA) in 1958 (more on HLA in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

) .
 It wasn't until 1964, with Paul Terasaki's development of the microcytotoxicity
 assay, that testing for antibodies became reliable.
 The test involved mixing the donor's lymphocytes with the recipient's serum
 and swiftly became the standard, known as the 
\emph on
crossmatch test
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
For several years, Terasaki performed typing for most U.S.
 transplant centers and found a couple of observations: 1) Positive crossmatch
 test predicts hyperacute rejection.
 2) matching can reliably identify optimal donors within a family, and it
 was assumed that the same principle would apply to non-related recipients.
\end_layout

\begin_layout Standard
However, in 1970, Terasaki's review of his extensive database of cadaver
 renal allografts revealed no correlation with the typing.
 It raised much agitation in the tissue typing community, and his grant
 was temporarily suspended until others didn't report the same.
 Since then, many crucial histocompatibility antigens have been discovered
 (Class II locus: D and DR, more on antigens in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Histocompatibility matching remains essential in bone marrow transplantation
 and in selecting family donors 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
While tissue typing and matching have played a crucial role in improving
 transplantation outcomes, the evolution of immunosuppressive drugs has
 been equally, if not more, important.
 The subsequent section tells more about new developments in immunosuppressive
 drugs in upcoming years.
\end_layout

\begin_layout Subsection*
Advancements in Immunosuppression
\end_layout

\begin_layout Standard
The discovery of cyclosporine in 1976 by Jean-François Borel marked a significan
t milestone in the realm of transplantation.
 As a fungal derivative with potent immunosuppressive properties, cyclosporine
 drastically improved the outcomes of both renal and extra-renal transplants,
 surpassing the efficacy of the previously used drug, azathioprine.
 Similar to 6-MP, to achieve the best results, it had to be combined with
 prednisone.
 This protocol remained standard until 1989, when Tacrolimus, an even more
 powerful immunosuppressive agent, was introduced.
 Tacrolimus proved to be effective in cases where the combination of cyclosporin
e and prednisone was insufficient, effectively replacing cyclosporine as
 a usual baseline agent 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In conclusion, the history of renal transplantation has been marked by groundbre
aking discoveries and persistent challenges.
 From the pioneering attempts of xenografts in the early 20th century to
 the technical advancements and immunological breakthroughs of the mid-century
 — each milestone has been essential in shaping the current landscape of
 organ transplantation As we explored the history of kidney transplantation,
 a deeper understanding of the immune system has become crucial.
 This sets the stage for the next section, which examines the fundamentals
 of immunology, laying the foundation for understanding the interplay between
 the immune system and the transplanted organ.
 .
\begin_inset Note Note
status open

\begin_layout Plain Layout
don't like the paragraph
\end_layout

\end_inset


\end_layout

\begin_layout Section
The Introduction to Immunology
\begin_inset CommandInset label
LatexCommand label
name "sec:Intro-to-Immunology"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
add that cells are generated in thymus and in bone marrow
\end_layout

\begin_layout Itemize
define MHC (major histocompatibility complex)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
immune system
\emph default
 is a sophisticated defense mechanism that evolved to protect multicellular
 organisms from pathogens such as bacteria, fungi, viruses, and parasites.
 It is a network of many cells and tissues that compose a complex system
 that detects, evaluates, and responds to the invader.
 It is essential to understand these mechanisms, as the immune response
 plays a crucial role in graft acceptance or rejection 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Innate and adaptive immunity are the two interconnected systems of immune
 response.
 
\emph on
Innate immunity (
\emph default
also
\emph on
 natural 
\emph default
or
\emph on
 native)
\emph default
 includes primitive built-in cellular and molecular mechanisms that provide
 rapid, albeit non-specific responses to common pathogens.
 In contrast, 
\emph on
adaptive (specific 
\emph default
or
\emph on
 acquired) immunity
\emph default
 is slower to respond but capable of providing more targeted responses 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Physical barriers, including epithelia and mucous membranes, constitute
 the host's first line of defense and are a part of innate immunity.
 This branch of the immune system not only prevents invaders from infiltrating
 the host but also quickly destroys many microbes that succeed in breaching
 these barriers.
 Innate immunity provides the necessary protection until adaptive immunity
 is activated.
 It also communicates to the adaptive immunity how to best respond to the
 invader.
 Furthermore, innate immunity plays an important role in the clearance of
 dead tissue and the initiation of repair after the tissue is damaged 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Adaptive immunity is broadly categorized into 
\emph on
humoral
\emph default
 and 
\emph on
cell-mediated immunity
\emph default
.
 Central to both humoral and cell-mediated immune responses are 
\emph on
lymphocytes
\emph default
, also known as 
\emph on
white blood cells
\emph default
.
 
\emph on
B lymphocytes
\emph default
 (
\emph on
B cells)
\emph default
 mediate humoral response by producing antigen-specific antibodies on encounter
 with the antigen.
 Produced antibodies then bind themselves to 
\emph on
antigens
\emph default
 — foreign molecular structures identified by common molecular patterns
 known as 
\emph on
pathogen-associated molecular patterns
\emph default
 (
\emph on
PAMPs
\emph default
) — to mark them for destruction.
 The immune system uses 
\emph on
pathogen recognition receptors
\emph default
 (
\emph on
PRRs
\emph default
), found on the surface of T cells, in conjunction with antibodies to detect
 and categorize these
\emph on
 
\emph default
PAMPs, which can take the form of molecules on the surface of a pathogen
 or its by-products
\begin_inset Note Note
status open

\begin_layout Plain Layout
not ideal sentence
\end_layout

\end_inset

.
 PRRs bind to PAMPs and initiate a targeted cascade of events that culminate
 in the pathogen's elimination 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
T lymphocytes, on the other hand, when encountering an antigen, start to
 proliferate, forming an army of T cells that will eliminate the invader
 and will form long-term memory about the pathogen.
 Activated T cells are divided into the following categories: helper T cells
 (
\begin_inset Formula $CD4^{+}$
\end_inset

), that help B cells to produce antibodies and help kill ingested microbes;
 
\emph on
cytotoxic T cells
\emph default
 (
\begin_inset Formula $CD8^{+}$
\end_inset

) that target and kill infected cells; 
\emph on
regulatory T lymphocytes
\emph default
 that prevent or limit immune responses; and 
\emph on
memory cells
\emph default
, that remain in the body long-term to provide faster and stronger immune
 response if the same antigen is encountered in the future 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Pathogen-host interaction is a continuous arms race, as pathogens usually
 have a short life cycle and can modify their DNA to elude the host's recognitio
n systems.
 The immune system counters this with the generation of host-tolerant lymphocyte
s with diverse PRRs during the development in bone marrow.
 Cells that react to the host's own cells are eliminated, ensuring that
 only non-self-reactive cells are allowed in circulation.
 The principle of recognizing self vs.
 non-self is called tolerance 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
MHC can be explained here
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In conclusion, the immune system is a complex network of molecules, cells,
 tissues, and organs that cooperate in protecting the organism from pathogens.
 The system can be divided into two main branches: innate and adaptive,
 which cooperate in protecting the host from infections while developing
 long-term immunity to specific pathogens.
 Understanding the mechanisms of the immune system is essential to understanding
 the domain of kidney transplantation.
\end_layout

\begin_layout Section
Immunology of Kidney Transplant 
\begin_inset CommandInset label
LatexCommand label
name "sec:Immunology-of-kidney"

\end_inset


\end_layout

\begin_layout Standard
The degree to which the immune system responds to a graft depends on the
 type of graft.
 There are four types of grafts:
\end_layout

\begin_layout Itemize

\series bold
Autograft 
\series default
is body tissue transfer from one body site to the other in the same individual.
\end_layout

\begin_layout Itemize

\series bold
Isograft
\series default
 is a tissue transplanted between two genetically identical individuals.
 In humans, it is usually homozygotic (identical) twins.
\end_layout

\begin_layout Itemize

\series bold
Allograft
\series default
 is a tissue transplanted between two genetically non-identical individuals
 of the same species.
\end_layout

\begin_layout Itemize

\series bold
Xenograft
\series default
 is a tissue transplanted between individuals of different species.
\end_layout

\begin_layout Standard
Autografts and isografts are generally accepted because they are genetically
 identical.
 Allografts, being genetically different, are typically identified as foreign
 by the immune system and as a result rejected.
 Xenografts, which have the most significant genetic differences, are the
 most vigorously rejected by the immune system 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Immune response to the graft
\end_layout

\begin_layout Standard
There are three reasons why the immune system may react to the allograft:
 damage due to ischemia, reaction to the incompatible blood group antigens,
 and reaction to major histocompatibility complex (MHC) and minor histocompatibi
lity (miH) antigens.
\end_layout

\begin_layout Paragraph
Ischemia Reperfusion Injury
\end_layout

\begin_layout Standard
The transplantation process inevitably includes termination of blood flow
 and, as a result, oxygenation.
 This interruption in blood supply inhibits the cell's ability to generate
 sufficient energy to maintain homeostasis, leading to cellular damage or
 death.
 If this happens, the kidney is said to experience 
\emph on
ischemia-reperfusion injury (IRI)
\emph default
.
 IRI is a significant factor in the success of kidney transplantation.
 When the blood flow to the ischemic kidney is restored, dead cells trigger
 an innate immune response.
 It is associated with the release of 
\emph on
danger-associated molecular patterns (DAMP) 
\emph default
from the compromised cells
\emph on
,
\emph default
 serving as a critical alert mechanism.
 These DAMPs are recognized by both innate and adaptive immunity, although
 the former is predominately activated.
 Such an immune response might damage the graft even more and contribute
 to acute rejection 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 Because of that, the duration of ischemia is crucial in predicting the
 graft (and patient) survival.
\end_layout

\begin_layout Paragraph
Blood Group Compatibility
\end_layout

\begin_layout Standard
ABO blood group antigens, expressed by most cell types, play a crucial role
 in the compatibility of organ transplants.
 If the transplantation between incompatible pairs were performed, it would
 result in a severe and often immediate reaction known as hyperacute antibody-me
diated rejection (AMR or ABMR).
 The rejection risk is so high that the ABO blood group compatibility is
 the first thing checked before performing the transplant.
\end_layout

\begin_layout Standard
There are four primary blood groups: A, B, O, and AB.
 Within this system, individuals with blood group O are so-called ”universal
 donors” – organs from them can be transplanted to recipients with any ABO
 blood group.
 Whereas recipients in the AB group can safely receive organs from recipients
 with any ABO blood group and are called ”universal recipients”.
 In clinical practice, however, particularly with deceased donors, the preferenc
e is to match organ recipients with ABO-identical organs to prevent potential
 inequities in organ allocation and access.
 In the case of living donors, the principle of ABO-identity is somewhat
 more flexible, where organs from ABO-compatible donors are considered acceptabl
e for transplantation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Histocompatibility
\end_layout

\begin_layout Standard
Genetically similar tissues, known as 
\emph on
histocompatible
\emph default
, are less likely to trigger an immune response post-transplant, as the
 immune system does not recognize the transplanted tissue as foreign.
 Conversely, 
\emph on
histoincompatible
\emph default
 tissues, characterized by genetic differences, typically trigger an immune
 response.
 Although more than 40 distinct loci encode the antigens responsible for
 histocompatibility, the most vigorous allograft-rejection responses are
 attributed to loci within the 
\emph on
major histocompatibility complex (MHC)
\emph default
.
 The organization of MHC in humans is called 
\emph on
human leukocyte antigen (HLA)
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Histocompatibility antigens, genetically encoded antigens that cover cell
 surfaces, play a crucial role in recognizing self versus non-self.
 In all vertebrates, histocompatibility antigens are categorized into a
 single 
\emph on
major histocompatibility complex (MHC) 
\emph default
and
\emph on
 
\emph default
numerous
\emph on
 minor histocompatibility (miH)
\emph default
 systems.
 Mismatches in either MHC or miH result in an immune reaction, most severe
 in the case of MHC.
 Rejection in MHC-compatible donor-recipient pair is usually delayed, in
 some cases forever.
 However, the miH mismatch might be so drastic that it would be comparable
 to the MHC mismatch 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The MHC itself is divided into class I and class II antigens.
 MHC class I antigens cover the surfaces of most cells and can activate
 cytotoxic CD8 cells.
 MHC class II antigens, found on specific immune cells, play an essential
 role in immune response coordination 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 In humans, each MHC class is further divided into three subgroups, as illustrat
ed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:MHC-class-division"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MHC class division
\begin_inset CommandInset label
LatexCommand label
name "tab:MHC-class-division"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DR
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DP
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DQ
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is important to note that class I and II mismatches differ in their impact
 on graft survival.
 A mismatch of one or two class I antigens has little effect on graft survival,
 whereas a single mismatch in class II antigen is equivalent to a mismatch
 of 3 or 4 class I antigens.
 When there are mismatches in both class I and II antigens, the risk and
 severity of the rejection are notably increased 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph*
HLA Typing and Matching
\end_layout

\begin_layout Standard
In clinical practice, clinicians asses and try to match donors and recipients
 according to the number of HLA-A, -B, and -DR mismatches, ranging from
 zero mismatches (0-0-0) to a maximum of 6 mismatches (2-2-2).
 Generally, more emphasis is placed on DR loci due to the capability of
 CD4 T cell activation, which might trigger both humoral and cellular adaptive
 immune responses 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
HLA typing of potential donors and recipient is conducted with a 
\emph on
microcytotoxicity test
\emph default
.
 The subject's white blood cells are distributed into various wells on a
 microtiter plate, after which specific antibodies for various class I and
 class II MHC alleles are added to different wells.
 Following incubation, complement is introduced to the wells, and cytotoxicity
 is assessed by the cells' absorption of various dyes.
 If a cell has an MHC allele for which a particular antibody is specific,
 then the cell will die upon the addition of a complement, and these dead
 cells will take up a dye 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Even when a fully HLA-compatible donor is not available, the transplantation
 still can be successful.
 In this situation, a one-way mixed-lymphocyte reaction (MLR) test can be
 used to assess the degree of class II MHC compatibility.
 The irradiated (or treated with mitomycin C) potential donor's lymphocytes
 play the role of stimulator, and the unaltered recipient's lymphocytes
 take a responder role.
 Then, the proliferation of recipient cells is measured.
 The intense recipient leukocyte proliferation indicates a poor prognosis
 for graft survival.
 The MLR gives a better understanding of the degree of CD4 cell activation.
 However, it takes several days to run the assay, often making it less suitable
 in the case of cadaver transplantation compared to a quicker microcytotoxicity
 test, which takes only a couple of hours.
 Notably, even perfect HLA matching does not guarantee rejection-free transplant
ation.
 Minor histocompatibility loci mismatches can still lead to graft rejection,
 necessitating at least some level of immunosuppression even in HLA-identical
 pairings.
 It is particularly critical in kidney and bone marrow transplants, where
 HLA matching is vital, whereas heart and liver transplantation can withstand
 higher levels of mismatching 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Paragraph
Mechanisms of Graft Rejection
\end_layout

\begin_layout Standard
Graft rejection is mainly due to cell-mediated immune response to alloantigens,
 predominantly MHC antigens, present on the graft cells.
 The process of cell-mediated graft rejection is divided into two phases.
 The first is known as the sensitization phase, which involves the recognition
 of MHC and miH alloantigens by helper CD4 and killer CD8 cells, leading
 to their proliferation.
 The second stage, the effector stage, is where the actual destruction of
 the graft occurs 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
A range of effector mechanisms are involved in graft rejection.
 The most prevalent are cell-mediated reactions involving delayed-type hypersens
itivity and cytotoxic T lymphocyte-mediated cytotoxicity.
 Less common mechanisms are antibody-plus-complement lysis (cell disintegration)
 and destruction by antibody-dependent cell-mediated cytotoxicity (ADCC)
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Clinical Manifestations of Graft Rejection
\end_layout

\begin_layout Standard
The onset and severity of graft rejection varies depending on the organ
 transplanted and the underlying immune response mechanisms involved.
 Rejection can be categorized into three types: hyperacute, acute, and chronic
 rejections.
\end_layout

\begin_layout Standard

\emph on
Hyperacute rejections
\emph default
 occur within the first 24 hours post-transplant.
 This type of rejection occurs due to the prior sensitization to graft antigens,
 with specific antibodies already present in the bloodstream.
 It may happen due to several reasons: recipients of repeated blood transfusions
 sometimes develop significant amounts of antibodies to MHC antigens present
 on white blood cells of the transfused blood; women, through repeated pregnanci
es, can become sensitized to paternal alloantigens of the fetus; individuals
 with previous grafts may have antibodies against alloantigens of that graft;
 and naturally occurring antibodies to blood group antigens are always present,
 although pre-transplant blood group matching has made rejections of this
 nature rare 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Acute rejections
\emph default
 typically happen within the first few weeks post-transplant.
 This type of rejection is primarily mediated by adaptive immunity via T
 cell responses 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Chronic rejections
\emph default
, on the other hand, happen from months to years after the transplantation
 and are mediated by both humoral and cell-mediated mechanisms.
 While the advancements in immunosuppression and tissue typing significantly
 improve short-term survival, little progress has been made in terms of
 long-term survival.
 Chronic rejections are hard to manage with immunosuppressants and may necessita
te another transplantation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Immunosuppression
\end_layout

\begin_layout Standard
Most of the available immunosuppression methods, while essential in preventing
 graft rejections, have the disadvantage of being non-specific, meaning
 they suppress immune reactions to all antigens, not only ones of the graft,
 placing the recipient at the risk of infection and, in some cases, more
 severe complications.
 For instance, some drugs target the speed of proliferation of activated
 lymphocytes.
 In doing so, they slow down the proliferation of all cells in the body.
 It can be particularly problematic for rapidly dividing cells, such as
 those of gut epithelia or bone marrow hematopoietic stem cells, placing
 patients at risk of severe or even life-threatening complications.
 Moreover, long-term use of immunosuppressive agents puts patients at risk
 of cancer, hypertension, and metabolic bone disease.
 In this section, we will delve into commonly used immunosuppressive methods
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
A class of drugs known as 
\emph on
mitotic inhibitors
\emph default
 is used to inhibit rapid cell division across all cells.
 These are administered just before and after the transplantation to stop
 T-cell proliferation.
 The most commonly used mitotic inhibitors include 
\emph on
Azathioprine
\emph default
, 
\emph on
Cyclophosphamide
\emph default
, and 
\emph on
Methotrexate
\emph default
.
 However, their broad action on cell division can lead to significant adverse
 effects 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Corticosteroids
\emph default
, another class of drugs, are often used in conjunction with mitotic inhibitors
 to prevent acute rejection.
 They have anti-inflammatory properties and act on different levels of immune
 response.
 Commonly used corticosteroids are 
\emph on
prednisone
\emph default
 and 
\emph on
dexamethasone
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Fungal metabolites, such as 
\emph on
Cyclosporine A (CsA)
\emph default
, 
\emph on
Tacrolimus
\emph default
, and 
\emph on
rapamycin
\emph default
 are also widely used due to their potent immunosuppressive properties in
 heart, liver, kidney, and bone marrow transplants.
 A substantial drawback is their nephrotoxicity.
 While CsA has been widely used, Tacrolimus and rapamycin, being much more
 potent, might be administered at much lower doses, minimizing the side
 effects 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Radiation, known for its effectiveness in destroying lymphocytes, is another
 lymphocyte elimination method employed just before transplantation.
 In one such procedure, lymph nodes, spleen, and thymus are irradiated.
 Later, newer, more tolerant to the graft alloantigens lymphocytes will
 emerge, as the bone marrow was unaffected 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Ideal immunosuppression would be antigen-specific, targeting only alloantigens
 of the graft, preserving the recipient's ability to respond to infections.
 Such specific immunosuppression has not been achieved in humans yet, but
 animal models suggest its feasibility.
\end_layout

\begin_layout Subsection
Transplant Tolerance
\end_layout

\begin_layout Standard
Taking into account the detrimental effect of long-term immunosuppression,
 one of the primary objectives in transplantation is the induction of immunologi
c non-responsiveness (tolerance) to an allograft.
 There are several pathways of immune non-responsiveness generation described
 in the literature.
 However, it has not gone further than animal models yet 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 Tolerance is usually induced by prior exposure to donor antigens in a way
 that causes immune tolerance rather than sensitization in the recipient
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
write some conclusion to the chapter
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Machine Learning Background
\end_layout

\begin_layout Standard
Machine learning is a subfield of computer science that consists of building
 algorithms capable of processing large amounts of data, finding patterns,
 and performing actions such as predictions or generating new data.
 It is an intersection of many fields of science, such as statistics, theory
 of probability, linear algebra, calculus, and certainly, computer science.
 
\end_layout

\begin_layout Standard
Machine learning excels in problems that are either overly complex or have
 no known algorithm.
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

 It can help us generate knowledge.
 We can extract previously unknown correlations from the data and build
 knowledge.
 It might make fewer errors in decision-making than humans.
\end_layout

\begin_layout Standard
Based on the problem and, therefore, on our approach to building a dataset
 and the model, machine learning can be divided into four subfields: 
\emph on
supervised
\emph default
, 
\emph on
semi-supervised
\emph default
, 
\emph on
unsupervised
\emph default
, and 
\emph on
reinforcement learning
\emph default
.
 
\emph on
Supervised learning
\emph default
 means that data is labeled, and we want to predict labels for the unlabeled
 data.
 The term labeled data is explained in the following section dedicated to
 supervised learning.
 Unsupervised learning deals with unlabeled data.
\end_layout

\begin_layout Standard

\emph on
Semi-supervised
\emph default
 learning deals with partially labeled data, and we need to label it fully
 either manually, or using techniques such as 
\emph on
clustering
\emph default
.
 
\end_layout

\begin_layout Standard
In 
\emph on
reinforcement learning
\emph default
, we create an environment, set up rewards for performing certain actions
 and punishment for others, and let the machine (actor) perform actions
 that produce the highest reward.
\end_layout

\begin_layout Standard
Every field is affected by human errors, and medicine is no exception.
 Machine learning also makes mistakes, but if we manage to get at least
 1% fewer errors than humans make, this will be a substantial achievement.The
 human body is a complex system, where it is very difficult to comprehend
\series bold
 
\series default
all processes and and how they  relate to each other.
 In addition, machine learning can help us gain insight into them through
 accumulated data and discover new relations between them.
\end_layout

\begin_layout Standard
In this chapter, we will cover all theoretical backgrounds that might prove
 useful for solving our problem, including classical machine learning, statistic
al survival analysis, basic steps that are required to 
\series bold
create machine learning system
\series default
s, and data preprocessing.
 We will begin by exploring supervised learning.
\end_layout

\begin_layout Section
Supervised Learning
\end_layout

\begin_layout Standard
Supervised learning is the process of training a model on data where the
 outcome is known, to make predictions for data where the outcome is not
 known
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"
literal "false"

\end_inset

.
 
\emph on
Classification
\emph default
 and 
\emph on
regression
\emph default
 are common supervised learning tasks.
 In this section we will define these problems and the necessary terminology,
 and describe commonly used algorithms that are used to solve these types
 of problems.
\end_layout

\begin_layout Standard
In supervised learning the 
\emph on
dataset
\emph default
 is the collection of labeled examples 
\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, where each individual 
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is called a 
\emph on
feature vector
\emph default

\begin_inset Note Note
status open

\begin_layout Plain Layout
y
\end_layout

\end_inset

.
 A feature vector is a vector that in each its dimension 
\begin_inset Formula $j=1,...,D$
\end_inset

 contains a value that describes an example in some way.
 This value is called a 
\emph on
feature
\emph default
 and is denoted as 
\begin_inset Formula $x^{(j)}$
\end_inset

.
 The 
\emph on
label
\emph default
 
\begin_inset Formula $y^{i}$
\end_inset

 might be either a finite set of classes 
\begin_inset Formula $\{1,2,...,C\}$
\end_inset

, in case of a classification task, or a real number, a vector, a matrix
 or graph, in case of a regression.
 The goal of supervised learning algorithm is to create a model using the
 dataset that will take the feature vector as an input and produce a label
 or a more complex structure as an output.
\end_layout

\begin_layout Standard
Classification is a problem of assigning a label to an unlabeled example.
 This problem is solved by a classification learning algorithm that takes
 a labeled set of examples as input and produces a model that takes an unlabeled
 example as input and outputs a label.
 If the set of labels has only two classes we talk about 
\emph on
binary classification
\emph default
.
 Consequently, if the set of labels has three or more classes, it is a 
\emph on
multiclass classification
\emph default
.
 Some algorithms are binary classifiers by definition while others are multiclas
s classifiers.
 It is possible to create an 
\emph on
ensemble
\emph default
 out of binary classifiers that will be able to perform multiclass classificatio
n.
 An ensemble is a combination of algorithms that are connected to perform
 one task.
\end_layout

\begin_layout Standard
Regression is a problem of predicting a 
\emph on
target value
\emph default
 given an unlabeled example.
\begin_inset Note Note
status open

\begin_layout Plain Layout
given->from
\end_layout

\end_inset

 The problem is solved by a regression learning algorithm that takes a set
 of labeled examples as input, and produces a model that takes an unlabeled
 example as input and outputs a target value.
 
\end_layout

\begin_layout Standard
Classification and regression tasks are similar in many ways and often for
 each classifier there is an equivalent regressor, and vice versa.
 In the following subsections we are going to explore some techniques for
 supervised learning.
\end_layout

\begin_layout Subsection
Linear Regression
\end_layout

\begin_layout Standard
Linear regression is a popular regression learning algorithm.
 The model produced is a linear combination of all features.
\end_layout

\begin_layout Standard
The problem formulation we are trying to solve is as follows:
\series bold
 
\series default
Given a collection of labeled examples
\series bold
 
\series default

\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, create a model 
\begin_inset Formula 
\begin{equation}
f_{\bar{w},b}(\overline{x})=\overline{w}\overline{x}+b,\label{eq:lin_reg}
\end{equation}

\end_inset

where N is the size of the collection, 
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is a 
\emph on
feature vector 
\emph default
of D dimensions of example 
\begin_inset Formula $i=1,…,N$
\end_inset

, every feature 
\begin_inset Formula $x_{i}^{(J)}\epsilon\mathbb{R},$
\end_inset

 
\begin_inset Formula $y_{i}\epsilon\mathbb{R}$
\end_inset

 is the target value.
 
\begin_inset Formula $\overline{w}$
\end_inset

 is a D-dimensional vector of parameters and 
\begin_inset Formula $b\epsilon\mathbb{R}$
\end_inset

.
 Notation 
\begin_inset Formula $f_{\bar{w},b}(\overline{x})$
\end_inset

 means that 
\begin_inset Formula $f$
\end_inset

 is parametrized by 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
To train the linear regression means to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 In graphical terms, it means finding such a hyperplane that fits data points
 from the training set as well as possible, as shown in image
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linear-regression-for"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/linear_regression.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Linear regression for two-dimensional data
\begin_inset CommandInset label
LatexCommand label
name "fig:Linear-regression-for"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
To find optimal parameters we need to minimize the following expression:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{1}{N}\mathop{\underset{i=1...N}{\sum}(}f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}.\label{eq:mse}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
It is called 
\emph on
mean squared error (MSE), 
\emph default
the 
\emph on
loss function 
\emph default
that comprises of 
\emph on
squared error loss 
\emph default

\begin_inset Formula $(f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}$
\end_inset

, another loss function
\emph on
 
\emph default
that
\emph on
 
\emph default
evaluates individual predictions
\emph on
.
 
\emph default
The loss function measures the model's overall performance (MSE) or evaluates
 each prediction (square error loss).
 
\end_layout

\begin_layout Standard
There is a 
\emph on
closed-form solution
\emph default
 for finding optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

.
 A closed-form solution is a simple algebraic expression that gives the
 result directly.
 In case of linear regression, it is the 
\emph on
normal equation
\emph default
, and it looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{\overline{\mathbf{w}}^{*}=(x^{T}x)x^{T}y}.\label{eq:norm_eq}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where 
\begin_inset Formula $x^{T}$
\end_inset

means transposed feature matrix 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
We could select another loss function, but according to Andriy Burkov, it
 would be a different algorithm.
 For example, we could take the absolute difference between 
\begin_inset Formula $f(x_{i})$
\end_inset

 and 
\begin_inset Formula $y_{i}$
\end_inset

 but that would create problems as the derivative of absolute value is not
 continuous.
 Therefore the function is not smooth, which might create unnecessary complicati
ons during the optimization process.
 
\end_layout

\begin_layout Standard
Linear models are usually resilient to overfitting because they are simple.
 The model overfits when it learns the intricacies of the training dataset
 so well that it remembers actual values instead of learning the underlying
 pattern.
 Such model is unable to make accurate predictions when confronted with
 unseen data.
 More on overfitting in section 3.6.
\end_layout

\begin_layout Subsection
Logistic Regression
\end_layout

\begin_layout Standard

\emph on
Logistic regression 
\emph default
is a binary classifier that estimates the probability of an example belonging
 to a particular class.
 If the predicted probability of the instance belonging to a class is greater
 than 50%, then the model concludes that it belongs to the class (referred
 to as positive class and labeled as 1).
 Otherwise, it predicts that the example does not belong to that class (but
 belongs to the negative class, labeled 0).
 Logistic regression comes from statistics where its mathematical formulation
 is similar to a regression, hence the name.
 Multiclass classification is available in softmax regression, a multiclass
 variant of logistic regression.
 
\end_layout

\begin_layout Standard
As with linear regression, in logistic regression, we want to model 
\begin_inset Formula $y$
\end_inset

 as a linear combination of 
\begin_inset Formula $\overline{x}$
\end_inset

, but in this case, it is not that straightforward.
 
\end_layout

\begin_layout Standard
The logistic regression model looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f_{\bar{w},b}(\overline{x})\stackrel{def}{=}\frac{1}{1+e^{-(wx+b)}}.\label{eq:log_reg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Similar to linear regression, our task is to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
Once we found 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

, in other words, we trained the model, we can apply the model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

 on features 
\begin_inset Formula $x_{i}$
\end_inset

 from an example 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

.
 The output value lies in the range 
\begin_inset Formula $0<p<1$
\end_inset

 .
 If 
\begin_inset Formula $y_{i}$
\end_inset

 is the positive class, the likelihood of 
\begin_inset Formula $y_{i}$
\end_inset

 being a positive class is given by 
\begin_inset Formula $p$
\end_inset

.
 Consequently, if 
\begin_inset Formula $y_{i}$
\end_inset

 is the negative class, the likelihood for it being the negative class is
 given by 
\begin_inset Formula $1-p$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/logistic_regression.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic function
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the figure we can see that if the 
\begin_inset Formula $y$
\end_inset

 has a value lower than 
\begin_inset Formula $\frac{1}{2}$
\end_inset

, it has negative 
\begin_inset Formula $x$
\end_inset

 values and will be marked as a negative class.
 If 
\begin_inset Formula $y$
\end_inset

 is greater than 
\begin_inset Formula $\frac{1}{2}$
\end_inset

, it is positive.
 Although, depending on the context, the threshold may be different.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
ocitovat cely odstavec.
 Dohledat jak
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In logistic regression, instead of 
\emph on
minimizing
\emph default
 MSE we are trying to 
\emph on
maximize
\emph default
 the 
\emph on
likelihood function
\emph default
.
 In statistics, the likelihood function tells how likely the example is
 according to our model.
 The objective function in logistic regression is called 
\emph on
maximum likelihood
\emph default
.
 It looks like the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L_{\bar{w},b}\stackrel{def}{=}\underset{i=1...N}{\prod}f_{\bar{w},b}(\overline{x}_{i})^{y_{i}}(1-f_{\bar{w},b}(\overline{x}_{i}))^{(1-y_{i})}.\label{eq:max_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
On the other hand, due to the exponential function in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:max_likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it is better to use the 
\emph on
log-likelihood
\emph default
 instead, to make calculations easier.
 As 
\begin_inset Formula $Log$
\end_inset

 is a strictly increasing function, maximizing it is the same as maximizing
 its argument.
 The solution to this optimization problem is the same as the solution to
 the original problem.
 The log-likelihood function looks like the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
LogL_{\bar{w},b}\stackrel{def}{=}ln(L_{\overline{w},b}(\overline{x}))\stackrel[i=1]{N}{\sum}y_{i}lnf_{\bar{w},b}(\overline{x})+(1-y_{i})ln(1-f_{\bar{w},b}(\overline{x})).\label{eq:log_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Unfortunately, there is no closed-form solution for this optimization problem.
 Nonetheless, the function is convex, hence gradient descent (or any other
 optimization algorithm) pretty much guarantees the finding of the global
 minimum, provided that the learning rate is not too large and enough time
 is given.
 
\end_layout

\begin_layout Subsection
Support Vector Machines
\end_layout

\begin_layout Standard

\emph on
Support vector machine (SVM)
\emph default
 is a widely-used and powerful machine learning algorithm that can perform
 a wide range of tasks, including linear and nonlinear classification, regressio
n, and outlier detection on small- to medium-sized datasets.
\end_layout

\begin_layout Subsubsection*

\series bold
Linear SVM
\end_layout

\begin_layout Standard
In its classical formulation, the support vector machine is a binary classifier.
 Classes are called positive and negative and are labeled +1 and -1, respectivel
y.
 
\end_layout

\begin_layout Standard
The model is described by the equation 
\begin_inset Formula 
\[
f(x)=sign(\overline{w}\overline{x}-b).
\]

\end_inset


\end_layout

\begin_layout Standard
The function 
\begin_inset Formula $sign$
\end_inset

 returns +1 if the input is positive, and -1 if it is negative.
 To train the SVM means to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 The process of finding 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 is called training.
\end_layout

\begin_layout Standard
The concept behind support vector machines is demonstrated in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVM-demonstration-for"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The image consists of two classes represented by red and blue dots, divided
 by a solid line termed the 
\emph on
decision boundary
\emph default
 
\begin_inset Formula $\overline{w}\overline{x}-b=0$
\end_inset

, with two dashed lines by its sides known as 
\emph on
support vectors
\emph default
 
\begin_inset Formula $\overline{w}\overline{x}-b=1$
\end_inset

 and 
\begin_inset Formula $\overline{w}\overline{x}-b=-1$
\end_inset

.
 Support vectors are defined by the closest instances of a class to the
 decision boundary.
 These instances are emphasized in the figure.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/svm1.jpeg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
SVM demonstration for two-dimensional dataset
\begin_inset CommandInset label
LatexCommand label
name "fig:SVM-demonstration-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The distance between the closest instances of two classes is called 
\emph on
margin 
\emph default
and is equal to 
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, where 
\begin_inset Formula $||\overline{w}||$
\end_inset

 is the Euclidean norm and 
\begin_inset Formula $\overline{w}$
\end_inset

 is a parameter vector of the same dimensionality as the feature vector.
 Thus, the smaller the norm, the larger the margin.
 The larger the margin, the better the model's generalization.
 The primary objective of the model is to find the largest possible margin
 
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, so, to do that we need to 
\emph on
minimize 
\emph default
the Euclidian norm defined by the expression
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||\overline{w}||=\sqrt{\stackrel[j=1]{D}{\sum}(w^{(j)})^{2}}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
The fundamental assumption of support vector machines is that classes are
 linearly separable, implying their instances can be separated by a hyperplane
 (decision boundary) with no examples of one class lying among the ones
 of the opposite class.
 It is illustrated in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linearly-non-separable"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In this case, the algorithm won't be able to find an optimal solution with
 no instances lying between the support vectors and the decision boundary.
 Consequently, the model is highly sensitive to outliers.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/svm2.jpeg
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Linearly non separable dataset
\begin_inset CommandInset label
LatexCommand label
name "fig:Linearly-non-separable"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Every optimization problem requires constraints, and for the support vector
 machine, they are the following:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{w}\overline{x_{i}}-b\geqslant+1$
\end_inset

 if 
\begin_inset Formula $y_{i}=+1$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{w}\overline{x_{i}}-b\leqslant-1$
\end_inset

 if 
\begin_inset Formula $y_{i}=-1$
\end_inset

.
\end_layout

\begin_layout Standard
These two equations can be reduced to one 
\begin_inset Formula $y_{i}(\overline{w}\overline{x}-b)\geqslant1$
\end_inset

.
\end_layout

\begin_layout Standard
The optimization problem we want to solve is the following: Minimize 
\begin_inset Formula $||\overline{w}||$
\end_inset

 subject to constraint 
\begin_inset Formula $y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1$
\end_inset

 for 
\begin_inset Formula $i=1,...,N$
\end_inset

, where N is the number of features.
 This problem can be modified so that the quadratic programming techniques
 could be used in the optimization process.
 The modified formula is 
\begin_inset Formula $\frac{1}{2}||\overline{w}||^{2}$
\end_inset

, and minimization of it would also mean minimization of 
\begin_inset Formula $||\overline{w}||$
\end_inset

.
 The updated optimization problem looks like this:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
min\frac{1}{2}||\overline{w}||^{2}\text{such that }y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1,i=1,...,N\label{eq:svm_optimisation}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection*

\series bold
Handling Noise
\end_layout

\begin_layout Standard
To introduce the ability of SVM to handle nonlinearly separable data (but
 not to the extreme), we define the hinge loss function: 
\begin_inset Formula $max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)).$
\end_inset

 It is zero if the constraints 1 and 2 are satisfied.
 If it is not, the data point does not lie on the right side of the decision
 boundary.
 The function value is proportional to the distance from the decision boundary.
 The resulting cost function looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C||\overline{w}||^{2}+\frac{1}{N}\stackrel[j=1]{N}{\sum}max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)),\label{eq:svm_loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where C is the hyperparameter that determines the trade-off between increasing
 the size of the decision boundary and ensuring that each 
\begin_inset Formula $x_{i}$
\end_inset

 lies on the correct side of the decision boundary.
 Its value is chosen experimentally.
 C handles the trade-off between classifying the training data well and
 classifying future examples well (generalization).
 For higher values of C, the misclassification error will be almost negligible,
 so the algorithm will try to find the highest margin without considering
 it.
 For lower values of C, the algorithm will try to make fewer mistakes by
 sacrificing the margin size.
 (A larger margin is better for the generalization.) Lower values lead to
 wider streets and more margin violations, higher values lead to narrower
 streets and fewer margin violations.
\end_layout

\begin_layout Standard
SVM with the hinge loss function is called 
\emph on
soft-margin SVM
\emph default
 while the original formulation that optimizes the Euclidian norm is referred
 to as 
\emph on
hard-margin SVM
\emph default
.
 
\emph on
Soft margin classification
\emph default
 tries to mitigate the downsides of the 
\emph on
hard margin classification
\emph default
 by trying to find a balance between keeping the margin as large as possible
 and mitigating the margin outliers (instances that lie on the margin or
 on the opposite side).
\end_layout

\begin_layout Subsubsection*

\series bold
Handling Non-linearity
\series default
 
\end_layout

\begin_layout Standard
We can adapt SVM to work with nonlinearly separable datasets by applying
 the kernel trick.
 The kernel trick means transforming the original space to a higher dimensional
 one during the cost function optimization with the hope that, in higher
 dimensional space, it will become linearly separable.
 In mathematical language: the kernel trick is mapping 
\begin_inset Formula $\varphi:\overline{x}\rightarrow\varphi(\overline{x})$
\end_inset

, where 
\begin_inset Formula $\varphi(\overline{x})$
\end_inset

 is a vector of higher dimensionality than 
\begin_inset Formula $\overline{x}.$
\end_inset

 The kernel trick allows us to save a lot of non-necessary computations.
\end_layout

\begin_layout Standard
There are multiple kernel functions.
 The most widely used are linear, polynomial, radial basis function (RBF),
\end_layout

\begin_layout Section
Unsupervised Learning
\end_layout

\begin_layout Standard

\emph on
Unsupervised learning 
\emph default
deals with a dataset that does not have labels.
 There are three main branches of unsupervised learning: clustering, dimensional
ity reduction and anomaly detection.
 
\emph on
Clustering
\emph default
 is a method that identifies similar instances and groups them into sets.
 It has applications in data analysis, namely, 
\emph on
exploratory data analysis (EDA),
\emph default
 customer segmentation, dimensionality reduction, and anomaly detection.
 Clustering might be either soft, where an instance has a score of belonging
 to a particular cluster, or hard, where an instance belongs to only one
 class.
 The score might be the distance from the cluster centroid or an affinity
 (similarity score).
 
\end_layout

\begin_layout Standard

\emph on
Dimensionality reduction
\emph default
 is useful for visualization and for the acceleration of learning.
 Datasets often have a lot of redundant data or the task requires a lot
 of features.
 Many algorithms, such as linear models, SVMs, decision trees, might have
 their performances compromised due to high-dimensional data.
 So called 
\emph on
curse of dimensionality
\emph default
 states that high dimensional data can cause slow learning and prevent us
 from getting an optimal model.
 Consequently, the reduction of the data dimensionality might be a good
 idea.
 However, it is worth noting that a dimensionality reduction algorithm might
 lose some useful information.
 A lot of modern algorithms, such as neural networks or ensemble algorithms,
 handle high dimensional data very well, and dimensionality reduction techniques
 are used less than in the past.
 However, they are still used for data visualization and cases when we need
 to build an interpretable model while we are limited in theb number of
 algorithms we can use.
\end_layout

\begin_layout Standard

\emph on
Anomaly (outlier) detection
\emph default
 involves the detection of instances strongly deviating from the norm.
 These instances are called 
\emph on
outliers
\emph default
 or anomalies while regular ones are referred to as 
\emph on
inliers
\emph default
.
 Anomaly detection has many applications.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout

\emph on
exploratory data analysis (EDA)
\end_layout

\end_inset

 For example, it can be used as a data preprocessing step to remove outliers
 from the dataset, which might improve the performance of the resulting
 model.
 In addition, it is used in the 
\emph on
fraud detection
\emph default
 task and the detection of faulty products in manufacturing facilities.
\end_layout

\begin_layout Standard

\emph on
Novelty detection 
\emph default
is closely related to anomaly detection.
 The only difference is that novelty detection assumes that the training
 dataset was not contaminated by outliers while anomaly detection does not
 make this assumption.
\end_layout

\begin_layout Subsection
Principal Component Analysis (PCA)
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
napsat uvod co to je a jak a k cemu to je
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Principal components 
\emph default
are vectors that define a new coordinate system.
 The first vector goes in the direction of the highest variance.
 The second vector is orthogonal to the first one and goes in the direction
 of the second highest variance, and so on.
 If we were to reduce dimensionality to 
\begin_inset Formula $D_{new}<D$
\end_inset

, we would pick 
\begin_inset Formula $D_{new}$
\end_inset

 largest principal components and 
\emph on
project
\emph default
 instances onto them.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
(Create images) 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is not advised to choose the number of dimensions arbitrarily.
 It is recommended to choose a number of dimensions that preserves a large
 amount of variance (e.g.
 95%), or in case of visualization to reduce the number of dimensions down
 to two or three.
 There are different versions of PCA; kernel PCA, Incremental PCA (online
 or batch PCA), and Randomized PCA.
 
\end_layout

\begin_layout Subsection
Gaussian Mixtures 
\begin_inset Note Note
status open

\begin_layout Plain Layout
premyslet nad smazanim nebo predelat
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Gaussian mixtures
\emph default
 is a common algorithm that can be used for anomaly detection.
 Gaussian mixtures assume that the dataset is generated by several Gaussian
 distributions.
 Any instance lying in a region of low density is an anomaly.
 The density threshold has to be specified.
 If one gets too many false positives (good products labeled as faulty)
 they need to decrease the threshold.
 Consequently, if we get too many false negatives (faulty products labeled
 as good) the threshold has to be increased.
 Gaussian mixtures belong to soft clustering.
 Gaussian mixtures require the number of clusters to be specified.
 It needs to be run a couple of times to avoid suboptimal solutions.
\end_layout

\begin_layout Section
Data Preparation
\end_layout

\begin_layout Standard
Due to factors such as curse of dimensionality and inherent noise, we cannot
 load raw data to an algorithm and expect good performance.
 Most often, the raw data has too many features and most of them have very
 little predictive power.
 We need to build a dataset first.
 
\emph on
Feature engineering
\emph default
 is responsible for transforming raw data into a dataset.
 It is a labor-demanding process that requires creativity and, most importantly,
 domain knowledge.
 
\end_layout

\begin_layout Standard
The objective of this stage is to create 
\emph on
informative
\emph default
 features or features with 
\emph on
high predictive power
\emph default
.
 For example, in our task of predicting survival time, donor-recipient blood
 group compatibility or recipient's age is likely to have much higher predictive
 power than the donor's or recipient's citizenship.
 
\series bold

\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
vic to rozvest.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Moreover, it is possible to create new features with higher predictive power
 out of those with low predictive power.
 For example, the calculation of 
\emph on
estimated Glomerular Filtration Rate (eGFR)
\emph default
, the metric of kidney function estimated on a patient's age, gender, and
 serum creatinine level, could potentially give more information to the
 learning algorithm than all features separately.
 
\end_layout

\begin_layout Standard
In the following subsections, we will cover some popular feature engineering
 techniques.
\end_layout

\begin_layout Subsection

\series bold
Handling Categorical Features
\end_layout

\begin_layout Standard
The majority of machine learning algorithms primarily operate with numerical
 features.
 To handle categorical features (the ones with only a few possible values),
 such as the age group or a blood group, we can use 
\emph on
one-hot encoding
\emph default
 to convert them to several binary ones.
 For instance, let's consider a blood group feature comprised of four primary
 blood groups: A, B, AB, and O.
 We can convert each blood group into a vector of four numerical values:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{align*}   
\end_layout

\begin_layout Plain Layout

A&=[1,0,0,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

B&=[0,1,0,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

AB&=[0,0,1,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

O&=[0,0,0,1]
\end_layout

\begin_layout Plain Layout


\backslash
end{align*}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
This technique will increase the dimensionality of the dataset but this
 is a trade-off we have to make because if we were to assign a number to
 each group (1 to A, 2 to B, etc.), that would imply gradation or ranking
 among these categories, while there is none.
 
\end_layout

\begin_layout Standard
However, if the categorical feature does suggest some gradation, for example,
 university marks as ”fail”, ”average”, ”good”, or ”excellent”, an enumeration
 of each value will be appropriate.
 This practice of assigning a number to categories that have ranking is
 called 
\emph on
ordinal encoding
\emph default
.
 
\end_layout

\begin_layout Standard

\emph on
Binning
\emph default
 (or 
\emph on
bucketing
\emph default
) is the technique used for converting numerical values into multiple binary
 features called 
\emph on
bins
\emph default
 or 
\emph on
buckets
\emph default
.
 For example, a patient's age can be transformed into age-range bins: 0
 to 18 years old, 18 to 25 y.o., 25 to 40 years old, and so on.
 This technique might help an a laearning algorithm learn better, particularly
 with smaller datasets.
\end_layout

\begin_layout Subsection

\series bold
Feature Scaling
\end_layout

\begin_layout Standard
Different ranges of feature values might pose a problem to some machine
 learning algorithms
\series bold
 
\series default
as they do not handle them very well.
 It might result in a slower training time or a poorer performance.
 This problem is solved by 
\emph on
normalization
\emph default
 and 
\emph on
standardization
\emph default
 scaling techniques.
\end_layout

\begin_layout Standard

\emph on
Normalization
\emph default
 (also known as 
\emph on
min-max scaling
\emph default
) is a technique of converting an actual range of numerical feature values
 into a standard range of values: 
\begin_inset Formula $[-1,1]$
\end_inset

 
\series bold

\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
pridat vzorec pro '-1
\end_layout

\end_inset


\series default
 or 
\begin_inset Formula $[0,1]$
\end_inset

 without losing any information.
 The normalization formula for value 
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature 
\begin_inset Formula $j$
\end_inset

, looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{x}^{(j)}=\frac{x^{(j)}-min(j)}{max(j)-min(j)},
\]

\end_inset

where 
\begin_inset Formula $min(j)$
\end_inset

 and 
\begin_inset Formula $max(j)$
\end_inset

 are minimal and maximal values of feature 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Standardization
\emph default
 is a scaling technique that scales numerical data in such a way that after
 scaling, it has properties of the 
\emph on
standard normal distribution
\emph default
 with the mean µ=0 (average value) and the standard deviation from the mean
 
\begin_inset Formula $\sigma=1$
\end_inset

.
 The standardization formula for value 
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature 
\begin_inset Formula $j$
\end_inset

, looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{x}^{(j)}=\frac{x^{(j)}-\mu^{(j)}}{\sigma^{(j)}}.
\]

\end_inset


\end_layout

\begin_layout Standard
Typically, standardization is used for supervised learning,
\begin_inset Note Note
status open

\begin_layout Plain Layout
premyslet nad carkou
\end_layout

\end_inset

 in case feature values are formed by standard distribution (bell curve)
 or a feature has outliers.
 In other cases, the normalization is preferred.
\end_layout

\begin_layout Subsection

\series bold
Handling Missing Feature Values
\end_layout

\begin_layout Standard
Datasets frequently have missing values and to handle them, we have one
 of the following options:
\end_layout

\begin_layout Enumerate

\series bold
Removal of rows with missing values
\series default
.
 The most direct and straightforward approach to managing missing data.
 If missing values are sparse or the dataset is large enough, the usage
 of this technique would be appropriate.
\end_layout

\begin_layout Enumerate

\series bold
Feature removal
\series default
.
 If the dataset has a feature with an excessive amount of missing values
 relative to its size, it is better to remove the feature.
\end_layout

\begin_layout Enumerate

\series bold
Regression imputation
\series default
.
 This technique implies the filling in a missing feature value with predictions
 of a machine learning regression algorithm.
\end_layout

\begin_layout Enumerate

\series bold
Mean/median imputation
\series default
.
 This method involves the filling of missing feature values with their mean
 or median value.
\end_layout

\begin_layout Enumerate

\series bold
Constant value imputation
\series default
.
 This technique entails the filling the missing values with clearly too
 high or too low values.
 The motivation is for the algorithm to discern the value as an outlier
 while considering other features.
 This method is not recommended as it can introduce bias.
\end_layout

\begin_layout Standard
It is often impossible to tell which data imputation method would work the
 best and therefore, it should be checked experimentally.
\end_layout

\begin_layout Section
Model Training and Hyperparameter Tuning
\begin_inset CommandInset label
LatexCommand label
name "sec:Model-Training-and-Hyperparameter-tuning"

\end_inset


\end_layout

\begin_layout Standard
It is a common practice to divide a dataset into three parts
\end_layout

\begin_layout Itemize
Training set (70% of the dataset)
\end_layout

\begin_layout Itemize
Validation set (15% of the dataset)
\end_layout

\begin_layout Itemize
Test set (15% of the dataset)
\end_layout

\begin_layout Standard
The training set, being the largest of them, is employed to train the machine
 learning model.
 Validation and test sets, which are of identical sizes and often called
 hold-out sets, are used in subsequent stages of model evaluation.
\end_layout

\begin_layout Standard
The rationale behind the use of separate training and validation sets is
 to prevent overfitting - a situation when the model performs well on the
 training data but poorly on the unseen data.
 Overfitting can occur if the model is tested and evaluated on the same
 dataset.
 As a result, the model may memorize the training examples and fail to make
 accurate predictions on the unseen data.
 To alleviate this, we use the validation set to fine-tune the model, and
 the test set to assess its performance before deploying it to production.
\end_layout

\begin_layout Standard
A typical workflow involves training the model on the training set, validation
 on the validation set using the selected metric, then adjusting the model's
 parameters to improve its performance.
 This process is repeated until no substantial improvement is observed.
 Finally, the model's performance is assessed on the test set.
 This iterative process is referred to as hyperparameter tuning.
\end_layout

\begin_layout Standard
An alternative to the three-set technique is 
\emph on
k-fold cross-validation
\emph default
.
 This technique involves splitting the dataset into k subsets, or folds,
 of equal size.
 One fold is used as a validation set, while the other k-1 folds constitute
 a training set.
 The model is trained exactly k times, with each fold serving as a validation
 set only once.
 The only drawback is that it is highly computationally demanding, particularly
 with a high k value and larger datasets, as the model will be trained k
 times.
\end_layout

\begin_layout Standard
A 
\emph on
hyperparameter
\emph default
 is a parameter specified before model training, in contrast to regular
 parameters that are calculated during training.
 Each model possesses a different set of hyperparameters and they profoundly
 influence the model's performance.
 The number of trees in Random Forest and the C hyperparameter in Support
 Vector Machines are examples of hyperparameters.
 The task of finding the optimal combination of hyperparameters is called
 hyperparameter tuning.
 One strategy might be to select hyperparameters manually and observe their
 impact on the performance.
 However, utilizing the grid search is a better way.
\end_layout

\begin_layout Standard

\emph on
Grid search
\emph default
 is a standard way of performing hyperparameter fine-tuning.
 It includes defining hyperparameters to experiment with, providing values
 for each hyperparameter to be tested, and training a model for each possible
 combination of hyperparameters.
 The performance of each individual model is assessed using k-fold cross-validat
ion and the best combination of hyperparameters is selected.
 This approach is used in sci-kit-learn's implementation - GridSearchCV.
\end_layout

\begin_layout Standard
Grid search proves to be effective when dealing with relatively few hyperparamet
er combinations.
 However, with larger number of hyperparameter combinations, it is advisable
 to use RandomizedSearch (RandomizedSearchCV in sci-kit-learn).
 This method is very similar to grid search but instead of trying every
 possible combination of provided values, it tests only a specified number
 of randomly selected hyperparameter combinations.
 The primary advantage of this method over grid search lies in more control
 over computational power and the time dedicated to hyperparameter tuning.
\end_layout

\begin_layout Section
Survival Analysis
\end_layout

\begin_layout Standard

\emph on
Survival analysis, 
\emph default
often referred to as
\emph on
 time-to-event analysis, 
\emph default
is a statistical technique employed to analyze and predict the time until
 an event of interest occurs.
 Its name originates from clinical and biological research where these methods
 are used to analyze survival time.
 These methods, however, found their use in areas far beyond clinical settings:
 in business to predict the time until the customer ”churns” from a subscription
; in engineering to estimate the product longevity or the longevity of its
 parts; in social sciences to estimate the longevity of a marriage; or to
 estimate a student dropout rate in an academic setting.
\end_layout

\begin_layout Standard
In the context of survival analysis, 
\emph on
time
\emph default
 (also 
\emph on
survival time
\emph default
) refers to the duration from the start of an individual's follow-up to
 the occurrence of an event.
 It is measured in days, weeks, months, or years 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
 The term 
\emph on
event
\emph default
 (also referred to as 
\emph on
deat
\emph default
h or 
\emph on
failure
\emph default
) encompasses any occurrence that permanently changes the state of the subject.
 It can be death, the onset of a disease, a relapse from remission, a recovery,
 or any other specified experience of interest that an individual might
 encounter 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13,key-30"
literal "false"

\end_inset

.
 Usually, only one event of interest is considered.
 When evaluating multiple events, the problem is categorized as 
\emph on
competing risks
\emph default
 or 
\emph on
recurrent events
\emph default
 problem 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In this section, we will delve into fundamental survival analysis terminology,
 such as censoring and survival function.
 We will explore the classification of the survival analysis methods, highlighti
ng the most popular statistical methods, as well as one machine learning
 method tailored to the needs of survival analysis.
 Additionally, we will discuss ways to evaluate survival models effectively.
\end_layout

\begin_layout Subsection
Basic Terminology
\end_layout

\begin_layout Standard
In this subsection, we are going to cover the basic terminology required
 for survival analysis such as censoring, censoring assumptions, survival,
 and hazard functions.
 
\end_layout

\begin_layout Subsubsection*
Censoring
\end_layout

\begin_layout Standard
The most distinct feature of survival analysis methods is the ability to
 handle censored data.
 
\emph on
Censoring
\emph default
 refers to a circumstance when the information about survival time is only
 partially known.
 For example, the dataset utilized in our research has 370,000 censored
 instances out of 500,000 performed transplantations.
 These patients were either still alive at the last date of observation
 or were lost to follow-up.
 This lack of complete information indicates 
\emph on
censoring
\emph default
 in survival analysis.
 Censoring makes the application of standard statistical and machine learning
 approaches to survival data impractical 
\begin_inset CommandInset citation
LatexCommand cite
key "key-38"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/censoring_ill.jpg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Censoring illustration
\begin_inset CommandInset label
LatexCommand label
name "fig:Censoring-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Look at the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Censoring-illustration"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 On the y-axis, we can see individual patients, while the x-axis corresponds
 to the study timeline (the right side is the end of the study).
 Cross (X) denotes an occurrence of the event, and circle (O) corresponds
 to the patient's exit from the study.
\end_layout

\begin_layout Standard
There are three types of censoring: 
\emph on
left
\emph default
, 
\emph on
right
\emph default
, and 
\emph on
interval
\emph default
 censoring.
 
\emph on
Right censoring
\emph default
, which is more common, occurs when we are sure that the event did not happen
 by a specific time and we don't know when it will happen.
 The situation arises when the patient drops out of a study, or the study
 ends when they are still alive, as illustrated with patients B, E, and
 F in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Censoring-illustration"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\emph on
Left censoring
\emph default
 is less common and happens when the event occurs before the study begins
 or before the initial observation.
 We know the event happened before a specific time, but the exact time is
 unknown.
 This type is typical in cases where a patient has already experienced the
 event (e.g., developed a disease) before enrolling in the study.
\end_layout

\begin_layout Standard

\emph on
Interval censoring
\emph default
 happens when the event occurs within a particular timeframe, but the exact
 time is unknown.
 It can be the case in studies involving periodic patient follow-up, where
 the event can happen at any point between two visits.
\end_layout

\begin_layout Standard
Understanding right, left, and interval censoring is essential in survival
 analysis.
 We will next turn our attention to the assumptions associated with these
 censoring types.
 These assumptions are inherent to many survival analysis methods and are
 critical in selecting an appropriate technique.
\end_layout

\begin_layout Subsubsection*
Censoring Assumptions
\end_layout

\begin_layout Standard
There are three types of censoring assumptions: 
\emph on
random
\emph default
, 
\emph on
independent
\emph default
, and 
\emph on
non-informative
\emph default
.
 Each shares certain similarities but also possesses unique distinctions
 which we will explore in detail.
 Censoring assumptions is the way censoring is managed.
\end_layout

\begin_layout Enumerate

\series bold
Random Censoring
\series default
: Subjects censored at time 
\emph on
t
\emph default
 are assumed to have the same failure rate as remaining subjects provided
 the same survival experience.
 Subjects that were censored are selected randomly, meaning the study does
 not influence or bias which participants are censored.
\end_layout

\begin_layout Enumerate

\series bold
Independent Censoring
\series default
: Independent censoring occurs when the censoring is random within certain
 subgroups, defined by specific covariates.
 If no covariates are present, it defaults to random censoring.
 This distinction might not be apparent when examining a single subgroup.
\end_layout

\begin_layout Enumerate

\series bold
Non-Informative Censoring
\series default
: Non-informative censoring occurs when censored instances do not provide
 any information on their survival prospects.
 In other words, whether or not the patient is censored, has no influence
 on experiencing the event.
\end_layout

\begin_layout Standard
Generally, it is safe to assume 
\emph on
non-informative
\emph default
 censoring when censoring is 
\emph on
independent
\emph default
 and/or 
\emph on
random
\emph default
.
 However, these assumptions are not equivalent.
 To better understand these concepts, let us look into some examples.
\end_layout

\begin_layout Standard
Consider a three-year disease occurrence study with 100 subjects at risk
 (group A).
 By the end of the study, 20 of them contracted the disease, giving a 20%
 three-year disease risk.
 Suppose we want to extend the study for another two years on the remaining
 80 individuals.
 However, 40 refuse to continue in the study and, are, therefore, lost to
 follow-up (censored).
 Of the remaining 40, 5 contracted the disease.
 Assuming those who left were representative of the remaining subjects (random
 and independent censoring), another 5 among the censored would have contracted
 it.
 Consequently, the five-year risk is 30% and the five-year survival is 70%
 under random and independent censoring assumptions.
 In this case, random and independent censoring are the same, as no predictor
 variables are considered.
\end_layout

\begin_layout Standard
To illustrate the difference between random and independent censoring, let
 us introduce another group to the study: group B with 100 individuals.
 In the first three years, 40 contracted the disease, and 10 left the study.
 So, the calculated three-year risk for group B is 40%.
 In the next two years, 10 out of 50 get the disease, yielding 20% risk
 for years between 3 and 5.
 Under the independent censoring assumption, we assume that out of 10 censored,
 2 contracted the disease.
 The five-year risk for group B is 52% with 48% survival under independent
 censoring assumptions.
\end_layout

\begin_layout Standard
As we can see, the five-year risk in the two groups differs significantly
 (30% against 52%), and the censoring proportion is also very different
 (50% against 17%).
 Hence, the overall censoring is not random.
 However, it is random within each group, so the censoring is independent.
 On other hand, if in group B, 30 subjects out of 60 were censored at the
 three-year mark, the censoring proportion would be the same in both groups,
 and the overall censoring would be random as those censored would be the
 representatives of those who remained at risk.
\begin_inset Note Note
status open

\begin_layout Plain Layout
50 vs.
 17???
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To best illustrate what non-informative censoring is, let us demonstrate
 informative censoring.
 Let us take a group of subjects under random and independent censoring
 assumptions.
 Every time subject A gets an event, subject B leaves the study (e.g., B is
 A's relative).
 If the censored subjects are representative of subjects at risk it would
 be random and independent censoring.
 Here, the censoring mechanism is directly related to the event occurrence,
 so the censoring is informative.
 
\end_layout

\begin_layout Subsubsection*
Survival Function
\end_layout

\begin_layout Standard
The 
\emph on
survival function
\emph default
, also known as the 
\emph on
survivor function
\emph default
, denoted by 
\begin_inset Formula $S(t)$
\end_inset

, represents the probability that the patient survives, in other words,
 does not experience the event of interest beyond the
\begin_inset Note Note
status open

\begin_layout Plain Layout
a
\end_layout

\end_inset

 given time 
\begin_inset Formula $t$
\end_inset

.
 Mathematically, it is denoted by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S(t)=P(T>t),\label{eq:f_surv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $P$
\end_inset

 represents the probability, 
\begin_inset Formula $t$
\end_inset

 is any specific time of interest, and 
\begin_inset Formula $T$
\end_inset

 is the random variable for the subject's survival time.
 For instance, if we want to know the likelihood that a patient is going
 to live for more than five years after a kidney transplant, we set 
\begin_inset Formula $t$
\end_inset

 equal to 5, and we evaluate 
\begin_inset Formula $S(t)$
\end_inset

 to determine the probability that 
\begin_inset Formula $T$
\end_inset

, actual survival time, is greater than 5 years.
 
\end_layout

\begin_layout Standard
The survival function has several key characteristics:
\end_layout

\begin_layout Enumerate
It continually decreases or maintains its value over time, theoretically
 extending from 0 to infinity.
 That is, if the study lasted indefinitely, the survival function would
 eventually fall to 0.
 In practical research scenarios, studies do not last forever, and not every
 patient experiences an event by the end of the study.
\end_layout

\begin_layout Enumerate
As it represents a probability, the function value ranges from 0 to 1.
 At the beginning of the observation period it starts at 1, indicating 100%
 survival probability, and declines over time, potentially reaching 0 as
 the probability of survival decreases.
\end_layout

\begin_layout Enumerate
Although the graph of the survival function is smooth by definition, in
 reality it is a step function.
 This stepwise representation is due to the nature of real-world data, where
 events are recorded at specific, discrete time points rather than continuously
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/survival_f.jpeg
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Survival function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Survival function 
\begin_inset Formula $S(t)$
\end_inset

 gives a comprehensive understanding of the probability of an individual
 surviving beyond a specific time point.
 It presents a declining or constant trend starting at 1 and potentially
 declining to 0.
 While understanding the probability of not failing at every time point
 is important, knowing the instantaneous risk might also be useful, leading
 us to the next tool: the hazard function.
\begin_inset Note Note
status open

\begin_layout Plain Layout
kouknut na tuhle vetu a prepsat
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Hazard Function
\end_layout

\begin_layout Standard
The survival function provides the probability of an individual surviving
 at each given point in time.
 This function is often preferred over the 
\emph on
hazard function
\emph default
 due to its more intuitive appeal: it directly communicates the chance of
 survival.
 However, there are scenarios where knowing the risk at each point in time
 is necessary, entailing the use of the hazard function.
 
\end_layout

\begin_layout Standard
The 
\emph on
hazard function
\emph default
, denoted as 
\begin_inset Formula $h(t)$
\end_inset

, represents the instantaneous potential per unit of time for the event
 to occur, provided the subject's survival up to time 
\begin_inset Formula $t$
\end_inset

.
 It is expressed by the following equation: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t)=\underset{\Delta t\rightarrow0}{lim}\frac{P(t\leq T<t+\Delta t|T\geq t)}{\Delta t}.\label{eq:f_hazard}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\Delta t$
\end_inset

 represents an infinitesimally small increment of time.
 The function 
\begin_inset Formula $h(t)$
\end_inset

 is equal to the limit, as 
\begin_inset Formula $\Delta t$
\end_inset

 approaches zero, of a conditional probability, divided by 
\begin_inset Formula $\Delta t$
\end_inset

.
 The conditional probability statement gives the probability that a person’s
 survival time 
\begin_inset Formula $T$
\end_inset

 will lie in the time interval between 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $t+\Delta t$
\end_inset

, given that the survival time is greater than or equal to 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
Occasionally, due to its structure, the hazard function is referred to as
 a 
\emph on
conditional failure rate
\emph default
.
 It is a rate because it represents a conditional probability per unit of
 time 
\begin_inset Formula $\Delta t$
\end_inset

, and it is conditional on the subject surviving until time 
\begin_inset Formula $t$
\end_inset

.
 Unlike probability, this rate has a scale from 0 to infinity — depending
 on the measure of time in days, weeks, or years.
 Essentially, by considering the limit as the 
\begin_inset Formula $\Delta t$
\end_inset

 approaches zero we basically get the instantaneous potential of failing
 at time 
\begin_inset Formula $t$
\end_inset

, given survival until that moment.
\end_layout

\begin_layout Standard
To best illustrate the concept of instantaneous potential, let us refer
 to the concept of velocity.
 Velocity gives us the speed at a specific point in time.
 For instance, the velocity of 80 km/h, means that maintaining the same
 speed for an hour would result in traveling 80 kilometers.
 However, it doesn't predict how much the car will travel in reality.
 The same works with the instantaneous potential: it might be high at one
 point but low at another, reflecting that both are measurements at an instant
 in time rather than an interval 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
The cumulative hazard function 
\emph default
further extends our understanding by quantifying the accumulated risk over
 time.
 It is the area under the hazard function that allows us to say which group
 has a greater risk.
 It is defined by the following function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H(t)\overset{def}{=}\int_{0}^{t}h(u)du,\text{ where }t>0.\label{eq:cumulative-hazard-function}
\end{equation}

\end_inset

Where 
\begin_inset Formula $h(u)$
\end_inset

 represents a hazard function.
 This integral measure enables a comprehensive comparison of risk between
 groups, showing which has a greater risk from the perspective of accumulated
 potential for the event to occur over time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Does not tell that CHF is specific to each point in ttime (accumulated hazard)
 somewhere in text I may have referred to it as a scalar value.
 Check that!
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/hazard_f.jpeg
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
The example of a hazard function.
 Note that it may signifficantly differ.
\begin_inset Note Note
status open

\begin_layout Plain Layout
divna veta.
 vyhodit
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*

\series bold
The Relationship Between the Survival and Hazard Functions
\end_layout

\begin_layout Standard
Some models, such as Cox Proportional Hazards, are written in terms of the
 hazard function, and it is necessary to be able to estimate the survival
 function out of the hazard function to make the model more flexible and
 accessible to a larger audience.
 Fortunately, there are ways to convert one into the other and vice versa.
 In this subsection, we are going to describe techniques for these conversions.
 
\end_layout

\begin_layout Standard
Let us start by considering how to obtain the survival function 
\begin_inset Formula $S(t)$
\end_inset

 from the hazard function 
\begin_inset Formula $h(t)$
\end_inset

.
 The relationship is captured in the following equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S(t)=exp\left[-\intop_{0}^{t}h(u)du\right].\label{eq:haz_to_surv}
\end{equation}

\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:haz_to_surv"
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrates that the survival function 
\begin_inset Formula $S(t)$
\end_inset

 is equal to the exponential of the negative cumulative hazard function
 from zero to 
\begin_inset Formula $t$
\end_inset

.
 As we can see, the integral in the equation is the 
\begin_inset Formula $H(t)$
\end_inset

 from the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cumulative-hazard-function"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 And we can simplify our equation to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S(t)=e^{-H(t)}
\]

\end_inset


\end_layout

\begin_layout Standard
On the other hand, to obtain the hazard function from the survival function,
 we can utilize the following relationship: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t)=-\left[\frac{dS(t)/dt}{S(t)}\right].\label{eq:surv_to_haz}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here, Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:surv_to_haz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 denotes that the hazard function 
\begin_inset Formula $h(t)$
\end_inset

 is the negative derivative of the survival function 
\begin_inset Formula $S(t)$
\end_inset

 with respect to time 
\begin_inset Formula $t$
\end_inset

 divided by 
\begin_inset Formula $S(t)$
\end_inset

.
\end_layout

\begin_layout Standard
Considering the fact that the survival function describes the probability
 of a patient surviving up to time 
\begin_inset Formula $t$
\end_inset

, and the hazard function shows the instantaneous risk of a person dying
 at a specific instant 
\begin_inset Formula $t$
\end_inset

, we can say that they provide complementary information about survival
 and risk over time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
 Of the two discussed functions, the survival function is used more often
 as it is more intuitive.
 In the practical part, we will estimate the survival function as well.
 Fortunately, we will not convert them manually.
 The 
\emph on
scikit-survival
\emph default
 library (more on it in the Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Overview-of-Machine"
plural "false"
caps "false"
noprefix "false"

\end_inset

) will do that for us.
 
\end_layout

\begin_layout Subsection
Taxonomy of Survival Analysis Methods
\end_layout

\begin_layout Standard
Survival analysis methods can be broadly categorized into 
\emph on
statistical
\emph default
 and 
\emph on
machine learning based
\emph default
 methods.
 Both aim to estimate the survival time and the survival probability at
 the estimated survival time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
 Statistical methods primarily characterize distributions of the event times
 and the statistical properties of the parameter estimation by estimating
 the survival curves.
 Typically, statistical methods are employed for low-dimensional data 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
On the other hand, machine learning methods focus on the prediction of the
 event occurrence at a given time.
 They harness the strengths of traditional survival analysis while integrating
 different machine learning techniques, leading to more potent algorithms.
 Machine learning methods are mostly used with high-dimensional data 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Statistical methods can be further subdivided based on their assumptions
 and parameter usage into parametric, non-parametric, and semi-parametric
 methods.
 Machine learning methods, encompassing methods like survival trees, ensembles
 (random survival forests), neural networks, and support vector machines
 form a distinct category.
 Advanced machine learning techniques, such as active learning, transfer
 learning, and multitask learning are included as well 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In the following sections, we will cover selected statistical methods and
 one machine learning technique.
 Machine learning techniques related to neural networks will be covered
 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Deep-Learning"
plural "false"
caps "false"
noprefix "false"

\end_inset

, dedicated to these advanced survival analysis techniques.
 
\end_layout

\begin_layout Subsection
Statistical Methods
\end_layout

\begin_layout Standard
This section provides a concise overview of statistical techniques.
 Here, we introduce three different types of statistical methods that are
 commonly used to estimate the survival and hazard functions: 
\emph on
non-parametric
\emph default
, 
\emph on
semi-parametric
\emph default
, and 
\emph on
parametric methods
\emph default
.
\end_layout

\begin_layout Standard

\emph on
Non-parametric methods
\emph default
 are preferred in situations where the event time does not adhere to any
 known distribution or when the proportional hazards assumption is not met
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
 There are three main non-parametric methods: the Kaplan-Meier (KM) method,
 the Nelson-Aalen (NA) estimator, and the Life-Table (LT) method.
 In the next section, we will cover Kaplan-Meier in more detail.
 The Life-Table method is more convenient than Kaplan-Meier for the estimation
 of survival curves when data subjects are segmented into distinct time
 intervals when dealing with an extensive number of subjects or a broad
 population scope.
 On the other hand, the Nelson-Aalen method is used for the estimation of
 hazard functions 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Semi-parametric models
\emph default
 offer a middle ground between fully parametric models, which make specific
 distributional assumptions, and non-parametric models, which make very
 few assumptions.
 Among semi-parametric methods, the Cox model is the most frequently employed
 model for survival regression analysis.
 It is semi-parametric as the distribution of the outcome is unknown.
 Unlike other approaches, this method is based on the proportional hazards
 assumption and uses partial likelihood for parameter estimation (more on
 that in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Cox-Proportional-Hazards"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 There are a couple of variants of the basic Cox model: the penalized Cox
 model, which will be used in the practical part of this work, the CoxBoost
 algorithm, and the Time-Dependent Cox model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Parametric methods
\emph default
 shine in their accuracy and efficiency when the time to the event conforms
 to a known distribution that can be specified in terms of certain parameters.
 With parametric models, estimating the time to the event is straightforward,
 whereas the Cox model can make this task somewhat cumbersome or unfeasible.
 In the domain of parametric models, linear regression is central.
 However, the Tobit model, Buckley-James regression, and penalized regression
 are the most favored.
 Beyond this, other parametric models like the Accelerated Failure Time
 (AFT) have gained traction.
 The AFT model represents survival time as a function of covariates 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
To conclude, statistical methods in survival analysis can be broadly categorized
 into non-parametric, semi-parametric, and parametric techniques, each with
 its unique strengths and applications.
 Among non-parametric methods, Kaplan-Meier stands out as particularly significa
nt.
 It provides a robust and intuitive way to estimate survival functions.
 The following section delves deeper into the Kaplan-Meier method.
\end_layout

\begin_layout Subsubsection

\series bold
Kaplan-Meier Survival Curves
\end_layout

\begin_layout Standard

\emph on
Kaplan-Meier
\emph default
 is a non-parametric method of survival function creation.
 It is 
\emph on
non-parametric
\emph default
 because it does not take into account any covariates, or parameters, and
 requires only the survival time and the censoring indicator.
 It works under an independent censoring assumption.
 The general Kaplan-Meier formula for plotting the survival function is
 the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{S}(t_{(f)})=\hat{S}(t_{(f-1)})\times\hat{P}(T>t_{(f)}|T\geq f_{(f)}).\label{eq:KM}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
That can be read as the survival function 
\begin_inset Formula $\hat{S}$
\end_inset

 of time 
\begin_inset Formula $t_{(f)}$
\end_inset

 is equal to the probability of surviving past the previous time point 
\begin_inset Formula $t_{(f-1)}$
\end_inset

 times the conditional probability of surviving past the time 
\begin_inset Formula $t_{(f)}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
 This function can also be expressed as a product limit:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{S}(t_{(f)})=\prod_{i=1}^{f-1}\hat{P}(T>t_{(i)}|T\geq f_{(i)}).
\]

\end_inset


\end_layout

\begin_layout Standard
In the Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KM"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we replaced 
\begin_inset Formula $\hat{S}(t_{(f-1)})$
\end_inset

 with the product of all fractions estimating the conditional probabilities
 for failure times 
\begin_inset Formula $t_{f-1}$
\end_inset

 and those preceding it 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
 Because of that the Kaplan-Meier estimator is sometimes referred to as
 the 
\emph on
product-limit method 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.

\emph default
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Survival-curve-estimated"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the survival curve created with the KM method on the dataset used
 in this work.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/Whole_dataset.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Survival curve estimated with KM 
\begin_inset CommandInset label
LatexCommand label
name "fig:Survival-curve-estimated"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
These survival curves are often compared using the log-rank test.
 The 
\emph on
log-rank test
\emph default
 is a way to compare two survival functions, that is often used in studies,
 where there is a target group and a placebo (control) group to assess the
 efficacy of the treatment or intervention in the study by comparing the
 survival curves of the two groups 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
To summarize, the Kaplan-Meier method provides a robust non-parametric approach
 for estimating survival functions, emphasizing its independence from covariates.
 While the log-rank test offers a mechanism to compare the survival curves
 and assess treatment efficacy, there are still situations that require
 taking covariates into consideration.
 That leads us to the Cox proportional hazards model, which inherently handles
 covariates.
\end_layout

\begin_layout Subsubsection
Cox Proportional Hazards Method 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Cox-Proportional-Hazards"

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
Cox proportional hazards
\emph default
 (also Cox PH) model is widely used in survival analysis semi-parametric
 model.
 This section explores its formulation, key properties, and reasons for
 its widespread use in research.
 
\end_layout

\begin_layout Standard
The Cox proportional hazards model is defined in terms of a hazard at time
 
\begin_inset Formula $t$
\end_inset

 for a subject with a given vector of explanatory variables 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t,\mathbf{X})=h_{0}(t)\times exp\left[\sum_{i=1}^{p}\beta_{i}X_{i}\right],\label{eq:cox_ph}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\series bold

\begin_inset Formula $h_{o}(t)$
\end_inset


\series default
 stands for the 
\emph on
baseline hazard function
\emph default
.
 Coefficients 
\begin_inset Formula $\beta$
\end_inset

 are the parameters of interest in the model.
 Since the exponential function has no 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is called 
\emph on
time-independent.

\emph default
 The exponential function ensures that the function is non-negative, satisfying
 the definition of the hazard function
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Even though Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cox_ph"
plural "false"
caps "false"
noprefix "false"

\end_inset

 contains the baseline hazard function, the function is not specified.
 Fortunately, we can calculate the hazard ratio, a measure of effect, without
 having to estimate the baseline hazard function.
 Similarly, the hazard function 
\begin_inset Formula $h(t,\mathbf{X})$
\end_inset

 and the survival function 
\begin_inset Formula $S(t,\mathbf{X})$
\end_inset

 can be estimated without the baseline function.
 So, with minimal assumptions, we can estimate everything we need (h, S,
 and HR).
\end_layout

\begin_layout Standard
The 
\emph on
hazard ratio
\emph default
 (HR) is a measure of the influence of an intervention on the outcome.
 A hazard ratio is defined as the hazard for one individual divided by the
 hazard for the other, as illustrated with the following formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{HR}=\frac{h(t,X^{*}\mathbf{)}}{h(t,X)}=exp\left[\sum_{i=1}^{p}\hat{\beta_{i}}(X^{*}-X)\right]\label{eq:cox_ph_hazard_ratio}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
As can be seen, the equation does not contain 
\begin_inset Formula $t$
\end_inset

 and the basic hazard function, as they are canceled out, making it a 
\emph on
proportional hazard assumption
\emph default
.
\end_layout

\begin_layout Standard
Like logistic regression, the CoxPH uses the 
\emph on
maximum likelihood 
\emph default
function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:max_likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

 to calculate its parameters (
\begin_inset Formula $\hat{\beta_{i}}$
\end_inset

).
 However, since the maximal likelihood considers only a part of patients,
 namely those who experienced an event, the formula is called 
\series bold
\emph on
partial
\series default
\emph default
 
\emph on
likelihood
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset


\emph on
.

\emph default
 
\end_layout

\begin_layout Standard
Let us define the partial likelihood.
 The subject's survival can be defined by 
\begin_inset Formula $h(t,X)dt$
\end_inset

 with 
\begin_inset Formula $dt→0$
\end_inset

.
 Consider 
\begin_inset Formula $J$
\end_inset

 (where 
\begin_inset Formula $J\leq N$
\end_inset

) as the total number of events of interest observed for 
\begin_inset Formula $N$
\end_inset

 instances.
 
\begin_inset Formula $T_{1}<T_{2}<\cdots<T_{J}$
\end_inset

 represents the unique and sequentially ordered times to the event of interest.
 Let 
\begin_inset Formula $X_{j}$
\end_inset

 be the vector of covariates for a subject who experiences the event at
 time 
\begin_inset Formula $T_{j}$
\end_inset

.
 
\begin_inset Formula $R_{j}$
\end_inset

 is the set of risk subjects at 
\begin_inset Formula $T_{j}$
\end_inset

.
 Given that the event happens at time 
\begin_inset Formula $T_{j}$
\end_inset

, the individual probability associated with 
\begin_inset Formula $X_{j}$
\end_inset

 can be described as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{h(T_{j},X_{j})dt}{\sum_{i\epsilon R_{j}}h(T_{j},X_{i})dt}.
\]

\end_inset


\end_layout

\begin_layout Standard
By taking the product across all subject's probabilities we get the partial
 likelihood.
 Based on Cox assumption and the presence of censoring, partial likelihood
 is defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
PL(\beta)=\prod_{j=1}^{N}\left[\frac{exp(X_{j}\beta)}{\sum_{i\epsilon R_{j}}exp(X_{i}\beta)}\right]^{\delta_{J}}.\label{eq:partial-likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\delta_{j}$
\end_inset

 is the indicator of censoring of a given instance (1 for event occurrence,
 0 for censoring).
 Therefore, if the subject is censored, 
\begin_inset Formula $\delta=0$
\end_inset

, the individual probability is equal to 1 and the subject does not affect
 the result.
 The vector of coefficients 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is estimated by either maximizing partial likelihood, defined above, or
 maximizing the negative 
\emph on
log-partial likelihood
\emph default
 to improve the efficiency:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
LL(\beta)=-\sum_{j=1}^{N}\delta_{j}\{X_{j}\beta-log[\sum_{i\epsilon R_{j}}exp(X_{i}\beta)]\}.
\]

\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
Being a semi-parametric, this model is a safe choice because it consistently
 delivers a sufficiently reliable result.
 The risk of choosing the wrong model as often happens with parametric models,
 is practically non-existent.
 However, if one is sure that a parametric model suits the problem, one
 should use the parametric model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
Penalized Cox Models
\end_layout

\begin_layout Standard
The Cox proportional hazards model is often chosen, since its coefficients
 can be interpreted in terms of hazard ratios, offering meaningful insights.
 Yet, when estimating coefficients for many features, the standard Cox model
 collapses due to matrix inversion getting disrupted by correlations between
 features.
 Feature (column) correlations make a matrix singular, i.e.
 impossible to revert.
 In this section, we aim to explore the 
\emph on
Ridge regression
\emph default
, 
\emph on
LASSO
\emph default
, and 
\emph on
Elastic Net
\emph default
 methods as extensions or modifications to the Cox model.
 These techniques address the inherent challenges in the standard Cox model,
 offering solutions for regularization and feature selection.
\end_layout

\begin_layout Paragraph
Ridge
\end_layout

\begin_layout Standard
By incorporating an 
\begin_inset Formula $l_{2}$
\end_inset

 penalty term on the coefficients, shrinking them to zero, we can avoid
 the problem of the inability to revert singular matrix.
 Consequently, our objective has the following form: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\frac{\alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
In the equation, 
\begin_inset Formula $PL(\beta)$
\end_inset

 denotes the partial likelihood of the Cox model (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:partial-likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

), terms 
\begin_inset Formula $\beta_{1},\ldots,\beta_{p}$
\end_inset

represent the coefficients corresponding to 
\begin_inset Formula $p$
\end_inset

 features, 
\begin_inset Formula $\alpha\geq0$
\end_inset

 is a hyper-parameter that controls the amount shrinkage.
 The resulting objective is referred to as 
\emph on
ridge regression.

\emph default
 If 
\begin_inset Formula $\alpha$
\end_inset

 is set to zero, we get the regular Cox model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-37"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
LASSO
\end_layout

\begin_layout Standard
While the 
\begin_inset Formula $l_{2}$
\end_inset

 ridge penalty solves the mathematical problem of fitting the Cox model,
 we would still need to take into account all features, no matter how many
 there are.
 Preferably, we would like to select a small subset of the most predictive
 features and ignore the rest, as too many features might result in overfitting.
 
\emph on
LASSO
\emph default
 (Least Absolute Shrinkage and Selection Operator) does exactly that.
 Rather than merely shrinking the coefficients to zero, it performs feature
 selection as a part of the optimization process, where a subset of coefficients
 is set to zero and is, therefore, excluded, reducing the number of features
 we would need for prediction.
 Mathematically, we would replace the 
\begin_inset Formula $l_{2}$
\end_inset

 penalty with the 
\begin_inset Formula $l_{1}$
\end_inset

 penalty, leading to the following optimization problem: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\alpha\sum_{j=1}^{p}|\beta_{j}|.
\]

\end_inset


\end_layout

\begin_layout Standard
The main drawback is that we cannot directly control the number of features
 selected.
 However, the value of 
\begin_inset Formula $\alpha$
\end_inset

 essentially determines the number of features selected.
 To achieve a refined model that requires fewer features, we need a data-driven
 way to determine the appropriate 
\begin_inset Formula $\alpha$
\end_inset

.
 This can be accomplished by first determining the 
\begin_inset Formula $\alpha$
\end_inset

 that would ignore all features (coefficients are set to zero) and then
 gradually decreasing its value, possibly down to 1% of its starting value.
 Fortunately, it is implemented in scikit-survival's 
\emph on
sksurv.linear_model.CoxnetSurvivalAnalysis.
 
\emph default
We would need to set 
\emph on
l1_ratio=1.0
\emph default
 and 
\emph on
alpha_min_ratio=0.01
\emph default
 to search for 100 
\begin_inset Formula $\alpha$
\end_inset

 values up to 1% of the estimated maximum 
\begin_inset CommandInset citation
LatexCommand cite
key "key-37"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Elastic Net
\end_layout

\begin_layout Standard
The LASSO method is effective for selecting a subset of discriminative features.
 However, it is not without its shortcomings.
 The first is that LASSO is unable to select more features than there are
 instances in the training data set.
 The second is its tendency to randomly select only one feature out of a
 set of highly correlated ones.
 The 
\emph on
Elastic Net
\emph default
 alleviates these issues by incorporating the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalties in a weighted manner, as is shown in the following optimization
 problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\alpha\left(r\sum_{j=1}^{p}|\beta_{j}|+\frac{1-r}{2}\sum_{j=1}^{p}\beta_{j}^{2}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $r\epsilon[0;1]$
\end_inset

 is the relative weight of the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalty (
\begin_inset Formula $r=1$
\end_inset

 is a LASSO penalty, 
\begin_inset Formula $r=0$
\end_inset

 is a ridge penalty).
 The Elastic Net penalty combines the LASSO's feature selection capability
 and Ridge's regularization power.
 As a result, it is more stable than LASSO, and in a situation of highly
 correlated features, it would select them all, while LASSO would randomly
 select only one.
 Usually, it is sufficient to give the 
\begin_inset Formula $l_{2}$
\end_inset

 penalty only a small weight to improve the stability of the LASSO, for
 example, 
\begin_inset Formula $r=0.9$
\end_inset

 might suffice.
\end_layout

\begin_layout Standard
Similar to LASSO, the weight 
\begin_inset Formula $\alpha$
\end_inset

 inherently dictates the size of the chosen subset.
 Its optimal value is typically estimated in a data-driven way 
\begin_inset CommandInset citation
LatexCommand cite
key "key-37"
literal "false"

\end_inset

.
 More on that in the practical part, where we will choose the best 
\begin_inset Formula $\alpha$
\end_inset

 for our data set.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
don't forget to add a reference to the practical part
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To conclude, Ridge, LASSO, and Elastic Net offer powerful extensions of
 the Cox PH model, especially when dealing with high-dimensional datasets
 and highly correlated features.
 They help to regularize the Cox model and select only the most significant
 features, avoiding overfitting and assuring a more stable and interpretable
 model.
 Nevertheless, it is important to acknowledge their limitations.
 As linear models, they inherently capture only linear relationships, and
 non-linear relationships remain uncaptured.
 That leads us to the next section dedicated to the Random Survival Forests,
 a machine learning approach.
 Machine learning techniques are known for their ability to catch complex
 non-linear relationships, often outperforming traditional statistical methods
 in predictive accuracy.
\end_layout

\begin_layout Subsection
Random Survival Forest 
\end_layout

\begin_layout Standard

\emph on
Random Survival Forest
\emph default
 (RSF) is an ensemble machine learning method tailored for survival analysis.
 It is derived from the original Random Forest method developed by 
\begin_inset CommandInset citation
LatexCommand cite
key "key-28"
literal "false"

\end_inset

.
 Random Forests have gained popularity in the machine learning community
 due to their effectiveness in classification and regression machine learning
 tasks.
 Random Forest is built from multiple decision trees, averaging their prediction
s allows for more accurate predictions than any single tree can provide.
 Similar to Random Forest, Random Survival Forest leverages the power of
 multiple survival trees making its predictions more robust.
 
\end_layout

\begin_layout Standard
Random Survival Forest is comprised of 
\emph on
Survival Trees
\emph default
.
 A survival tree is essentially a binary tree.
 It is developed by the iterative splitting of children nodes into two nodes
 until a certain criterion is met.
 An optimal node split is the one maximizing the survival difference between
 the children nodes.
 It is done by iterating through all features and finding such a value for
 a given feature that maximizes the survival difference 
\begin_inset CommandInset citation
LatexCommand cite
key "key-39"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Random Survival Forest training is comprised of the following steps:
\end_layout

\begin_layout Enumerate
Randomly draw 
\begin_inset Formula $L$
\end_inset

 bootstrap samples of size 
\begin_inset Formula $n$
\end_inset

 with replacement from the training dataset.
 
\begin_inset Formula $n$
\end_inset

 is about two-thirds of the original data.
 The remaining instances, about one-third, are termed out-of-bag (OOB) observati
ons and are not included in the bootstrap sample.
\end_layout

\begin_layout Enumerate
For each sample, develop a full-grown survival tree based on the selected
 splitting criterion.
 At each node, randomly select 
\begin_inset Formula $\sqrt{p}$
\end_inset

 (or other number) covariates, where 
\begin_inset Formula $p$
\end_inset

 is the total number of covariates.
 Stop developing when a certain condition is met (for example, when the
 terminal node has fewer observations than a predetermined threshold or
 the node reaches purity).
 
\end_layout

\begin_layout Enumerate
For each tree, calculate the cumulative hazard function with the Nelson-Aalen
 estimator.
 Find a mean of all trees to find ensemble CHF.
\end_layout

\begin_layout Enumerate
Use OOB data to determine the prediction error of the ensemble 
\begin_inset CommandInset citation
LatexCommand cite
key "key-38"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In summary, the Random Survival Forest is a robust and powerful approach
 to survival analysis that harnesses the collective power of multiple survival
 trees.
 Having explored several survival analysis methods, it is crucial to be
 able to evaluate them properly, since the standard machine learning performance
 metrics are not always applicable to survival analysis due to the presence
 of censoring.
 The next section will introduce essential criteria for the evaluation of
 survival analysis models and the primary performance metrics created for
 this purpose.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
Survival prediction models play a vital role in healthcare.
 They are often used to estimate the risk of developing a particular disease
 and are crucial in guiding the clinical management of patients.
 It is, therefore, essential to assess their performance accurately.
 Similar to machine learning, this process of model evaluation is referred
 to as model validation.
 There are three aspects we can asses our model on:
\end_layout

\begin_layout Enumerate

\series bold
Overall performance, 
\series default
which is the distance between the predicted and observed survival time.
\end_layout

\begin_layout Enumerate

\series bold
Discrimination,
\series default
 or the model's ability to distinguish between high- and low-risk patients.

\series bold
 
\end_layout

\begin_layout Enumerate

\series bold
Calibration
\series default
 is the agreement between the observed and predicted survival times 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The absence of bias in a situation when the validation set contains censored
 instances is a sign of a good performance measure.
 Otherwise, in the presence of high levels of censoring, the evaluation
 would be unreasonably optimistic 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In this section, we will cover three measures of discrimination and one
 measure that assesses both discrimination and calibration (overall performance).
\end_layout

\begin_layout Subsubsection
Harrel's and Uno's Concordance Indices
\end_layout

\begin_layout Standard
One way to measure discrimination is through 
\emph on
concordance
\emph default
.
 A pair of patients is 
\emph on
concordant
\emph default
 if the subject who experiences an event earlier has a greater estimated
 risk 
\begin_inset CommandInset citation
LatexCommand cite
key "key-30"
literal "false"

\end_inset

.
 
\emph on
Measures of concordance
\emph default
 quantify the rank correlation between the predicted risk and the observed
 survival times.
 Typically, their values range between 0.5 and 1.
 A value of 0.5 indicates no discrimination, while 1 corresponds to the ideal
 discrimination 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The 
\emph on
concordance index
\emph default
, or 
\emph on
C-index
\emph default
, is the most widely used performance metric in survival analysis.
 It is defined through the concordance probability.
 The
\emph on
 concordance probability
\emph default
 is the probability that from the arbitrarily selected pair of patients
 
\begin_inset Formula $(i,j)$
\end_inset

, the one with a shorter survival time 
\begin_inset Formula $T$
\end_inset

, has the higher predicted risk 
\begin_inset Formula $M$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

.
 Mathematically, this is expressed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=P(M_{i}>M_{j}|T_{i}<T_{j}).
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
Harrell's concordance index
\emph default
 is the most widely used implementation of concordance index.
 To compute Harrel's concordance index 
\begin_inset Formula $C_{H}$
\end_inset

, we consider every comparable pair of patients where the one with the shorter
 time failed.
 Pair is 
\begin_inset Quotes sld
\end_inset

comparable
\begin_inset Quotes srd
\end_inset

 if we can determine which of them experienced the event first 
\begin_inset CommandInset citation
LatexCommand cite
key "key-30"
literal "false"

\end_inset

.
 
\begin_inset Formula $C_{H}$
\end_inset

 is estimated as the proportion of these pairs in which the subject with
 the shorter survival time has a higher estimated risk.
 A modified version of this estimator, 
\begin_inset Formula $C_{H}(\tau)$
\end_inset

, only considers patients with 
\begin_inset Formula $T_{i}<\tau$
\end_inset

 and may provide more stable estimates 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Mathematically, Harrell's C-index is defined as a ratio between the number
 of concordant and comparable pairs:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{C}=\frac{\sum_{i=1}^{N}\Delta_{i}\sum_{j=i+1}^{N}\left[I(T_{i}^{obs}<T_{j}^{obs})+(1-\Delta_{j})I(T_{i}^{obs}=T_{j}^{obs})\right]\left[I(M_{i}>M_{j})+\frac{1}{2}I(M_{i}=M_{j})\right]}{\sum_{i=1}^{N}\Delta_{i}\sum_{j=i+1}^{N}\left[I(T_{i}^{obs}<T_{j}^{obs})+(1-\Delta_{j})I(T_{i}^{obs}=T_{j}^{obs})\right]}.\label{eq:c-index}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $I(\cdot)$
\end_inset

 is the indicator function.
 It is equal to 1 if its argument is true, and 0 if it is not.
 
\begin_inset Formula $T^{obs}$
\end_inset

 is the observation time.
 
\begin_inset Formula $\Delta_{i}$
\end_inset

 is a binary variable and 
\begin_inset Formula $\Delta_{i}=1$
\end_inset

 if the subject 
\begin_inset Formula $i$
\end_inset

 experienced an event during the time of observation and 
\begin_inset Formula $\Delta_{i}=0$
\end_inset

 if they did not.
\end_layout

\begin_layout Standard
According to the Scikit-survival's documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

 and Rahman et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34"
literal "false"

\end_inset

, Harrell's concordance index becomes biased in the presence of censoring.
 The bias increases with the level of censoring.
 Uno et.
 al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-31"
literal "false"

\end_inset

 introduced a modified concordance index, 
\begin_inset Formula $C_{U}(\tau)$
\end_inset

, which incorporates weights based on the probability of being censored.
 While their estimator proved robust to the choice of 
\begin_inset Formula $\tau$
\end_inset

, they noted that the error of the estimate might be quite large if there
 are too few instances beyond this time point 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21,key-22"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
I was unable to find usable uno's definition
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Having explored the concordance index and its modifications, it is evident
 that it provides valuable insight into the model's discriminatory abilities.
 Yet, it offers a singular value that characterizes a model's performance
 across different time points without considering fluctuations in concordance
 that tend to happen over time.
 To enhance our evaluation of model performance in terms of discrimination
 across varying time points, we turn our attention to another popular metric
 in survival analysis: the ROC AUC.
 The subsequent section explores it in detail.
\end_layout

\begin_layout Subsubsection
Time-dependent Area under the ROC
\end_layout

\begin_layout Standard
The 
\emph on
area under the receiver operating characteristic curve
\emph default
 (ROC AUC) is a popular performance measure for binary classification tasks.
 In survival analysis, it is used to determine how well estimated risk scores
 can distinguish diseased patients from healthy ones 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In binary classification, the
\emph on
 receiver operating characteristic (ROC)
\emph default
 is a curve that plots the 
\emph on
true positive rate (TPR or sensitivity)
\emph default
 against the 
\emph on
false positive rate (FPR)
\emph default
.
 The FPR is the ratio of negative instances that are falsely classified
 as positive.
 It is equal to 
\begin_inset Formula $1-$
\end_inset

 the 
\emph on
true negative rate
\emph default
 (the ratio of negative instances that are correctly classified, often referred
 to as 
\emph on
specificity
\emph default
).
 TPR, or sensitivity, represents the ratio of positive instances classified
 as positive 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
In survival analysis, we extend the ROC to continuous outcomes, where a
 patient is alive at the start of the observation, but might experience
 an event at some point later.
 Specificity and sensitivity, therefore, become time-dependent measures.
 It is specifically important, as model accuracy tends to be different at
 different points in time.
 Here we consider 
\emph on
cumulative cases
\emph default
 and 
\emph on
dynamic controls
\emph default
 at any given point in time 
\begin_inset Formula $t$
\end_inset

.
 
\emph on
Cumulative cases
\emph default
 are all subjects who experienced an event prior to or at time 
\begin_inset Formula $t$
\end_inset

, while 
\emph on
dynamic controls
\emph default
 are those who are yet to experience the event after time 
\begin_inset Formula $t$
\end_inset

.
 By calculating the ROC AUC for any given time point 
\begin_inset Formula $t$
\end_inset

, we assess the model's ability to differentiate between patients.
 Specifically, how well the model can distinguish patients who fail by a
 given time 
\begin_inset Formula $t_{i}<t$
\end_inset

 from subjects who fail after this time 
\begin_inset Formula $t_{i}>t$
\end_inset

.
 The time-dependent ROC AUC is especially useful when we want to predict
 an event happening in a period up to time 
\begin_inset Formula $t$
\end_inset

, rather than at a specific time-point 
\begin_inset Formula $t$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
While the time-dependent ROC AUC provides a valuable measure of a model's
 discrimination ability at various time points, it falls short in providing
 insight into the accuracy of individual predictions – calibration.
 To address this limitation and provide a more comprehensive model assessment,
 we turn our attention to the time-dependent Brier score.
\end_layout

\begin_layout Subsubsection
Time-dependent Brier Score 
\end_layout

\begin_layout Standard
Time-dependent ROC AUC and concordance index are great for assessing the
 overall discrimination among all time points (mean AUC and c-index) and
 the discrimination at any individual time point (the ROC graph), but they
 tell us nothing about the accuracy of individual predictions 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
 A metric analogous to regression performance measures used in machine learning
 would be ideal.
 Fortunately, such a metric exists.
 
\emph on
Time-dependent Brier score
\emph default
 is a modification of mean squared error (MSE) that handles right censored
 data.
\end_layout

\begin_layout Standard
While the concordance index and time-dependent ROC AUC measure only discriminati
on, the time-dependent Brier score measures both discrimination and calibration,
 making it a metric of 
\begin_inset Quotes sld
\end_inset

overall performance
\begin_inset Quotes srd
\end_inset

.
 It is defined by the following equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
BS^{c}(t)=\frac{1}{n}\sum_{i=1}^{n}I(y_{i}\leq t\wedge\delta_{i}=1)\frac{(0-\hat{\pi}(t|\mathbf{x}_{i}))^{2}}{\hat{G}(y_{i})}+I(y_{i}>t)\frac{(1-\hat{\pi}(t|\mathbf{x}_{i}))^{2}}{\hat{G}(t)}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\series bold

\begin_inset Formula $\hat{\pi(t|\mathbf{x})}$
\end_inset


\series default
is a model’s predicted probability of remaining event-free up to the time
 point 
\begin_inset Formula $t$
\end_inset

 for feature vector 
\series bold
x
\series default
, and 
\begin_inset Formula $\frac{1}{\hat{G}(t)}$
\end_inset

 is the inverse probability of censoring weight 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
 
\begin_inset Formula $I(\cdot)$
\end_inset

 is the indicator function.
 
\begin_inset Formula $y_{i}$
\end_inset

 is the subject's survival time, 
\begin_inset Formula $\delta_{i}$
\end_inset

 is the event/censoring indicator.
\end_layout

\begin_layout Standard
The limitation of the time-dependent Brier Score is its applicability exclusivel
y to the models that are capable of estimating the survival function.
 
\emph on
Integrated Brier Score
\emph default
 provides a scalar value for general model evaluation.
 It is beneficial for model comparison, as time-dependent measures are a
 bit harder to compare than scalar values, and for the model fine-tuning
 process.
\end_layout

\begin_layout Standard
Survival analysis provides a suite of techniques for the prediction of a
 time to event.
 As we explored its methods and evaluation metrics, it is evident that the
 traditional SA provides robust tools for handling censored data, evaluating
 risk factors, and making predictions over time.
 However, as data grows in both complexity and volume, there is a growing
 need for more advanced modeling techniques, leading us to advanced machine
 learning: deep learning.
 Deep Learning, with its ability to handle large and complex datasets, promises
 to expand traditional survival analysis methods.
 In the following section, we will explore how deep learning can be adapted
 for survival analysis to harness the power of neural networks for more
 precise predictions.
\end_layout

\begin_layout Section
Deep Learning 
\begin_inset CommandInset label
LatexCommand label
name "sec:Deep-Learning"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
No deep learning, the paper is already too long.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
universal approximation theorem
\end_layout

\begin_layout Plain Layout
architecture
\end_layout

\begin_layout Plain Layout
weights/parametersr
\end_layout

\begin_layout Plain Layout
independent variable
\end_layout

\begin_layout Plain Layout
loss
\end_layout

\begin_layout Plain Layout
poistive feedback loop -> bias
\end_layout

\begin_layout Plain Layout
architecture is not that important - don't spend much time on thinking about
 it.
\end_layout

\begin_layout Plain Layout
metric for humans, loss for algos
\end_layout

\begin_layout Plain Layout
it is better to always use pretrained models
\end_layout

\begin_layout Plain Layout
head of the nn - last layer
\end_layout

\begin_layout Plain Layout
transfer learning, pretrained models
\end_layout

\begin_layout Plain Layout
fine-tuning the model (in transfer learning)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
What is neuron
\end_layout

\begin_layout Plain Layout
epoch
\end_layout

\begin_layout Plain Layout
fitting the model
\end_layout

\begin_layout Plain Layout
Activation Function
\end_layout

\begin_layout Plain Layout
Optimisation Process
\end_layout

\begin_layout Plain Layout
Backpropagation
\end_layout

\begin_layout Plain Layout
Gradient Descent
\end_layout

\begin_layout Plain Layout
Stochastic Gradient Descent
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Feed Forward Neural Networks
\end_layout

\begin_layout Standard
Recurrent Neural Networks
\end_layout

\begin_layout Standard
Generative Models for Survival Analysis
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
I am going to use survival FFNN, likely DeepSurv
\end_layout

\end_inset


\end_layout

\begin_layout Section
Overview of Machine Learning Libraries and Tools 
\begin_inset CommandInset label
LatexCommand label
name "sec:Overview-of-Machine"

\end_inset


\end_layout

\begin_layout Standard
for the survival analysis the python library scikit-survival was used
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
python.
 what is the main disadvantage
\end_layout

\begin_layout Plain Layout
numpy 
\end_layout

\begin_layout Plain Layout
pandas
\end_layout

\begin_layout Subsection
Sci-kit learn
\end_layout

\begin_layout Plain Layout
- pros
\end_layout

\begin_layout Plain Layout
- cons
\end_layout

\begin_layout Plain Layout
- where it is used
\end_layout

\begin_layout Subsection
Keras
\end_layout

\begin_layout Subsection
Tensorflow
\end_layout

\begin_layout Plain Layout
- pros
\end_layout

\begin_layout Plain Layout
- cons
\end_layout

\begin_layout Plain Layout
- where it can be used
\end_layout

\begin_layout Subsection
PyTorch
\end_layout

\begin_layout Plain Layout
- pros
\end_layout

\begin_layout Plain Layout
- cons
\end_layout

\begin_layout Plain Layout
- where it can be used
\end_layout

\begin_layout Plain Layout
it is more research driven
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Chapter
Data Preparation and Analysis
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
In this chapter I will:
\end_layout

\begin_layout Itemize
make sense of the dataset
\end_layout

\begin_layout Itemize
explore features, their impact on the survival, how do they correlate
\end_layout

\begin_layout Itemize
explain my data pipeline and my approach to data processing
\end_layout

\begin_layout Itemize
explain how do I build the dataset
\end_layout

\begin_layout Itemize
the code for the image generation can be found in the github + give a link
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this chapter we age going to look into the UNOS dataset.
 Make sense of the dataset.
 Explore important features and their relationship with each other.
 Look into survival time for 
\end_layout

\begin_layout Standard
The dataset provided by the IKEM (Institute of Clinical and Experimental
 Medicine in Prague) that we had from the beginning was not suitable for
 any meaningful analysis.
 That is why it was decided to look for the dataset elsewhere.
\end_layout

\begin_layout Standard
The data reported here have been supplied by the United Network for Organ
 Sharing as the contractor for the Organ Procurement and Transplantation
 Network.
 The interpretation and reporting of these data are the responsibility of
 the author(s) and in no way should be seen as an official policy of or
 interpretation by the OPTN or the U.S.
 Government.
\end_layout

\begin_layout Standard
The dataset is not for the open use.
 If you are interested in testing the results achieved in this paper, you
 need to acquire the data first.
 The requirements for the data acquirement are written here 
\begin_inset Note Note
status open

\begin_layout Plain Layout
give a link to the UNOS website
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The dataset consists of 993 806 of records for both transplanted patients
 and ones from the waiting list, and 450 features comprised of waiting list
 data and already transplanted patients for kidney and pancreas transplant
 from October 1, 1987 to the present.
 Kidney transplants have 490 172 records.
\end_layout

\begin_layout Section
Data Loading
\end_layout

\begin_layout Standard
The data were provided in a form of a MongoDB database dump.
 It is impossible to perform data analysis with the database dump.
 So it was necessary to run the database first and import the database dump
 there.
 We set up the database in a Docker container (Docker is container management
 system
\begin_inset Note Note
status open

\begin_layout Plain Layout
+ 
\series bold
explain what is container
\end_layout

\end_inset

) locally on my Mac, as the university cluster unfortunatelly does not have
 Docker.
 The data from the database and table kidpan were then exported to CSV,
 compressed into zip and uploaded to the cluster.
\end_layout

\begin_layout Standard
The pandas DataFrame method read_csv() loaded data for too long to work
 comfortably (5 minutes), as the CSV file had the size of 80GB, so it was
 decided to use parquet file instead.
 It was done by dumping the pandas DataFrame into Parquet database file
 using DataFrame.to_parquet() method.
 Parquet is used for efficient cloud computing.
 It provides more efficient way of loading data, as it works on the principles
 of databases, so the loading time of the whole dataset was decreased to
 38 seconds.
 Additionally, it allows for specifying what columns to load, reducing the
 data loading time to mere 21 seconds.
 Thus using this technology has signifficantly improved the workflow.
\end_layout

\begin_layout Section
Data preprocessing pipeline
\end_layout

\begin_layout Standard
In this section I will describe the data pipeline that is used to create
 the dataset out of the raw data.
 The pipeline can be found in github repository of this paper: 
\begin_inset CommandInset href
LatexCommand href
name "survival_pipeline.py"
target "https://github.com/krllstdn/BachelorProject/blob/main/Code/surv_data_pipeline/survival_pipline.py"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The work with the pipeline is pretty straightforward: we initialize the
 class and call the 
\emph on
load()
\emph default
 method.
 As is shown in the following block of python code:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,numbers=left,basicstyle={\ttfamily},breaklines=true,tabsize=4"
inline false
status open

\begin_layout Plain Layout

from surv_data_pipeline.survival_pipline import ScikitSurvivalDataLoader
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

loader = ScikitSurvivalDataLoader()
\end_layout

\begin_layout Plain Layout

X, y = loader.load()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Two main constants of the class are 
\emph on
categorical_values 
\emph default
and 
\emph on
numerical_values.
 
\emph default
Categorical and numerical features must be specified there.
 It is important for following preprocessing steps.
\end_layout

\begin_layout Standard
The main method of the class 
\emph on
ScikitSurvivalDataLoader 
\emph default
is 
\emph on
load().
 
\emph default
This method loads the data into the pandas DataFrame, applies exclusion
 criteria (more on that later), handles NaN values and returns X and y,
 X being numerical (categorical values were handled with OneHot encoding
 and numerical values were scaled) and y having format of (PSTATUS, PTIME),
 first one is the boolean censoring indicator (True - event happened, False
 - otherwise), PTIME is the number of days survived.
 This format is required by the Scikit-survival Library to build survival
 estimators.
 
\end_layout

\begin_layout Standard
The first step is to load the data into pandas DataFrame from the parquet
 file.
 Fortunately, pandas has support for this kind of files.
 It is performed by the Pandas method 
\emph on
read_parquet(path, engine, columns).
 
\emph default
In 
\emph on
path
\emph default
 we need to specify the path to the parquet file, 
\emph on
engine
\emph default
 specifies what parquet library should be used, I use 'auto', it tries 
\emph on
pyarrow
\emph default
 if it doesn't work it uses 
\emph on
fastparquet.

\emph default
 In 
\emph on
columns 
\emph default
we need to specify the columns we want to load.
 (explain more what is pyarrow and parquet)
\end_layout

\begin_layout Standard

\series bold
Description of feature engineering step
\series default
: 
\end_layout

\begin_layout Standard
The next step is to divide the dataset into training, validation and test
 sets, the reasons behind that, were explained in the datapreprocessing
 section of the previous chapter.
 These sets are then assigned as class variables to the class and are sent
 to preprocessing method 
\emph on
_handle_nan(), 
\emph default
where the NaN values are filled with median, specific value, or examples
 with such values are deleted with the pandas DataFrame method 
\emph on
drop_na(), 
\emph default
depending on the 
\emph on
fill_na_with_median
\emph default
 boolean parameter.
\end_layout

\begin_layout Standard
After the NaN handling step, the training set is send to the method 
\emph on
_get_X_y()
\emph default
 where the numerical values are standartized
\series bold
 
\series default
and categorical are encoded with the OneHot encoding with the Scikit-Survival
 methods 
\emph on
standardize()
\emph default
 and 
\emph on
encode_categorical().
 
\emph default
Numerical and categorical values then comprise the 
\emph on
X
\emph default
 set, directly used in the training.

\emph on
 
\emph default
The target value set is constructed with 
\emph on
Surv.from_arrays()
\emph default
 utility that accepts event and survival time and builds the 
\emph on
y
\emph default
 value acceptable to scikit-survival algorithms.
 The class variable 
\emph on
df 
\emph default
is then set to None with the goal of memory optimisation.
 X and y are then returned.
\end_layout

\begin_layout Standard
When we need the validation and the test sets, we just call methods 
\emph on
get_validate_X_y() 
\emph default
and 
\emph on
get_test_X_y()
\emph default
 which will provide us with the sets for the hyperparameter tuning step
 with the validation set and the final evaluation step with the test set.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Reexamine it, as some things were already changed.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Exploratory Data Analysis
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Goals of this section:
\end_layout

\begin_layout Plain Layout
- explore how different features influence survival
\end_layout

\begin_layout Plain Layout
- look into the literature: what does it say
\end_layout

\begin_layout Plain Layout
- explain why is this the case, speculate and link to the literature
\end_layout

\begin_layout Plain Layout
- explain the dataset building
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this section we are going to cover all the most signifficant features.
 The data were not adjusted to limit the influence of other factors, so
 the correlations I am trying to make here might not be fully correct, however
 some are confirmed in literature.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Transition from the subsub sections to unnumbered ones.
 remove 
\begin_inset Quotes sld
\end_inset

in this section...
\begin_inset Quotes srd
\end_inset

 Make it shorter
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Survival Data
\end_layout

\begin_layout Standard
In this subsection we are going to explore the 
\emph on
y
\emph default
 axis that is going to be used for the training of the survival estimators.
 The 
\emph on
y
\emph default
 value consists of censoring status, which is a boolean value, and time
 to event, which is a numerical value representing the survival time or
 the time at which it was censored.
 The y value is called the 
\emph on
survival data
\emph default
.
 The column PSTATUS is a censoring status, while the PTIME column represents
 the time-to-event variable.
\end_layout

\begin_layout Standard
To best visualize the time-to-event variable we are going to use a box plot.
 A box plot is a simple, yet powerful statistical graph based on quartiles,
 that allows to quickly make sense of the data distribution.
 (odkaz na statistics for data scientists) It is based on three quartiles:
 the first(
\begin_inset Formula $Q_{1}$
\end_inset

), the second(
\begin_inset Formula $Q_{2}$
\end_inset

) and the third (
\begin_inset Formula $Q_{3}).$
\end_inset

 First quartile corresponds to 25 percentile and means that 25% of the datapoint
s are below it.
 The second quartile corresponds to 50% percentile, or median, and it means
 that below and above that point lies an equal amount of data points.
 The third quartile corresponds to 75 percentile and it means that below
 it lies 75% of data points.
 The quartiles form the box: the first quartile forms the left edge(or bottom
 edge, in case of horizontal box plot), the third quartile forms the right
 edge (or top edge) of the box and the median is drawed inside of the box.
 The box itself represents interquartile range (IQR), that is calculated
 as 
\begin_inset Formula $IQR=Q_{3}-Q_{1}$
\end_inset

.
 The lines that lie beyond the box are called 
\emph on
whiskers
\emph default
 and indicate a range for 
\begin_inset Quotes sld
\end_inset

a bulk of the data
\begin_inset Quotes srd
\end_inset

.
 The whiskers extend to the furthest points outside of the box, except they
 cannot be longer than 1,5 times the IQR.
 The values lying outside of the whiskers are considered outliers.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Think if it makes sense to include explanation of boxplot.
 Probably not.
 It is a common knowledge I think.
 So remove the explanation.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/PTIME_box.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Box plot for the survival time
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "box_surv_time"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Look at the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "box_surv_time"
plural "false"
caps "false"
noprefix "false"

\end_inset

, there you can see the box plot of patient survival time (PTIME column).
 The 
\begin_inset Formula $Q_{1}$
\end_inset

is equal to 867 days (2.4 years), the median is 2136 days (5.85 years) and
 the third quantile 
\begin_inset Formula $Q_{3}$
\end_inset

is equal to 3828 days (10.5 years).
 The interquartile range (IQR) is equal to 2961 days.
 This makes up the box.
 The left whisker extends from 0 day up to the first quartile of 867 days.
 The right whisker is much longer, and extends from the third quartile up
 to the 
\begin_inset Formula $Q_{3}+1.5*IQR$
\end_inset

, which in our case is equal to 8269,5 days.
 The values above 8269,5 can be considered outliers.
 As can be seen from the figure (almost straight black line, consisting
 of individual dots) there are a lot of them.There are less than 
\series bold
10 000
\series default
 (
\series bold
get the actual value
\series default
) outliers, which is not that much compared to the 490 000 of total kidney
 transplantations.
 It is not the wisest choice to simply remove the outliers, as they still
 might have useful information to the model.
 However it has to be estimated experimentally.
 The handling of outliers will be described in the section dedicated to
 the dataset building.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/bar_PSTATUS.png
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Bar chart of PSTATUS colum that provides information on censoring.
 0 corresponds to censored instance, 1 corresponds to event happening.
 There are 308823 censored instances, and 181349 times event happened.
\begin_inset CommandInset label
LatexCommand label
name "fig:bar_pstatus"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Look at the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bar_pstatus"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where there is plotted the distribution of the PSTATUS column values.
 0 corresponds to censored instances, 1 corresponds to event happening.
 As can be seen, the distribution is quite uneven, and there is much larger
 amount of censored instances than ones with the event happening, as there
 are 308 823 censored instances and only 181 349 non-censored ones.
 The percentage of censoring is 63%, which is quite high.
\end_layout

\begin_layout Subsection
Age
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Goals of this subsection:
\end_layout

\begin_layout Plain Layout
- explore how donor age and recipient age influence survival
\end_layout

\begin_layout Plain Layout
- look into the literature: what does it say
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this subsection we are going to explore both the donor and recipint ages
 in the dataset, and will see what the literature tells about their importance
 to long-term survival.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/box_age_vs_age_don.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Box plot for the recipient age versus donor age.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:box_age_vs_age_don"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As can be seen on the box plot in the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:box_age_vs_age_don"
plural "false"
caps "false"
noprefix "false"

\end_inset

, in this dataset donors are usually younger than the recipients by median
 11 years.
 Median recipient age in the dataset is 50, median donor age is 39.
 The vast majority, 75% of the recipients aged lie between 38 and 60 years
 old, making IQR of 22, while 75% of the donor age lie between 26 and 50
 years old, making IQR of 24.
 Interestingly, there are not many outliers, as the whiskers cover ages
 from about 5 to 93, covering the most of human life range.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/hex_bin_age_vs_surv_time.png
	scale 60

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Hexagonal binning for the recipient age versus survival time.
 The brighter the color of the hexagon is, the more instances lie in it.
\begin_inset CommandInset label
LatexCommand label
name "fig:hb_age_ptime"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:hb_age_ptime"
plural "false"
caps "false"
noprefix "false"

\end_inset

 you can see the hexagonal binning for the recipient age vs.
 the survival time.
 The hexagonal binning is a substitute for a scatter plot for large datasets,
 as scatter plots do not handle large data sets very well.
 The hexagonal binning plot consists of colored hexagons, and the darker
 the color is, the more instances lie in it.
\end_layout

\begin_layout Standard
The figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:hb_age_ptime"
plural "false"
caps "false"
noprefix "false"

\end_inset

 more or less corresponds to box plots 
\begin_inset CommandInset ref
LatexCommand ref
reference "box_surv_time"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:box_age_vs_age_don"
plural "false"
caps "false"
noprefix "false"

\end_inset

, as the majority of colored hexons lie in box ranges for age and the survival
 time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
mozna hexagonal binning plot je k nicemu - neukazuje relaci mezi vekem a
 dobou preziti, to je proste mnozstvi instanci
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Donor Type
\end_layout

\begin_layout Standard
In this subsection we are going to explore the influence of donor type (living
 or diseased) on the survival.
 Recipients with kidneys from living donors live longer, this is a well
 established fact
\begin_inset CommandInset citation
LatexCommand cite
key "key-29"
literal "false"

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-30"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add citation from some meta analysis
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Let's look at the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_donor_types"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where are plotted two Kaplan-Meier survival curves for all patients from
 the dataset, without taking into account other covatiates.
 On the graph we can see that the survival probability of living donor transplan
t is indeed significantly higher than the survival probability of deceased.
\end_layout

\begin_layout Standard
This is the case because often there is no time to make full HLA screening,
 that may allow for HLA mismatches.
 Additionally, deceased transplants may suffer from mild kidney damage due
 to the delay in transplantation.
 While living donor transplants are often performed between siblings that
 have similar HLA, that creates better compativility.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/Donor_type.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir survival curve donor types
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_donor_types"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Gender
\end_layout

\begin_layout Standard
In this subsection we are going to explore the gender distribution in our
 dataset, and its influence on survival.
\end_layout

\begin_layout Standard
Let's explore the distribution of gender in our dataset.
 Take a look at the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bar_gender"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are 297279 men and 192882 women - the 35% difference.
 Despite the fact that the chronic kidney disease is more common in women,
 the end stage kidney failure and therefore kidney transplantation is more
 common in men
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/bar_gender.png
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Bar chart for gender 
\begin_inset CommandInset label
LatexCommand label
name "fig:bar_gender"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Let's take a look at gender's influence on survival.
 In the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_sex"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see the Kaplan-Meier survival curves for men and women on the whole
 dataset.
 As can be seen from the graph, females generally have less risk than their
 male counterparts.
 Women usually live longer 
\begin_inset CommandInset citation
LatexCommand cite
key "key-14"
literal "false"

\end_inset

.
 Quite signifficant factor is the difference between male and female immune
 responses - males ususally have greater risk to get an infection, than
 females, and the intesity of the infection is higher
\begin_inset CommandInset citation
LatexCommand cite
key "key-15"
literal "false"

\end_inset

.
 Furthermore, the influence of immunosuppresants make the problem of infection
 even worse.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/Gender.png
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir survival curve for genders
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_sex"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The Use of Dialysis
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
OFC find paper, altough it literature it was referred to as common knowledge
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this subsection we are going to explore the influence of dialysis on
 survival.
 In the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_dialysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see two survival curves for the patients who were on dialysis, and
 for those who were not.
 The whole dataset was used.
 As can be seen in the Figure, the patients who were on dialysis before
 the transplantation have a greater risk, while those who were not.
 This agrees with .....
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-33"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
add some meta analysis or link to a textbook 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Why exactly this is the case, unfortunately, I was not able to find.
 The literatures just states it as fact.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/Dialysis.png
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir survival curve for using dialysis or not
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_dialysis"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Race
\end_layout

\begin_layout Standard
In this subsection we are going to explore the survival curves for different
 ethnic groups.
 In the Figure 
\begin_inset CommandInset citation
LatexCommand cite
key "key-31"
literal "false"

\end_inset

 are plotted 7 survival curves for different ethnicities.
 As can be seen on the image, 5 ethnicities share about the same survival
 probability during the cource of 8000 days, while two of them differ substantia
lly from the other.
 White americans had higher survival probability than other ethnical groups,
 later converging to the others.
 This corresponds to (
\begin_inset CommandInset citation
LatexCommand cite
key "key-31"
literal "false"

\end_inset

) examining graft survival.
 
\end_layout

\begin_layout Standard
The least survival probability had multiracial group.
 Their number, however, was the lowest - only 1849 instances, so it is not
 enough to make any conclusions.They might be later removed, as the definition
 is too vague and there are not many instances.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/Kaplan-Meier/Race.png
	scale 55

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir survival curve for ethnicities
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_race"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Dataset building, Exclusion criteria and noice reduction
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Exclude
\end_layout

\begin_layout Itemize
children
\end_layout

\begin_layout Itemize
people who died from unrelated reasons (suicide, trauma, etc.)
\end_layout

\begin_layout Plain Layout
Divide the dataset into two parts: deceased and living
\end_layout

\begin_layout Plain Layout
handling missing values: rows with missing values are just removed, as there's
 a lot of data.
\end_layout

\begin_layout Plain Layout
list features used for living and deceased
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Machine Learning Model
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Combine this chapter with the data load and processing.
 As it makes more sense to include all practical things from data science
 into one chapter.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
In this chapter I will:
\end_layout

\begin_layout Itemize
Explain how do I approach the problem of estimating the survival time
\end_layout

\begin_layout Itemize
Explain how do I approach the model selection
\end_layout

\begin_layout Itemize
Explain the way of fine-tuning the selected model
\end_layout

\begin_layout Itemize
Speculate why some models perform better than the others.
\end_layout

\begin_layout Itemize
Explain my choice for the demographics in the dataset
\end_layout

\end_inset


\end_layout

\begin_layout Section
Problem Formulation
\end_layout

\begin_layout Standard
Predicting the survival time after a successful kidney transplant can be
 approached in three ways: as a regression problem, classification problem,
 or through survival analysis.
 
\end_layout

\begin_layout Standard
A 
\emph on
regression
\emph default
 model may seem an intuitive choice, as we want to predict a numerical value
 – the survival time.
 But it is not the best option for the following reasons:
\end_layout

\begin_layout Standard
1.
 
\series bold
The censored dataset
\series default
.
 The dataset has a high level of censoring – 76%.
 The dataset contains the number of days survived, along with the survival
 status.
 Including both living and deceased patients would introduce too much noise
 to the model, making it highly inaccurate.
 It is impossible to predict the number of days survived with regression
 methods based on a dataset comprised of both living and deceased patients.
 
\end_layout

\begin_layout Standard
2.
 
\series bold
Censoring removal would produce bias
\series default
 
\series bold
and signifficantly reduce the dataset
\series default
.
 We could remove all censored instances, but that would reduce the dataset
 from 500 000 to roughly 120 000 examples.
 It would also introduce significant bias, as the dataset would contain
 only deceased patients, and most of them passed away before the introduction
 of modern techniques for treating the rejection.
 As a result, the model created from such a dataset would be highly inaccurate.
 
\end_layout

\begin_layout Standard
3.
 
\series bold
Regression predicts only one single number.

\series default
 It poses a problem, especially over extended time frames, as there are
 too many factors that we can’t account for, leading to incorrect predictions.
 
\end_layout

\begin_layout Standard
Another way of formulating the problem is 
\emph on
classification
\emph default
.
 We can theoretically divide the dataset into groups: ”less than one year”,
 ”one to five years”, ”five and more”, or even more groups and train a classifie
r based on them, as it was done by ....
 et al..
 And again, we would face problems of censoring and bias mentioned above.
 So the classification is not the best option as well.
 
\end_layout

\begin_layout Standard
A more appropriate way of problem formulation is in terms of 
\emph on
survival analysis
\emph default
.
 Survival analysis methods handle censoring and provide a better form of
 prediction: survival function or hazard function, which represents survival
 probability or the failure rate at each moment in time, respectively.
\begin_inset Note Note
status open

\begin_layout Plain Layout
vic to rozvest
\end_layout

\end_inset


\end_layout

\begin_layout Section
Model selection
\end_layout

\begin_layout Standard
The algorithms provided by the scikit-survival do not handle large datasets
 very well (never ending training process and worse results probably due
 to the noise) that is why I chose to train different models for different
 demographics, as one specific model for one specific demographic will perform
 better that one model trained for all demographics.
 In addition, the living donor transplantation differs a bit from the diseased
 transplantation, that might introduce some noice into the model.
\end_layout

\begin_layout Standard
The way I approach the model selection model automation with the class SurvivalE
stimators defined in 
\begin_inset CommandInset href
LatexCommand href
name "estimator_automation.py"
target "https://github.com/krllstdn/BachelorProject/blob/main/Code/surv_data_pipeline/estimator_automation.py"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Run the following class and short list the most promising models.
 In this case it is survival gradient boosting and random survival forests.
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
it is not correct to compare deceased model performance with the living
 model performance.
 better to compare living models with living models and deceased models
 with deceased.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Goals of this section:
\end_layout

\begin_layout Plain Layout
- explore Coxnet and RSF for living and deceased subsets
\end_layout

\begin_layout Plain Layout
- evaluate them: AUC, brier and Uno's c_index
\end_layout

\begin_layout Plain Layout
- compare them
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In this section we are going to discuss two models, Coxnet and Random Survival
 Forest.
 These models were chosen because they both are able to generate survival
 functions in the scikit-survival library, unlike others that only estimate
 the risk score.
 Unfortunately, the older version of scikit-survival was used - 0.14.0, due
 to the limitation in python version on cluster where these models were
 trained.
 The survival function will later be used in the application KidneyLife
 to visually illustrate the probability of survival in each moment in time.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Explain why do I use three performance measures.
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Coxnet
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
mention that the timeframe is not random - 10th to 90th percentile
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Coxnet, or an elastic net, is a linear model, so it is fast even with large
 datasets, a bit worse results, compared to the Random survival forest.
 It makes prediction both in a form of the risk score or a survival function.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Living
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.705
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.135
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.704
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Deceased
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.681
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.165
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.714
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Coxnet performance on the test set
\begin_inset CommandInset label
LatexCommand label
name "tab:Coxnet-performance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As can be seen in the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Coxnet-performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the Coxnet performed the best for the living group, the worst for the
 deceased, and somewhere in between for the both living and deceased.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/coxnet_auc.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
AUC curve for coxnet 
\begin_inset CommandInset label
LatexCommand label
name "fig:AUC_coxnet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Discussion of the AUC figure
\series default
: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
- living model is a bit more precise, although in some timeframes it performs
 a bit worse
\end_layout

\end_inset


\end_layout

\begin_layout Standard
As was covered in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Training-and-Hyperparameter-tuning"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the hyperparameter tuning is usually performed with either GridSearch
 or RandomizedSearch.
 Unfortunately, the older Python version on the cluster did not allow to
 install the newest version of scikit-survival, where the GridSearch was
 implemented.
 So, the hyperparamer tuning was performed with a custom script that is
 designed to imitate the GridSearchCV but without the k-fold cross-validation.
 The script optimizes for the Integrated Brier Score (IBS) that directly
 tells the accuracy of predictions.
 The script can be found: here
\begin_inset Note Note
status open

\begin_layout Plain Layout
(
\series bold
add link
\series default
)
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/coxnet_brier.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Time-dependent Brier score for the Coxnet
\begin_inset CommandInset label
LatexCommand label
name "fig:Brier-coxnet"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Discussion of the Brier Figure:
\begin_inset Note Note
status open

\begin_layout Plain Layout

\series bold
- coxnet is much more precise with the living as the lower the area , the
 better
\end_layout

\begin_layout Plain Layout
- 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet (standard hyperparams, living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.668
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet (the best hyperparams, living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.705
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.135
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.704
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Cox before and after fine tuning.
\begin_inset CommandInset label
LatexCommand label
name "tab:cox-finetuning"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The coxnet has only two hyperparameters: L1 ratio and alpha, the latter
 is calculated by fitting the model and we then need to choose the best
 of them.
 L1 ratio defines the relative weight of the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalty.
\end_layout

\begin_layout Standard
On the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Coxnet-Feature-Importance"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see the importances of features for the prediction.
 Features with zero influence were omitted.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Feature description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Importance
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Abbreviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor Age
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient Age
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor Type
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor gender
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient gender
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor blood group
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient blood group
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient on dyalisis
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient creatinine at the time of tx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Coxnet Feature Importance
\begin_inset CommandInset label
LatexCommand label
name "tab:Coxnet-Feature-Importance"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Random Survival Forest
\end_layout

\begin_layout Standard
Random survival forest is a powerful ensemble machine learning algorithm,
 that is comprised of multiple submodels, and therefore it takes a lot of
 time to train, a lot of time to make a prediction, depending on the selection
 of hyperparameters, making it a bit difficult to work with, especially
 during the hyperparameter tuning.
 Extremely memory hungry.
 The prediction is a survival function or a hazard score.
 It was covered in detail in (
\series bold
RSF subsection
\series default
).
 The living and deceased subsets had 34951 and 
\series bold
70 000 
\series default
instances respectively.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Living
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.727
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.129
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.743
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Deceased
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.678
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Random Survival Forest performance on the test set
\begin_inset CommandInset label
LatexCommand label
name "tab:RSF_performance"

\end_inset

 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As can be seen from the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:RSF_performance"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the survival forest performed the best for the living group, the worst
 for the deceased, and somewhere in between for the both living and deceased,
 just as it was in case with the Coxnet.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/rsf_auc.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
AUC for RSF
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add a deceased curve
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:AUC_RSF"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
discussion of AUC image
\series default
: 
\begin_inset Note Note
status open

\begin_layout Plain Layout
a bit volatile, but generally increases over time -> model's discriminative
 ability increases over time, but does not tell much about model's performance
\end_layout

\begin_layout Plain Layout
interestingly, it starts to grow faster after the 75th percentile (3800
 days)
\end_layout

\begin_layout Plain Layout
- the probability that the test will correctly classify a randomly chosen
 positive instance (an event has occurred) as more likely than a randomly
 chosen negative one (an event has not occurred).
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/brier_rsf.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Time-dependent Brier score for the Random Survival Forest
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add a deceased curve
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:Brier_rsf"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
discussion of the Brier imag
\series default
e: On the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Brier_rsf"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is plotted the time dependent Brier score for the Random Survival Forest.
 As we can see, it increases over time, meaning that predictions get worse
 over time.
 This totally makes sense, as the more time passes after the transplant,
 the less pretransplant information, that was used for training, has influence
 on the survival.
\end_layout

\begin_layout Standard
The fine-tuning was performed with a custom script, described in the previous
 subsection dedicated to the coxnet.
 The hyperparameters that were fine-tuned are the following: n_estimators,
 max_depth, min_sample_split and max_features.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest (standard hyperparams, living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.708
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.160
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest (the best hyperparams, living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.129
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.743
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
RSF before and after fine tuning.
\begin_inset CommandInset label
LatexCommand label
name "tab:RSF-finetuning"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
On the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:RSF-finetuning"
plural "false"
caps "false"
noprefix "false"

\end_inset

 we can see the performance of regular RSF with the standard hyperparameters
 compared to the performance of RSF with fine-tuned hyperparams.
 
\end_layout

\begin_layout Standard
Feature importance can be seen on the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Feature-Importance-for-RSF"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Write some discussion.
\end_layout

\begin_layout Plain Layout
- plot feature importance as a graph
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="10" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Feature description
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Importance
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Abbreviation
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor Age
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient Age
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor Type
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor gender
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient gender
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Donor blood group
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient blood group
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient on dyalisis
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Recipient creatinine at the time of tx
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Feature Importance for RSF
\begin_inset CommandInset label
LatexCommand label
name "tab:Feature-Importance-for-RSF"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Comparison
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest (living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.129
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.742
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet (living)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.705
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.135
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.704
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest (deceased)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet (deceased)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.681
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.165
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.714
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model comparison on the test set
\begin_inset CommandInset label
LatexCommand label
name "tab:Coxnet_vs_RSF"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
*image with the two BS and AUC graphs for each model*
\end_layout

\begin_layout Standard
As can be seen on the table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:Coxnet_vs_RSF"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and on the image ..., the Random survival forest performed better than the
 elastic net.
 However, there are some differences in timeframes, where at certain points
 in time the coxnet performed better than the RSF.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
plot the survival functions of multiple predictions and draw a red line
 of actual survival.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
AUC RSF vs.
 Coxnet:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/auc_rsf_vs_coxnet.png
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
AUC: Coxnet vs.
 RSF, Living 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Add a deceased curve
\end_layout

\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
the trends are the same, but hte rsf is higher
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Brier Coxnet vs.
 RSF
\series default
: 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/brier_rsf_vs_coxnet.png
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Brier: Coxnet vs.
 RSF, Living
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
- the flatter, the better
\end_layout

\begin_layout Plain Layout
- the lower, the better
\end_layout

\end_inset


\end_layout

\begin_layout Section
Scoring algorithm
\end_layout

\begin_layout Standard
the cumulative hazard suits the place of transplantation score very well
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
- risk score that is predicted with the model
\end_layout

\begin_layout Plain Layout
- mention txmatching scoring (idk what to say exactly)
\end_layout

\begin_layout Plain Layout
- make up the formula: find out how the score looks like for the worst and
 the best instances.
 create formula to convert the predicted risk score to a number 0-100, with
 0 the worst and 100 ideal transplantation.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Limitations
\end_layout

\begin_layout Standard
these models probably aren't suitable for KEP (check results), bc they're
 slow, but are good for prediction estimated survival and when it is best
 to intervene.
\end_layout

\begin_layout Standard
the RSF deceased could be better fine-tuned
\end_layout

\begin_layout Section
Further work
\end_layout

\begin_layout Standard
more thorough hyperparameter tuning, especially with the RSF on the deceased
 dataset.
\end_layout

\begin_layout Standard
another model for follow up data
\end_layout

\begin_layout Standard
deep survival neural network
\end_layout

\begin_layout Chapter
Applications
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
we realize that it is not a part of the task, but what good is a model if
 one cannot use it.
 We highly doublt that anyone would be comfortable with entering data in
 the jupyter notebook and making predictions there.
 So it was decided to create an application for it.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Existing Solutions
\end_layout

\begin_layout Subsection
Txmatching
\end_layout

\begin_layout Standard
Txmatching is a software that allows kidney pair donation centre to find
 the best possible matches from a pool of patients.
 It is not always possible to find a suitable donor in the social circle
 of the patient.
 In this case it is possible to register the patient and a person who is
 willing to donate their kidney, but can't do so due to the incompatibility,
 into Kidney Exchange Program (KEP), or 
\emph on
kidney paired donation
\emph default
 (KPD).
 KEP is an approach to living donor transplantation where pairs with incompatibl
e donors exchange their kidneys and get a compatible one.
 There can be multiple pairs, but the higher the number of pairs, the more
 risk that someone will refuse to donate their kidney and the whole sequence
 of donations would be compromised.
 The example of how it works is the follows: A, B, C are donor recipient
 pairs.
 Donor A gives their kidney to recipient B, donor B gives theirs kidney
 to recipient C, donor C gives their kidney to recipient A.
 This sequence of transplantations is called 
\emph on
loop.

\emph default
 
\end_layout

\begin_layout Standard
There are cases when an altruistic donor is availble, that is they are willing
 to donate kidney to any compatible donor and they are not bound to any
 recipient.
 With such donors there is a special sequence of transplantations called
 
\emph on
chain, 
\emph default
and it goes only forward.
\end_layout

\begin_layout Standard
What Txmatching does is iterates through the database of such donor-recipient
 pairs and altruistic donors and tries to find the best possible loops and
 chains with an optimisation algorithm.
\end_layout

\begin_layout Section
KidneyLife
\end_layout

\begin_layout Standard
It was not a part of the task to create any kind of software, however, we
 think it is only right to make something that will allow anyone to use
 our models, as entering data into the jupyter notebook just to make a predictio
n
\end_layout

\begin_layout Standard
Our supervizors originally suggested to create a module for Txmatching using
 the developed models.
 However, ue to different project philosophy and different kind of data
 that Txmatching uses, it was decided to create a whole separate application.
 In the following section we will present the application, its architecture,
 how does it work, the process of its development and what can be done further.
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
We present KidneyLife.
 KidneyLife is an inference application, an application dedicated solely
 to the user-friedly use of machine learning models.
 It is designed to assist doctors in decision making by estimaing recipient's
 lifespan after kidney transplantation.
 On the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kidney-Life-UI"
plural "false"
caps "false"
noprefix "false"

\end_inset

 you can see KidneyLife's user interface.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/KidneyLife.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kidney-Life-UI"

\end_inset

Kidney Life UI
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
On the left you can see the form for the input of the data.
 On the top of the form you can see big select field for the selection of
 a model.
 The form contains select inputs for the input of categorical features and
 inputs of type text for manual input of numerical data.
 On the bottom of the form you can see the Submit button which sends the
 data to the server to make a prediction, but before it does that, it performs
 checks if the data was inputted correctly and there is no missing values.
 More on the validation and the details of the front end application you
 can find in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the 
\begin_inset Quotes sld
\end_inset

Front End
\begin_inset Quotes srd
\end_inset

 subsection.
 About the UI you can learn more at 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Functionality-and-UX"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 More on the prediction, you can find in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the 
\begin_inset Quotes sld
\end_inset

Back End
\begin_inset Quotes srd
\end_inset

 subsection.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
two identical sentences.
 make it to not repeat
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Under the Submit button you can find a button that fetches synthetical data
 from the server.
 This was done for the better showcase of the model.
 So that everyone can test the model in one click without the chore of entering
 the data manually.
 The details of the synthetic data generation you can find find in Section
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the 
\begin_inset Quotes sld
\end_inset

Back End
\begin_inset Quotes srd
\end_inset

 subsection.
\end_layout

\begin_layout Standard
It is important to note that the application should be looked at as a 
\emph on
minimum viable product
\emph default
 (MVP), an application that provides the minimum usable set of features
 to users for future feedback.
 It was done so, because the complexity of development started to grow too
 fast and it was unclear whether anyone will be interested in the application
 or the provided features were at all what the potential users would want.
 It would be a shame to develop a big application only to find out that
 it is not what the users want.
 In the Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Future-Work"
plural "false"
caps "false"
noprefix "false"

\end_inset

 dedicated to the future work you can see what the application was conceptualize
d initially and how it can be developed further.
\end_layout

\begin_layout Subsection
Architecture 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Architecture"

\end_inset


\end_layout

\begin_layout Standard
The application was built with docker compose.
 The docker compose stack consists of backend, frontend and reverse proxy
 services.
 Docker is a software platform that allows developers to package their applicati
ons in containers, a standard unit of software that bundles code and its
 dependencies to ensure fast and reliable execution across various computing
 environments 
\begin_inset CommandInset citation
LatexCommand cite
key "key-39"
literal "false"

\end_inset

.
 Docker compose is a utitlity for building and destributing multi-container
 applications.
 Each individual container within the docker compose application stack is
 called service.
\end_layout

\begin_layout Paragraph

\series bold
Front End
\end_layout

\begin_layout Standard
The front end service contains a front end application developed with React,
 the most popular Java Script front end framework developed in Facebook
 at 2013.
 Front end stands for the software that makes up a user interface.
 
\end_layout

\begin_layout Standard
The architecture of the front end application is rather simple, as it consists
 only from a few components.
 Component is an independent and reusable bit of code that takes form of
 JS function that returns HTML.
 The application consists of main page compontent that contains all other
 components and all logic related to the storing data and sending requests.
 Input component renders the input type text.
 Any characters except for numbers and a dot are replaced with an empty
 string.
 It was made so, as the input type number provided inconsistent behaviour:
 it worked as expected in browsers based on Chromium, such as Brave and
 Chrome, but allowed other characters in other browsers, such as Firefox
 and Safari.
 Moreover, input type number allows character 
\begin_inset Quotes sld
\end_inset

e
\begin_inset Quotes srd
\end_inset

, as it is a part of scientific notation.
 In addition to that, it contained unaesthetic arrows for increasing and
 decreasing the number, that was not possible to style.
\end_layout

\begin_layout Standard
Select component renders the input type select, that allows to select value
 from provided values which is especially useful for categorical values.
 Then there is the ModalContainer which is a wrapper and a background for
 the form.
 And the final major component is the PlotComponent, which naturally renders
 the interactive plot from Plotly.js package.
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Rename the components to normal names and put them here
\end_layout

\end_inset


\end_layout

\begin_layout Standard
We implemented the front end validation.
 Naturally, first of all it checks if the value was indeed provided.
 It has the min/max value validation, that is the value must fall into a
 specific range.
 There is float/integer validation, depending which data type the field
 needs.
 Finally it has a check if the selected categorical value is indeed from
 the provided values, to prevent user from adding his own category using
 the browser build-in developer tools and sending the request to the server.
 
\end_layout

\begin_layout Standard
The styling was made with Tailwind a CSS library that signifficantly easies
 the process of transforming the design into code.
 Because with vanilla CSS the process is cumbersome and timeconsuming.
\end_layout

\begin_layout Standard
A part of the front end service build is build of React code to production
 ready optimized code that is later served with the nginx server.
\end_layout

\begin_layout Paragraph

\series bold
Back End
\end_layout

\begin_layout Standard
For backend was used Django Rest Framework, a popular Python web framework
 for building RESTful APIs.
 An 
\emph on
application program interface
\emph default
 (API) is a set of guidelines that dictate the procedures for connectivity
 and data exchange between applications or devices, while a REST API specificall
y adheres to the principles of the 
\emph on
representational state transfer
\emph default
 (REST) architectural style.
 Essentially, API serves as a mechanism of accessing resourse in one application
 or service from another application or service.
 Application or service that is accessesing the resource is called 
\emph on
client
\emph default
, application or service providing the resource is reffered to as server.
 The resources we are accessing here are the machine learning models trained
 previously.
 They were serialized or 
\begin_inset Quotes sld
\end_inset

pickled
\begin_inset Quotes srd
\end_inset

 with Python package 
\emph on
pickle
\emph default
 and are deserialized when we need to make a prediction.
\end_layout

\begin_layout Standard
The back end REST API has two endpoints: one for prediction and the second
 is for the synthetic data generation.
 The prediction endpoint expects the request body with two parameters: the
 model name and the data dictionary.
 It then validates if the data is valid and then loads the model, passes
 data through the data pipeline, that makes data usable by the model, loads
 the model, makes prediction and then sends the results to the front end
 as a response.
\end_layout

\begin_layout Standard
The synthetic data generation endpoint has only one parameter - the model
 name.
 When the request is sent it loads the proper model description JSON document
 looks at its statistical data, particularly 10th and 90th quantiles for
 numerical data and the frequencies for categorical data.
 Then under an assumption of normal distribution we generate data for each
 numerical feature and under given value frequencies the values for catgeorical
 features.
 We use these quantiles to avoid peculiar situations when minimal values
 were generated along with the maximal (e.g.
 it could generate 6 year old donor and 90 year old recipient).
\end_layout

\begin_layout Paragraph*

\series bold
Reverse proxy
\end_layout

\begin_layout Standard
Even though the front end code is located in the container on the server,
 it is still executed in user's browser.
 Cross Origin Resource Sharing (CORS) is a mechanism that allows an application
 on one URL to request data from another URL.
 Browser implements Same-origin policy as a part of its security measures.
 It allows request data from its own URL and blocks requests from other
 URLs unless certain conditions are met.
 When browser sends request to the server it has Origin property in its
 header.
 The server adds header Access-Control-Allow-Origin to its response.
 If Origin and Access-Control-Allow-Origin are not the same, the browser
 will prevent the server from sharing the contents of the response with
 the client.
 
\end_layout

\begin_layout Standard
It is not possible, however, to indentify or specify the URL of each user,
 nor is it secure to allow calling the request from all sources.
 To solve this problem we use the reverse proxy.
 A 
\emph on
reverse proxy
\emph default
 is an intermediary server that directs the client request to the appropriate
 backend server 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 When we access the applications's URL we get redirected to the front end.
 When we want to make a prediction, we make a request to the proxy and it
 redirects the request to the back end, we get our prediction and render
 it on the front end.
 This schema is illustrated on the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Application-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
In addition to the handling of the CORS, reverse proxy increases security,
 as it serves as a barier between the internet and the backend.
 It also increases application's scalability, as we can create multiple
 backends on different servers and balance the load between them.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/AppDiagram.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Application-architecture"

\end_inset

Application architecture.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deployment
\end_layout

\begin_layout Standard
First of all, to be able to deploy the app we need a place to deploy it
 to.
 We rent 
\emph on
virtual private server
\emph default
 (VPS), or a virtual machine on the cloud.
 Initially we used Google Cloud on the free trial, but it turned out to
 be too expensive so we later migrated to another VPS hosting provider.
\end_layout

\begin_layout Standard
After we secured the VPS, we bought the domain name 
\begin_inset Quotes sld
\end_inset

kidneylife.site
\begin_inset Quotes srd
\end_inset

 from the domain registrar Namecheap.
 Then we registered the domain in the 
\emph on
domain name system
\emph default
 (DNS), associating the VPS' IP address with the acquired domain.
\end_layout

\begin_layout Standard
Then the application was deployed to the internet using a Shell script.
 It goes through the following steps:
\end_layout

\begin_layout Enumerate
Check if the connection to the VPS can be made
\end_layout

\begin_layout Enumerate
Zip the application folder
\end_layout

\begin_layout Enumerate
Send the zip to the VPS
\end_layout

\begin_layout Enumerate
Unzip the folder
\end_layout

\begin_layout Enumerate
Build and run the docker compose stack with the production environment variables.
\end_layout

\begin_layout Subsection
Functionality and UX
\begin_inset CommandInset label
LatexCommand label
name "subsec:Functionality-and-UX"

\end_inset


\end_layout

\begin_layout Standard
Kidney life is a standard machine learning 
\emph on
inference
\emph default
 application which stands for an application whose solely purpose is the
 user-friendly access to the machine learning model.
 As such, it is an one-page application that has select input with two models,
 the inputs are rendered based on the selected model.
 And the interactive graph made with Plotly.js.
 It also features a button 
\begin_inset Quotes sld
\end_inset

generate synthetic data
\begin_inset Quotes srd
\end_inset

 made for testing and illustration purposes.
\end_layout

\begin_layout Standard
The design of the application was crafted with consideration of the following
 design principles.
 Shades of blue were chosen because of its calming effects and its assosiation
 with reliability.
 Moreover blue is generally a safe color as it appeals to larger audience.
 Simplicity, which is one of the main principles of the modern web design,
 was naturally prioritized as inference applications inherently have little
 complexity.
 Furthermore, the larger margins and paddings were employed to provide breathing
 room within the interface, preventing it from feeling overcrowded.
 Lastly, the design exhibits rensponsiveness, accomodating a wide range
 of devices, from tablets to large 2k monitors, delivering consistent and
 comfortable user experience across different screen sizes.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
figma
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Future Work 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Future-Work"

\end_inset


\end_layout

\begin_layout Standard
Moving forward, there are several avenues for enhancing the application's
 functionality and user experience.
 Firstly, the integration of additional machine learning models stands as
 a promising avenue for enriching the predictive capabilities of the application.
 In this work only models from Sci-Kit survival were used.
 However it is not the only package for survival analysis in Python, there
 is also 
\emph on
lifelines
\emph default
.
 By incorporating a diverse range of models the application can provide
 more comprehensive and accurate predictions, thus offering greater value
 to users.
\end_layout

\begin_layout Standard
Secondly, implementing a robust login system is essential to ensure secure
 access to the application's features and confidential data.
 It is particularly important if we are to follow through with the features
 outlined in the subsequent paragraphs.
 By introducing user authentication mechanisms, such as username-password
 authentication or OAuth, the application can restrict access to unauthorized
 individuals, thereby safeguarding sensitive data associated with subsequent
 features.
\end_layout

\begin_layout Standard
Moreover, the introduction of patient (recipient and donor) and pair management
 systems can improve user experience, as it will allow to save the predictions
 for the provided pair of patients and model in the database, and the user
 will be able to look at the predicted survival curve later only with a
 couple of clicks instead of entering the data manually every time.
 You can see how it can look like on the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Patient-MS"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/KL_Patient_MS.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Patient-MS"

\end_inset

Patient and pair management system concept
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Furthermore, the integration of FHIR HL7 standard would present an opportunity
 to enhance interoperability and data exchange between the application and
 hospital's ecosystem.
 FHIR, which stands for 
\emph on
Fast Healthcare Interoperability Resources
\emph default
 (FHIR) is a 
\emph on
Health Level Seven 
\emph default
(HL7) standard for the electronic exchange of health care data.
 It is important to note that while only a few hospital systems support
 FHIR HL7, its adoption is seen as the future of healthcare data exchange.
\end_layout

\begin_layout Standard
If any of these changes are to take place, to prevent vulnerabilities in
 software delivery we need to add continuous integration (CI) and continuous
 delivery (CD).
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
tell what the CI/CD is
\end_layout

\end_inset

 While the the deployment has already been automated the testing has not.
 With Git Hub Actions we can test the application automatically on the server
 every time the changes are pushed to Git Hub and prevent the Pull Request
 from merging into Main if the tests have failed.
 The tests for the backend already exist.
 So the task would be only to automate it with Git Hub Actions.
\end_layout

\begin_layout Standard
If you are interested how would it look like, you can visit the project's
 Figma page to see the early stages of the design.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/KidneyLifeDraft.png
	scale 40

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kidney-Life-Dashboard"

\end_inset

Kidney Life Dashboard page concept
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
add a link to the figma to illustrate how it can look like.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
In the later stages of development it was found out that nonprofit organisation
 with the name KidneyLife already exist.
\end_layout

\begin_layout Plain Layout
If someone will be interested in the application, it would be necessary
 to rebrand and rename the application, to avoid confusion and possible
 legal problems.
\end_layout

\end_inset


\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Text of the conclusion\SpecialChar ldots

\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Knechtle, S.
 J., Marson, L.
 P., & Morris, P.
 (2019).
 Kidney transplantation - principles and practice: Expert consult - online
 and print (8th ed.).
 Elsevier - Health Sciences Division
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"
literal "false"

\end_inset

Nobel prize in physiology or medicine (2022) Our Scientists.
 Available at: https://www.rockefeller.edu/our-scientists/alexis-carrel/2565-nobel
-prize/ (Accessed: February 6, 2023).
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"
literal "false"

\end_inset

Barker, C.
 F., & Markmann, J.
 F.
 (2013).
 Historical Overview of Transplantation.
 Cold Spring Harbor Perspectives in Medicine, 3(4).
 https://doi.org/10.1101/cshperspect.a014977
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"
literal "false"

\end_inset

Matevossian, Edouard, et al.
 "Surgeon Yurii Voronoy (1895-1961)-a pioneer in the history of clinical
 transplantation: in memoriam at the 75th anniversary of the first human
 kidney transplantation." Transplant International 22.12 (2009): 1132.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"
literal "false"

\end_inset

PUNT, Jenni et al.
 Kuby immunology.
 Eight.
 vyd.
 New York: Macmillan Education, 2019.
 ISBN 9781319114701;1319114709;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"
literal "false"

\end_inset

ABBAS, Abul K., Andrew H.
 LICHTMAN a Shiv PILLAI.
 Basic immunology: functions and disorders of the immune system.
 Sixth.
 vyd.
 Philadelphia: Elsevier, 2020.
 ISBN 9780323549431;0323549438;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"
literal "false"

\end_inset

NCI Dictionary of Cancer terms (no date) National Cancer Institute.
 Available at: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/a
bo-blood-group-system (Accessed: March 6, 2023).
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"
literal "false"

\end_inset

Dean L.
 Blood Groups and Red Cell Antigens [Internet].
 Bethesda (MD): National Center for Biotechnology Information (US); 2005.
 Chapter 2, Blood group antigens are surface markers on the red blood cell
 membrane.
 Available from: https://www.ncbi.nlm.nih.gov/books/NBK2264/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"
literal "false"

\end_inset

Aurélien Géron.
 Hands-on Machine Learning with Scikit-Learn and TensorFlow Concepts, Tools,
 and Techniques to Build Intelligent Systems.
 O’Reilly Media, Inc., Sept.
 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"
literal "false"

\end_inset

Andriy Burkov.
 THE HUNDRED-PAGE MACHINE LEARNING BOOK.
 Andriy Burkov, 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"
literal "false"

\end_inset

Makary M A, Daniel M.
 Medical error—the third leading cause of death in the US BMJ 2016; 353
 :i2139 doi:10.1136/bmj.i2139
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"
literal "false"

\end_inset

Bruce, P., Bruce, A., & Gedeck, P.
 (2020).
 Practical statistics for data scientists: 50+ Essential concepts using
 R and python (2nd ed.).
 O’Reilly Media.
 p.
 141
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13"
literal "false"

\end_inset

Kleinbaum, D.
 G., & Klein, M.
 (2011).
 Survival analysis: A self-learning text, third edition (3rd ed.).
 Springer.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"
literal "false"

\end_inset

 Ostan R, Monti D, Gueresi P, Bussolotto M, Franceschi C, Baggio G.
 Gender, aging and longevity in humans: an update of an intriguing/neglected
 scenario paving the way to a gender-specific medicine.
 Clin Sci (Lond).
 2016 Oct 1;130(19):1711-25.
 doi: 10.1042/CS20160004.
 PMID: 27555614; PMCID: PMC4994139.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-15"
literal "false"

\end_inset

vom Steeg LG, Klein SL.
 SeXX Matters in Infectious Disease Pathogenesis.
 PLoS Pathog.
 2016 Feb 18;12(2):e1005374.
 doi: 10.1371/journal.ppat.1005374.
 PMID: 26891052; PMCID: PMC4759457.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-16"
literal "false"

\end_inset

Rodrigues S, Escoli R, Eusébio C, Dias L, Almeida M, Martins LS, Pedroso
 S, Henriques AC, Cabrita A.
 A Survival Analysis of Living Donor Kidney Transplant.
 Transplant Proc.
 2019 Jun;51(5):1575-1578.
 doi: 10.1016/j.transproceed.2019.01.047.
 Epub 2019 Jan 21.
 PMID: 31155195.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-17"
literal "false"

\end_inset

Nemati E, Einollahi B, Lesan Pezeshki M, Porfarziani V, Fattahi MR.
 Does kidney transplantation with deceased or living donor affect graft
 survival? Nephrourol Mon.
 2014 Jul 5;6(4):e12182.
 doi: 10.5812/numonthly.12182.
 PMID: 25695017; PMCID: PMC4317718.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-18"
literal "false"

\end_inset

Pisavadia B, Arshad A, Chappelow I, Nightingale P, Anderson B, Nath J, Sharif
 A.
 Ethnicity matching and outcomes after kidney transplantation in the United
 Kingdom.
 PLoS One.
 2018 Apr 13;13(4):e0195038.
 doi: 10.1371/journal.pone.0195038.
 PMID: 29652887; PMCID: PMC5898720.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-19"
literal "false"

\end_inset

Guillermo García García, Arpana Iyengar, François Kaze, Ciara Kierans, Cesar
 Padilla-Altamira, Valerie A.
 Luyckx, Sex and gender differences in chronic kidney disease and access
 to care around the globe, Seminars in Nephrology, Volume 42, Issue 2, 2022,
 Pages 101-113, ISSN 0270-9295, https://doi.org/10.1016/j.semnephrol.2022.04.001.
 (https://www.sciencedirect.com/science/article/pii/S0270929522000092)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-20"
literal "false"

\end_inset

Mange KC, Joffe MM, Feldman HI.
 Effect of the use or nonuse of long-term dialysis on the subsequent survival
 of renal transplants from living donors.
 N Engl J Med.
 2001 Mar 8;344(10):726-31.
 doi: 10.1056/NEJM200103083441004.
 PMID: 11236776.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-21"
literal "false"

\end_inset

Rahman MS, Ambler G, Choodari-Oskooei B, Omar RZ.
 Review and evaluation of performance measures for survival prediction models
 in external validation settings.
 BMC Med Res Methodol.
 2017 Apr 18;17(1):60.
 doi: 10.1186/s12874-017-0336-2.
 PMID: 28420338; PMCID: PMC5395888.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-22"
literal "false"

\end_inset

Evaluating survival models — scikit-survival 0.21.0.
 (n.d.).
 Readthedocs.Io.
 Retrieved July 18, 2023, from https://scikit-survival.readthedocs.io/en/stable/us
er_guide/evaluating-survival-models.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-23"
literal "false"

\end_inset

Ping Wang, Yan Li, and Chandan k.
 Reddy.
 2019.
 Machine Learning for Survival Analysis: A Survey.
 ACM Comput.
 Surv.
 51, 6, Article 110 (February 2019), 36 pages.
\begin_inset Flex URL
status open

\begin_layout Plain Layout

 https://doi.org/10.1145/3214306
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-24"
literal "false"

\end_inset

Definitions:, 1.
 1.
 (n.d.).
 Survival distributions, hazard functions, cumulative hazards.
 Stanford.edu.
 Retrieved October 23, 2023, from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-25"
literal "false"

\end_inset

Penalized Cox Models — scikit-survival 0.22.1.
 (2015).
 Readthedocs.io.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://scikit-survival.readthedocs.io/en/stable/user_guide/coxnet.html
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-26"
literal "false"

\end_inset

‌Ishwaran, H., Kogalur, U.
 B., Blackstone, E.
 H., & Lauer, M.
 S.
 (2008).
 Random survival forests.
\begin_inset Note Note
status open

\begin_layout Plain Layout
these 3 citations cause the glyph problem 26,27,28
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-27"
literal "false"

\end_inset

Wang, H., & Li, G.
 (2017).
 A selective review on random survival forests for high dimensional data.
 Quantitative bio-science, 36(2), 85.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-28"
literal "false"

\end_inset

Breiman, L.
 (2001).
 Random forests.
 Machine learning, 45, 5-32.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-29"
literal "false"

\end_inset

Gönen M, Heller G.
 Concordance probability and discriminatory power in proportional hazards
 regression.
 Biometrika 2005;92(4):1799–09.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-30"
literal "false"

\end_inset

Enrico Longato, Vettoretti, M., & Barbara Di Camillo.
 (2020).
 A practical perspective on the concordance index for the evaluation and
 selection of prognostic time-to-event models.
 Journal of Biomedical Informatics, 108, 103496–103496.
 https://doi.org/10.1016/j.jbi.2020.103496
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-31"
literal "false"

\end_inset

‌Uno H, Cai T, Pencina MJ, D'Agostino RB, Wei LJ.
 On the C-statistics for evaluating overall adequacy of risk prediction
 procedures with censored survival data.
 Stat Med.
 2011 May 10;30(10):1105-17.
 doi: 10.1002/sim.4154.
 Epub 2011 Jan 13.
 PMID: 21484848; PMCID: PMC3079915.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-32"
literal "false"

\end_inset

 Kindt TJ Goldsby RA Osborne BA Kuby J.
 Kuby Immunology.
 6th ed.
 New York: W.H.
 Freeman; 2007.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-33"
literal "false"

\end_inset

Health.
 (2022).
 Kidneys.
 Vic.gov.au.
 https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/kidneys
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-34"
literal "false"

\end_inset

‌Facts About Chronic Kidney Disease.
 (2020, May 15).
 National Kidney Foundation; https://www.kidney.org/atoz/content/about-chronic-kid
ney-disease
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-35"
literal "false"

\end_inset

‌Kamyar Kalantar-Zadeh, Jafar, T.
 H., Nitsch, D., Neuen, B.
 L., & Perkovic, V.
 (2021).
 Chronic kidney disease.
 The Lancet, 398(10302), 786–802.
 https://doi.org/10.1016/s0140-6736(21)00519-5
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-36"
literal "false"

\end_inset

‌Holland, K.
 (2019, May 23).
 Everything You Need to Know About Kidney Failure.
 Healthline; Healthline Media.
 https://www.healthline.com/health/kidney-failure#outlook
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-37"
literal "false"

\end_inset

‌Causes of chronic kidney disease.
 (2022, August 29).
 National Institute of Diabetes and Digestive and Kidney Diseases; NIDDK
 - National Institute of Diabetes and Digestive and Kidney Diseases.
 https://www.niddk.nih.gov/health-information/kidney-disease/chronic-kidney-disease
-ckd/causes
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-38"
literal "false"

\end_inset

Estimated Glomerular Filtration Rate (eGFR).
 (2015, December 24).
 National Kidney Foundation; https://www.kidney.org/atoz/content/gfr
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-39"
literal "false"

\end_inset

What is a Container? | Docker.
 (2023, October 26).
 Docker.
 https://www.docker.com/resources/what-container/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-40"
literal "false"

\end_inset

‌Use Docker Compose.
 (2024).
 Docker Documentation.
 https://docs.docker.com/get-started/08_using_compose/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-41-reverse-proxy"
literal "false"

\end_inset

What is a Reverse Proxy Server? | NGINX.
 (2023, June).
 NGINX.
 https://www.nginx.com/resources/glossary/reverse-proxy-server/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-42"
literal "false"

\end_inset

Kendra Cherry, Mse.
 (2022, November 22).
 How the color blue impacts moods, feelings, and behaviors.
 Verywell Mind.
 https://www.verywellmind.com/the-color-psychology-of-blue-2795815 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
the following bad citation: 30
\end_layout

\begin_layout Plain Layout
The bug probably ruined some citations and will continue to ruin new ones
 on the addition
\end_layout

\end_inset


\end_layout

\end_body
\end_document
