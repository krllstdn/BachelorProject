#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}
\usepackage{booktabs}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards 
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\begin_local_layout
Format 49

Float

Type listing

GuiName "Code listing"

Placement tbp

Extension lol

NumberWithin chapter

Style ruled

ListName "List of code listings"

LaTeXBuiltin false

End
\end_local_layout
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{August 2, 2023}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods 
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Bachelor's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author: 
\series bold
Kyrylo Stadniuk
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor: 
\series bold
Ing.
 Tomáš Kouřim
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Consultant: 
\series bold
Ing.
 Pavel Strachota, Ph.D.
\end_layout

\begin_layout Labeling
\labelwidthstring MMMMMMMMM
Language
\begin_inset space ~
\end_inset

advisors: 
\series bold
PaedDr.
 Eliška Rafajová 
\series default
and
\series bold
 Bc.
 Nathaniel Tobias Patton
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2023/2024
\end_layout

\begin_layout Standard
\begin_inset External
	template PDFPages
	filename zadani_cele.pdf
	extra LaTeX "pages={1,2}"

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I am grateful to Ing.
 Tomáš Kouřim for his expert guidance and to Dr.
 Pavel Strachota for his invaluable support and insightful feedback throughout
 this project.
 I would also like to extend my sincerest appreciation to PaedDr.
 Eliška Rafajová and Bc.
 Nathaniel Tobias Patton for their language assistance.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Bachelor's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
documentdate
\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset

Kyrylo Stadniuk
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size small
\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\align block

\series bold
\size larger
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Autor:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Obor:
\emph default
 Aplikovaná Informatika
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Druh práce:
\emph default
 Bakalářská práce
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Vedoucí práce:
\emph default
 Ing.
 Tomáš Kouřim Mild Blue, s.r.o.
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Konzultant: 
\emph default
Ing.
 Pavel Strachota, Ph.D., Katedra matematiky FJFI ČVUT
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Abstrakt:
\emph default
 Transplantace ledvin je nejlepší lečbou selhání ledvin, která ve srovnání
 s dialýzou přináší vyšší kvalitu a delší očekávanou délku života.
 Před transplantací se v naději na lepší výsledky maximalizuje kompatibilita
 dárce a příjemce, nicméně dlouhodobé přežití zůstává nejisté, protože je
 notoricky obtížně předvídatelné a chybí přesné prediktivní nástroje pro
 dlouhodobé výsledky po transplantaci.
 Tato práce se pokouší tuto mezeru překlenout trénováním několika prediktivních
 modelů strojového učení pro transplantace od žijících dárců (LDT) a zemřelých
 dárců (DDT), včetně regularizované Coxovy regrese, náhodných přežívacích
 lesů a přežívacích stromů s gradientním boostingem, s cílem odhadnout pravděpod
obnost dlouhodobého přežití.
 Modely LDT dosáhly indexu shody Uno (c-index) 0.72 a integrovaného Brierova
 skóre (IBS) 0.136, zatímco modely DDT dosáhly c-indexu Uno 0.69 a IBS 0.162.
 S využitím těchto modelů byla vyvinuta aplikace, která má lékařům pomoci
 při rozhodování a potenciálně zlepšit přežítí pacientů.
\end_layout

\begin_layout Standard

\size small
\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
\lang czech
Klíčová slova:
\emph default
 Analýza přežití, 
\lang american
Gradient Boosting Survival Analysis
\lang czech
, 
\lang american
Inference strojového učení,
\lang czech
 Předpověď přežití, Random Survival Forests, Regularizovaná Coxová regrese,
 Strojové učení, Transplantace ledviny, Transplantace od žijícího dárce,
 Transplantace od zemřelého dárce, 
\lang american
United Network for Organ Sharing (UNOS), Umělá Inteligence
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\size small
\emph on
Title:
\end_layout

\begin_layout Standard
\align block

\series bold
\size larger
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
Author:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard

\size small
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
Abstract:
\emph default
 Kidney transplantation is the gold standard treatment for kidney failure
 that provides an increased quality of life and a longer life expectancy
 compared to dialysis.
 Before transplantation, donor-recipient compatibility is maximized in hopes
 of better outcomes, yet long-term survival remains uncertain due to being
 notoriously hard to predict and lack of accurate predictive tools for long-term
 post-transplant outcomes.
 This thesis attempts to bridge this gap by training several predictive
 machine learning models for living donor (LDT) and deceased donor transplantati
ons (DDT), including Cox elastic net regression, random survival forests,
 and gradient-boosted survival trees, to estimate long-term survival probabiliti
es.
 LDT models achieved an Uno concordance index of 0.72 and an integrated Brier
 score (IBS) of 0.136, while DDT models achieved an Uno c-index of 0.69 and
 an IBS of 0.162.
 Leveraging these models, an application was developed to aid clinicians
 in decision-making, potentially improving patient outcomes.
\end_layout

\begin_layout Standard
\noindent

\size small
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
It is organized in a similar way as the paper, i.e.
 it includes the following parts: - paper objective and background (based
 on facts from the Introduction), - method and approach (based on facts
 from the Materials and Methods section), - results (based on facts from
 the Results section), - conclusion(s) (based on concluding remarks from
 the Conclusion).
\end_layout

\end_inset


\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Useful hints for writing the abstract: - Do not exceed the word limit.
 - Do not include references, figures, equations, and details.
 - Avoid overusing “we” constructions if possible.
 Prefer passive, i.e.
 impersonal, constructions, because the agent of processes and events is
 usually not known or is not important.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\size small
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\size small
\emph on
Keywords:
\emph default
 Artificial Intelligence, Deceased Donor Transplantation, Gradient Boosting
 Survival Analysis, Kidney Transplantation, Lifespan Prediction, Living
 Donor Transplantation, Machine Learning, Machine Learning Inference, Random
 Survival Forest, Regularized Cox regression, Survival Analysis, United
 Network for Organ Sharing (UNOS)
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter*
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Introduction}
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\emph on
Chronic kidney disease (CKD)
\emph default
 is a common disease with an incidence of 8 to 16% of the population, often
 leading to kidney failure 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35,key-66"
literal "false"

\end_inset

.
 
\emph on
Kidney transplantation
\emph default
 (KT) is the best treatment for it, but organs are scarce.
 Every 10 minutes, a new patient joins the transplant waiting list, and
 17 people die each day while waiting 
\begin_inset CommandInset citation
LatexCommand cite
key "key-67"
literal "false"

\end_inset

.
 Given this shortage, it is paramount to maximize the longevity of a recipient.
 Although some scoring techniques consider the estimated longevity 
\begin_inset CommandInset citation
LatexCommand cite
key "key-60"
literal "false"

\end_inset

, there is a lack of software dedicated solely to this problem.
 
\end_layout

\begin_layout Standard
Artificial intelligence (AI) and its subset, machine learning (ML), have
 gained popularity with their ability to find patterns in large amounts
 of data and solve practical problems, finding their uses in medicine.
 The most prominent ones in genetic and molecular research have led to the
 discovery of new therapeutic targets, enabling the development of novel
 treatments 
\begin_inset CommandInset citation
LatexCommand cite
key "key-65"
literal "false"

\end_inset

.
 This thesis aims to: 1) train predictive machine learning models capable
 of accurately estimating patient life expectancy; 2) develop a minimum-viable-p
roduct (MVP) application based on those models.
 
\end_layout

\begin_layout Standard
Kidneys are vital organs with functions such as regulating fluid balance
 and aiding red blood cell production.
 Conditions like diabetes and hypertension, often linked to lifestyle choices,
 can impair kidney function, leading to CKD.
 Routine health checks usually identify this gradual decline, but in some
 cases, the patient starts to experience the first symptoms, such as flank
 pain, ”foamy urine”, nocturia, and decreased urine output.
 With advanced CKD, the patient might exhibit nausea, poor appetite, fatigue,
 weight loss, shortness of breath, itching, or peripheral edema 
\begin_inset CommandInset citation
LatexCommand cite
key "key-66"
literal "false"

\end_inset

.
 If kidneys fail and a suitable donor is not available, the patient must
 undergo dialysis, a machine mimicking kidney function.
 Dialysis severely limits patient mobility and life expectancy.
 These factors make transplantation preferable.
 Moreover, performing transplantation as early as possible is crucial, as
 prolonged dialysis tends to adversely affect the subsequent transplantation. 
\end_layout

\begin_layout Standard
There are two types of transplantation: living donor transplantation (LDT)
 and deceased donor transplantation (DDT).
 LDT provides superior results, but DDT is much more prevalent.
 Prior to transplantation, donors and recipients undergo compatibility assessmen
t.
 Blood groups must be compatible, and the recipient immune system must not
 be sensitized to the donor human leukocyte antigens (HLA), proteins on
 cell surfaces crucial for immune system host recognition.
 After the transplantation, the recipient must take multiple immunosuppressive
 drugs and visit regular checkups to prevent rejection and other complications
 caused by the treatment.
\end_layout

\begin_layout Standard
Machine learning (ML) is a subset of artificial intelligence (AI) known
 for its ability to process large amounts of data, find complex relationships,
 and make predictions.
 It has found tremendous popularity across numerous fields, including medicine.
 Here, it is utilized for drug discovery, diagnostics, enhancing patient
 care, and personalized treatment 
\begin_inset CommandInset citation
LatexCommand cite
key "key-65"
literal "false"

\end_inset

.
 We will utilize machine learning methods for survival analysis on the UNOS
 dataset to create several models to estimate the life expectancy after
 transplantation.
 These models might help clinicians make more informed decisions about transplan
tation.
\end_layout

\begin_layout Standard
Now let us review the structure of this work.
 In Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Medical-Background"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we explore the medical aspects of kidney transplantation.
 Specifically, we review the organ characteristics, common causes of its
 failure, clinical aspects of its transplantation, and the immunology of
 transplantation.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Machine-Learning-Background"
plural "false"
caps "false"
noprefix "false"

\end_inset

 explores machine learning and survival analysis.
 We review the main branches of ML and elaborate on the most prevalent technique
s used in the branch most relevant to our task.
 In addition to that, we discuss approaches to data preparation, model training,
 and hyperparameter tuning.
 Lastly, we provide a comprehensive overview of survival analysis (SA),
 its methods, and machine-learning-based survival analysis techniques.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Data-Preparation-and"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is devoted to data.
 In this chapter, we analyze the UNOS dataset, including examining value
 distributions and the influence of various features on survival.
 Moreover, we describe the data acquisition, data loading, and the preprocessing
 pipeline, along with the feature engineering process. 
\end_layout

\begin_layout Standard
Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Machine-Learning-Models"
plural "false"
caps "false"
noprefix "false"

\end_inset

 presents the results of our study and thoroughly evaluates trained models
 for the deceased and living donor subgroups.
 As part of this, we examine model performance metrics and analyze their
 changes over time.
 Furthermore, we analyze feature importance to understand the factors that
 drive model predictions.
 Models are compared based on training experience, training time, and practicali
ty.
 Finally, the results are discussed and compared with the state-of-the-art.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Applications"
plural "false"
caps "false"
noprefix "false"

\end_inset

 focuses on the software for KT.
 We review existing software solutions and introduce KidneyLife, an inference
 application based on the trained models.
 The chapter delves into its architecture and design principles.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
- we didn't do deep survival learning
\end_layout

\begin_layout Plain Layout
- only one dataset was used.
 while the most extensive and diverse, still....
\end_layout

\begin_layout Plain Layout
- models assume that the donor-recipient pair is well-matched, so they should
 only be used for 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\series bold
Motivation behind the paper
\series default
: We participated in the development of TX Matching, an open-source kidney
 pair matching software, which estimates the optimal series of exchanges
 between incompatible donor-recipient pairs.
 And while developing it, we got interested with the results of transplants,
 how many years after the transplant a person can live.
 What happens after the transplantation.
 What are the underlying medical pricinples that govern wheether the organ
 will be accepted or rejected.
 This thesis is a result of this interest and we hope that the reader will
 share it with us.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
intro paragraph
\end_layout

\begin_layout Plain Layout
general -> specific
\end_layout

\begin_layout Plain Layout
subtopics 
\end_layout

\begin_layout Plain Layout
the first sentence must catch reader's attention and point to the direction
 of the main subject.
 might contain statistic or a quote
\end_layout

\begin_layout Plain Layout
thesis statement is the last in the paragraph
\end_layout

\begin_layout Plain Layout
4.
 thesis statement should contain subtopics, which will be used as a paragraph
 topics in the body.
 order must be preserveed
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
1.
 what 
\end_layout

\begin_layout Plain Layout
2.
 why
\end_layout

\begin_layout Plain Layout
3.
 the scope of the research
\end_layout

\begin_layout Plain Layout
4.
 limitations
\end_layout

\begin_layout Subsubsection*
7 essentials
\end_layout

\begin_layout Plain Layout
1.
 opening section (1 paragraph)
\end_layout

\begin_layout Itemize
introduce the field
\end_layout

\begin_layout Itemize
introduce the research problem(gap)
\end_layout

\begin_layout Itemize
state research aims
\end_layout

\begin_layout Itemize
layout the intro chapter structure
\end_layout

\begin_layout Plain Layout
2.
 study background
\end_layout

\begin_layout Itemize
relevatnt bg info to give a decent foundational understanding of the topic
\end_layout

\begin_layout Plain Layout
3.
 research gap
\end_layout

\begin_layout Itemize
narrow the focus to the problem
\end_layout

\begin_layout Itemize
what exactly is missing in literature and why is that a problem
\end_layout

\begin_layout Itemize
what is already known, what is missing, why is it a problem
\end_layout

\begin_layout Plain Layout
4.
 aims, objectives and questions
\end_layout

\begin_layout Itemize
clearly state the research aim
\end_layout

\begin_layout Itemize
aim(high-level what) objectives are somthing specific
\end_layout

\begin_layout Itemize
state the boundaries of the research, what you will and won't' do
\end_layout

\begin_layout Plain Layout
5.
 signifficance - value to the world
\end_layout

\begin_layout Itemize
state how is the research might benefit to the world
\end_layout

\begin_layout Plain Layout
6.
 limitations
\end_layout

\begin_layout Itemize
narrow scope, methodology, resource limitations, generalizability of the
 findings, 
\end_layout

\begin_layout Plain Layout
7.
 structure
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
Medical Background 
\begin_inset CommandInset label
LatexCommand label
name "chap:Medical-Background"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Kidney — An Overview
\end_layout

\begin_layout Standard
The kidneys are reddish-brown, bean-shaped organs located on both sides
 of the spine beneath the lower ribs.
 They serve several critical functions, such as filtering blood, regulating
 blood pressure, aiding red blood cell production, and maintaining fluid
 balance.
 Structurally, a kidney consists of about 1 million nephrons, each consisting
 of a small filter called a glomerulus, attached to a tubule, responsible
 for filtering waste products from the blood and excreting them with small
 amounts of fluid as urine 
\begin_inset CommandInset citation
LatexCommand cite
key "key-33"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Each kidney performs 50% of the normal kidney function.
 However, if one kidney is lost, the remaining kidney can adapt, increasing
 its capacity to as much as 75% of normal function.
 Kidney function is measured with
\emph on
 glomerular filtration rate (GFR)
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-33"
literal "false"

\end_inset

.
 There are two types of GFR: 
\emph on
measured GFR (mGFR)
\emph default
 and 
\emph on
estimated GFR (eGFR)
\emph default
.
 Measuring mGFR is problematic and lengthy, so healthcare practitioners
 usually use a formula to calculate eGFR.
 An eGFR of 90 and above is the normal range.
 An eGFR of 15-89 is a range for different stages of chronic kidney disease.
 Values below 15 might indicate renal failure 
\begin_inset CommandInset citation
LatexCommand cite
key "key-38"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Impairment in kidney function can have serious health consequences.
 There are many reasons why kidneys may become dysfunctional, such as end-stage
 chronic kidney disease, heavy metal poisoning, polycystic kidney disease,
 infection, nephrotoxic drugs, lupus, and physical injury 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36,key-37"
literal "false"

\end_inset

.
 Among these conditions, chronic kidney disease is the most prevalent —
 approximately 10% of the population worldwide is affected by some form
 of chronic kidney disease 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
 
\emph on
Chronic kidney disease (CKD)
\emph default
 is an incurable condition characterized by a gradual reduction in kidney
 function over time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34,key-35"
literal "false"

\end_inset

.
 Diabetes and high blood pressure are responsible for the majority of cases
 of chronic kidney disease, with high blood sugar damaging the glomeruli
 and high blood pressure damaging the kidney blood vessels 
\begin_inset CommandInset citation
LatexCommand cite
key "key-34,key-37"
literal "false"

\end_inset

.
 The latter interaction is associated with a negative feedback loop, where
 it is unclear whether kidney damage causes high blood pressure or vice
 versa 
\begin_inset CommandInset citation
LatexCommand cite
key "key-35"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
CKD progresses in 5 stages, ultimately culminating in 
\emph on
end-stage renal disease (ESRD)
\emph default
, where kidneys fail.
 Treatment options include 
\emph on
dialysis
\emph default
, a mechanical process that substitutes kidney function but cannot fully
 replicate it.
 It leads to a reduced life expectancy of, on average, 5 to 10 years.
 Alternatively, 
\emph on
kidney transplantation
\emph default
 offers a more permanent solution but requires a compatible donor, waiting
 for which can take many months or years, and the use of immunosuppressants
 after the transplantation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-36"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In conclusion, the kidneys play an essential role in maintaining overall
 health with their ability to filter waste, regulate blood pressure, and
 balance fluids.
 Kidney failure has profound health implications, and there are multiple
 reasons for it.
 Chronic kidney disease, resulting from various causes such as diabetes
 and hypertension, progressively impairs kidney function, leading to end-stage
 renal disease.
 While dialysis offers temporary relief, it cannot fully substitute for
 a healthy kidney, which brings us to the crucial role of kidney transplantation.
 Kidney transplantation offers a more conclusive and sustainable solution
 for patients with ESRD, albeit accompanied by challenges related to donor
 compatibility and life-long immunosuppression.
 Both of which will be discussed in the subsequent section.
\end_layout

\begin_layout Section
Clinical Aspects of Kidney Transplantation 
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, kidney transplantation stands as the most commonly performed organ transplant
 worldwide.
 This prevalence is partly due to the high incidence of diseases such as
 diabetes and chronic kidney disease, which often lead to chronic renal
 failure.
 Unlike organs such as the liver and heart, kidney transplantation is relatively
 straightforward, contributing to its frequency of transplantation.
 Kidneys can be obtained from both deceased and living donors - relatives
 or altruistic volunteers.
 Living donors can donate one kidney and continue to lead normal and healthy
 lives, significantly expanding the potential donor pool.
\end_layout

\begin_layout Standard
ABO blood group and histocompatibility (defined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

) matchings for kidney transplantation are of utmost importance.
 Proper matching substantially reduces the risk of rejection and improves
 transplant outcomes.
 Compared to organs such as the liver or bone marrow, kidneys do not present
 additional challenges such as graft versus host disease (GVHD), simplifying
 the transplantation process.
\end_layout

\begin_layout Standard
However, kidney transplantation, despite its advances, faces significant
 challenges.
 Two critical issues confront potential recipients: the scarcity of available
 organs and the risk of sensitization following a failed first transplant,
 decreasing the pool of available donors even further.
 In such cases, the immune system develops antibodies to the graft alloantigens
 – proteins on the surface of the cell that are recognized by the recipient's
 immune system as foreign.
 An antigen is any substance that triggers an immune response, and an alloantige
n is a type of antigen that varies between individuals of the same species
 and helps the body distinguish its cells from the cells of an invader.
 The antibodies bind with these alloantigens, marking the cells for destruction
 by the recipient's immune system.
 Hyperacute rejection, a rapid and severe rejection of the transplanted
 organ, occurs when antibodies react immediately with the organ's alloantigens.
 This happens mostly if the donor-recipient blood groups were mismatched
 or the recipient was sensitized to similar alloantigens.
 Most likely with the previous transplant.
 This fact often leaves many patients unable to find a compatible donor
 after one or two rejection episodes.
\end_layout

\begin_layout Standard
Recipients of kidney transplants typically require life-long immunosuppression.
 While immunosuppressants are essential to prevent organ rejection, they
 are associated with severe side effects, such as increased risk of cancer,
 hypertension, and metabolic bone disease.
 
\end_layout

\begin_layout Standard
Given the high incidence of renal failure in diabetic patients, which occurs
 in 30% of individuals with advanced diabetes, simultaneous kidney and pancreas
 transplants are sometimes performed.
 This approach addresses renal failure and underlying diabetes, offering
 a comprehensive solution for these patients.
\end_layout

\begin_layout Standard
In summary, kidney transplantation has become a vital and frequent medical
 procedure, offering improved quality of life to many suffering from kidney
 failure.
 While simpler than other organ transplants, the procedure has its own challenge
s.
 The following section will examine the history of kidney transplantation,
 showing its path from the initial experiments to the most frequently performed
 organ transplantation today.
 The historical perspective will provide insights into the advancements
 in surgical techniques, immunosuppressive therapy, and donor-recipient
 matching that have shaped the current state of kidney transplantation.
\end_layout

\begin_layout Section
The History of Kidney Transplantation 
\begin_inset CommandInset label
LatexCommand label
name "sec:history-of-kidney-transplantation"

\end_inset


\end_layout

\begin_layout Subsection*
Early Animal Experiments
\end_layout

\begin_layout Standard
As outlined in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

, advancements in surgical methods at the beginning of the 20th century
 eventually led to experiments with organ transplantation.
 One of the first recorded transplantations was an 
\emph on
autograft
\emph default
, where the donor and recipient are the same individual, performed by Emerich
 Ullmann on March 1, 1902, at the Vienna Medical School.
 Ullmann successfully connected the dog's kidney to the vessels of its neck,
 which resulted in the urine production.
 The success of this experiment was notable enough to be presented to the
 Vienna Medical Society, sparking considerable interest.
\end_layout

\begin_layout Standard
That year, other experiments followed.
 Alfred von Decastello performed a dog-to-dog kidney 
\emph on
allograft
\emph default
, a transplant between two individuals of the same species, at the Institute
 of Experimental Pathology in Vienna.
 Although initially, the transplanted kidney produced urine, it eventually
 ceased.
 Later, Ullman performed a dog-to-goat kidney 
\emph on
xenograft
\emph default
, a transplantation between individuals of different species, which also
 resulted in brief function time.
\end_layout

\begin_layout Standard
At the same time, in Lyon, Alexis Carrel and his colleagues were working
 on vascular suturing methods.
 Carrel's technique, known as Carrel's seam, was a considerable improvement
 over existing methods, addressing the common issues of thrombosis, hemorrhage,
 stricture, and embolism.
 His consequent move to The Rockefeller Institute for Medical Research in
 the United States led to further refinements of his method and the documentatio
n of organ rejection.
 Carrel received the Nobel Prize in Medicine in 1912 for his contributions
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-2-nobel-prize-medicine"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
These early experiments, although varied in their outcomes, were crucial
 in shaping the future of organ transplantation.
 They not only illustrated the possibility of organ transplantation but
 also highlighted the challenges, such as rejection, that would direct the
 course of future research.
 The insights gained in animal experiments laid the groundwork for the next
 big milestone in transplantation medicine: the advent of human organ transplant
ation.
 This transition from animals to humans marked the beginning of a new era
 in medical history.
\end_layout

\begin_layout Subsection*
Early Human Transplantation
\end_layout

\begin_layout Standard
The first recorded human renal 
\emph on
xenografts
\emph default
 were credited to Mathieu Jaboulay in 1906.
 It involved a pig and a goat as donor animals.
 One kidney was transplanted to the arm and the second to the thigh.
 Although each kidney functioned only for an hour, these efforts marked
 the beginning of human transplantation attempts 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book,key-4"
literal "false"

\end_inset

.
 Ernst Unger's xenografts in 1909 gained more attention.
 His first attempt, involving the transplantation of a kidney from a stillborn
 baby to a baboon, resulted in a lack of kidney function despite a successful
 connection of vessels.
 Inspired by a successful surgery, Unger attempted a monkey-to-human xenograft,
 which also failed.
\end_layout

\begin_layout Standard
These experiments demonstrated the technical feasibility of kidney transplantati
on but also exposed the challenge of graft rejection.
 In a 1914 lecture, Alexis Carrel mentioned J.
 B.
 Murphy's work on irradiation and benzol treatment, suggesting their potential
 to improve graft survival 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

.
 Inspired by these findings, Carrel conducted his experiments with irradiation,
 achieving prolonged graft survival.
 However, these findings were never formally published 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The 1930s and 1940s were stagnant compared to the beginning of the century.
 European surgical centers that studied transplantology before were in decline.
 Meanwhile, the Mayo Clinic in the US was conducting some cautious experiments
 without considering Carrol's works and attempts at immunosuppression 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
However, it was during that period that Yurii Voronyi achieved a significant
 milestone.
 On March 3, 1933, in Kherson, Ukraine, Voronyi performed the first human
 kidney allograft on a woman suffering from acute renal failure due to mercury
 chloride poisoning.
 Given the ethical concerns surrounding living donors and previous failures
 of xenografts, Voronyi considered a cadaveric transplant the only viable
 option.
 Although there was initial urine production, the transplant failed 48 hours
 post-surgery due to blood group incompatibility and prolonged warm ischemia,
 triggering an immune reaction.
 Despite this setback, Voronyi continued to perform similar transplantations.
 He viewed these transplants as temporary measures to bridge the gap until
 the recipient's kidneys could recover.
 Out of the six transplants he performed, two patients experienced a complete
 recovery, regaining normal kidney function 
\begin_inset CommandInset citation
LatexCommand cite
key "key-4-matevossian"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The pioneering work of Jaboulay, Unger, and Voronyi demonstrated the technical
 feasibility of human organ transplantation and highlighted its main challenge
 of graft rejection.
 Despite the progress, the field has yet to witness success, primarily due
 to the lack of effective immunosuppression.
 In the next section, we will explore essential moments of the 1950s that
 transformed human renal transplantation from a daring experiment to a feasible
 medical procedure.
\end_layout

\begin_layout Subsection*
First Successes
\end_layout

\begin_layout Standard
In 1946, at the Peter Bent Brigham Hospital in Boston, a group of surgeons
 performed kidney transplantation under local anesthetic on the arm vessels.
 The short period of kidney functioning may have helped the patient recover
 from acute renal failure, igniting the hospital's interest in renal transplanta
tion.
\end_layout

\begin_layout Standard
Meanwhile, European surgeons were making significant advancements.
 Notably, Simonsen in Denmark, Dempster in London, and Küss in Paris concluded
 that placing the kidney in the pelvis is preferable.
 Furthermore, both Simonsen and Dempster deduced that the immune response
 was responsible for graft failure and hypothesized that the humoral mechanism
 of rejection was probable.
\end_layout

\begin_layout Standard
The early 1950s marked a period of active experimentation.
 In Paris, Jean Hamburger reported the first live-related kidney transplant
 between a mother and her child, achieving the immediate function of the
 transplanted kidney for 22 days until it was rejected.
 Meanwhile, in Boston, a series of nine transplantations with the thigh
 position of the allograft was closely studied.
 Moreover, in 1953, David Hume introduced the pre-transplant use of hemodialysis.
 Although some success was achieved with the administration of the adrenocortico
tropic hormone (more known as cortisone), it was deemed clinically insignificant.
 Hume's further research suggested the potential benefits of prior blood
 transfusions, blood group compatibility, and removal of both host's kidneys
 for transplant success - insights later validated with further research.
 On December 23, 1954, in Boston, Joseph Murray performed a kidney allograft
 from one identical twin to another, bypassing the rejection barrier.
 From that time, many similar surgeries were performed in Boston 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book,key-3"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
To conclude, the early successes during the 1950s marked a transformative
 period in the history of kidney transplantation.
 Works of Hume, Murray, and others underlined the critical role of immune
 response in graft survival, starting the quest for effective immunosuppression,
 to which the subsequent section is dedicated.
\end_layout

\begin_layout Subsection*
Attempts in Immunosuppression
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3-barker"
literal "false"

\end_inset

, the exploration of immunosuppression in transplantation began as early
 as the late 1940s.
 At the Mayo Clinic in 1948, patients with rheumatoid arthritis were administere
d cortisone, an adrenal cortical hormone with mild immunosuppressive properties,
 which provided temporary relief.
 Although initially praised, its effects were deemed clinically insignificant
 for transplantation purposes.
 It led researchers to revisit earlier experiments with irradiation.
 Experiments on mice by Joan Main and Richmond Prehn showed promising results,
 inspiring human trials in Boston and Paris.
\end_layout

\begin_layout Standard
In 1958, Murray's team in Boston employed radiation in human transplantation,
 achieving a significant breakthrough with a 20-year-long kidney transplant
 between non-identical twins.
 Similarly, in Paris, Jean Hamburger's team accomplished a 26-year functioning
 transplant using radiation.
\end_layout

\begin_layout Standard
The quest for safer immunosuppressive methods led to the anticancer drug
 6-mercaptopurine (6-MP).
 In 1959, Schwarz and Damesehek published a paper that described how 6-MP
 lowered immune response to foreign proteins in rabbits.
 Inspired by their work, Roy Calne performed his dog experiments, showing
 promising results backed by Charles Zukoski and David Hume.
 Despite initial setbacks, Küss and others reported prolonged graft survival
 from non-related donors using total body irradiation (TBI) complemented
 by 6-MP.
 The introduction of azathioprine in 1959, a derivative of 6-MP, by Gertrude
 Elion and George Hitchings, further improved results.
 Their groundbreaking work earned them the Nobel Prize, and by 1961, azathioprin
e was available for human use.
\end_layout

\begin_layout Standard
In 1963, a conference held by the National Research Council revealed a bleak
 outlook for kidney transplantation, with less than 10% survival beyond
 three months.
 It changed when Tom Starzl presented a protocol combining 6-MP with prednisone,
 leading to over one-year graft survival in 70% of cases.
 His results revolutionized the field, resulting in 50 new US transplantation
 programs and setting a twenty-year standard in post-transplant immunosuppressio
n.
\end_layout

\begin_layout Standard
In summary, the late 1940s to the early 1960s was a period of discoveries
 and experimentation.
 The use of cortisone in 1948, the administration of 6-MP for immunosuppression,
 and the invention of azathioprine in 1959 led to better immunosuppression,
 culminating in Tom Starzl's protocol, which became the standard for the
 next two decades.
 The following section is dedicated to a phase of steady developments in
 the literature referred to as a plateau that would solidify the practice
 of kidney transplantation.
\end_layout

\begin_layout Subsection*
Plateau
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand cite
key "key-3-barker"
literal "false"

\end_inset

 suggests that, from 1964 to 1980, kidney transplantation saw gradual progress.
 Dialysis, developed during WWII, finally became available for chronic renal
 failure as a result of the invention of Teflon arteriovenous conduits in
 the 1960s.
 Acceptance of brain death expanded donor pools.
 Organ preservation techniques also advanced: total body hypothermia was
 replaced by targeted cold solutions infusions that preserved organs better—by
 the mid-60s, longer preservation times allowed organ exchanges between
 centers.
 Concerns about equitable distribution of organs led to the National Transplant
 Act in 1984, establishing the United Network of Organ Sharing (UNOS) for
 oversight.
\end_layout

\begin_layout Standard
As techniques improved and donor pools expanded from 1964 to 1980, so did
 understanding of the nuances of immune compatibility.
 The following section describes the developments in tissue typing that
 would further expand the field of kidney transplantation.
\end_layout

\begin_layout Subsection*
Tissue Typing 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Tissue-Typing"

\end_inset


\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3-barker"
literal "false"

\end_inset

 further explains, the concept of tissue typing, suggested by Alexis Carrel
 in the early 20th century, remained unproven until Jean Dausset discovered
 the first human leukocyte antigen (HLA) in 1958 (more on HLA in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 It was not until 1964, with Paul Terasaki's development of the microcytotoxicit
y assay, that testing for antibodies became reliable.
 The test involved mixing the donor's lymphocytes with the recipient's serum,
 which swiftly became the standard, known as the 
\emph on
crossmatch test
\emph default
.
\end_layout

\begin_layout Standard
For several years, Terasaki performed typing for most U.S.
 transplant centers and found a couple of observations: 1) Positive crossmatch
 test predicts hyperacute rejection.
 The test is performed by mixing the recipient's serum with the donor's
 blood cells; if there is a reaction, the test is considered positive, and
 the transplantation cannot be performed.
 2) Matching can reliably identify optimal donors within a family, and it
 was assumed that the same principle would apply to non-related recipients.
\end_layout

\begin_layout Standard
However, in 1970, Terasaki's review of his extensive database of cadaver
 renal allografts revealed no correlation with the typing.
 This raised much agitation in the tissue typing community, and his grant
 was temporarily suspended until others reported the same.
 Since then, many crucial histocompatibility antigens have been discovered
 (Class II locus: D and DR; more on antigens in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Immunology-of-kidney"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Histocompatibility matching remains essential in bone marrow transplantation
 and in selecting family donors.
\end_layout

\begin_layout Standard
While tissue typing and matching have played a crucial role in improving
 transplantation outcomes, the evolution of immunosuppressive drugs has
 been equally, if not more, significant.
 The subsequent section elaborates on new developments in immunosuppressive
 drugs in the coming years.
\end_layout

\begin_layout Subsection*
Advancements in Immunosuppression
\end_layout

\begin_layout Standard
In continuation with 
\begin_inset CommandInset citation
LatexCommand cite
key "key-3-barker"
literal "false"

\end_inset

, the discovery of cyclosporine in 1976 by Jean-François Borel marked a
 significant milestone in transplantation.
 As a fungal derivative with potent immunosuppressive properties, cyclosporine
 drastically improved the outcomes of both renal and extra-renal transplants,
 surpassing the efficacy of the previously used drug, azathioprine.
 Similar to 6-MP, to achieve the best results, it had to be combined with
 prednisone.
 This protocol remained standard until 1989, when Tacrolimus, an even more
 powerful immunosuppressive agent, was introduced.
 Tacrolimus proved effective in cases where the combination of cyclosporine
 and prednisone was insufficient, effectively replacing cyclosporine as
 a usual baseline agent.
\end_layout

\begin_layout Standard
In conclusion, the history of renal transplantation has been marked by groundbre
aking discoveries and persistent challenges.
 From the pioneering attempts of xenografts in the early 20th century to
 the technical advancements and immunological breakthroughs of the mid-century
 — each milestone has been essential in shaping the current landscape of
 organ transplantation.
 As we explored the history of kidney transplantation, a deeper understanding
 of the immune system has become crucial.
 The following section examines the fundamentals of immunology, laying the
 foundation for understanding the interplay between the immune system and
 the transplanted organ.
\end_layout

\begin_layout Section
The Introduction to Immunology
\begin_inset CommandInset label
LatexCommand label
name "sec:Intro-to-Immunology"

\end_inset


\end_layout

\begin_layout Standard
As detailed in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-6-abbas"
literal "false"

\end_inset

, the 
\emph on
immune system
\emph default
 is a sophisticated defense mechanism that evolved to protect multicellular
 organisms from pathogens such as bacteria, fungi, viruses, and parasites.
 It is a network of many cells and tissues that compose a complex system
 that detects, evaluates, and responds to the invader.
 It is essential to understand these mechanisms, as the immune response
 plays a crucial role in graft acceptance or rejection.
\end_layout

\begin_layout Standard
Innate and adaptive immunity are the two interconnected systems of immune
 response.
 
\emph on
Innate immunity (
\emph default
also
\emph on
 natural 
\emph default
or
\emph on
 native)
\emph default
 includes primitive built-in cellular and molecular mechanisms that provide
 rapid, albeit non-specific responses to common pathogens.
 In contrast, 
\emph on
adaptive (specific 
\emph default
or
\emph on
 acquired) immunity
\emph default
 is slower to respond but capable of providing more targeted responses.
\end_layout

\begin_layout Standard
Physical barriers, including epithelia and mucous membranes, constitute
 the host's first line of defense and are a part of innate immunity.
 This branch of the immune system prevents invaders from infiltrating the
 host and quickly destroys many microbes that succeed in breaching these
 barriers.
 Innate immunity provides the necessary protection until adaptive immunity
 is activated.
 It also communicates how to best respond to the invader to the adaptive
 immunity.
 Furthermore, innate immunity plays a vital role in the clearance of dead
 tissue and the repair initiation after the tissue is damaged.
\end_layout

\begin_layout Standard
Adaptive immunity is broadly categorized into 
\emph on
humoral
\emph default
 and 
\emph on
cell-mediated immunity
\emph default
.
 
\emph on
Lymphocytes
\emph default
, also known as 
\emph on
white blood cells
\emph default
, are central to both humoral and cell-mediated immune responses.
 
\emph on
B lymphocytes
\emph default
 (
\emph on
B cells)
\emph default
 mediate humoral response by producing antigen-specific antibodies on encounter
 with the antigen.
 Produced antibodies then bind themselves to 
\emph on
antigens
\emph default
 — foreign molecular structures identified by common molecular patterns
 known as 
\emph on
pathogen-associated molecular patterns
\emph default
 (
\emph on
PAMPs
\emph default
) — to mark them for destruction.
 The immune system uses 
\emph on
pathogen recognition receptors
\emph default
 (
\emph on
PRRs
\emph default
), found on the surface of T cells, in conjunction with antibodies to detect
 and categorize these
\emph on
 
\emph default
PAMPs, which can take the form of molecules on the surface of a pathogen
 or its by-products.
 PRRs bind to PAMPs and initiate a targeted cascade of events culminating
 in the pathogen's elimination.
\end_layout

\begin_layout Standard
On the other hand, T lymphocytes start to proliferate when encountering
 an antigen, forming an army of T cells.
 The created 'army' will eliminate the invader and will form long-term memories
 about the pathogen.
 Activated T cells are divided into the following categories: helper T cells
 (
\begin_inset Formula $CD4^{+}$
\end_inset

), which help B cells to produce antibodies and help kill ingested microbes;
 
\emph on
cytotoxic T cells
\emph default
 (
\begin_inset Formula $CD8^{+}$
\end_inset

), which target and kill infected cells; 
\emph on
regulatory T lymphocytes
\emph default
, which prevent or limit immune responses; and 
\emph on
memory cells
\emph default
, which remain in the body long-term to provide faster and stronger immune
 response if the same antigen is encountered in the future.
\end_layout

\begin_layout Standard
Pathogen-host interaction is a continuous arms race, as pathogens usually
 have a short life cycle and can modify their DNA to elude the host's recognitio
n systems.
 The immune system counters this with the generation of host-tolerant lymphocyte
s with diverse PRRs during the development in bone marrow.
 Cells that react to the host's own cells are eliminated, ensuring that
 only non-self-reactive cells are allowed in circulation.
 The principle of recognizing self vs.
 non-self is called tolerance.
\end_layout

\begin_layout Standard
In conclusion, the immune system is a complex network of molecules, cells,
 tissues, and organs that cooperate in protecting the organism from pathogens.
 The system can be divided into two main branches: innate and adaptive,
 which cooperate in protecting the host from infections while developing
 long-term immunity to specific pathogens.
 Understanding the mechanisms of the immune system is essential to understanding
 the domain of kidney transplantation.
\end_layout

\begin_layout Section
Immunology of Kidney Transplant 
\begin_inset CommandInset label
LatexCommand label
name "sec:Immunology-of-kidney"

\end_inset


\end_layout

\begin_layout Standard
As outlined in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, the degree to which the immune system responds to a graft depends on the
 type of graft.
 There are four types of grafts:
\end_layout

\begin_layout Itemize

\series bold
Autograft 
\series default
is body tissue transfer from one body site to the other in the same individual.
\end_layout

\begin_layout Itemize

\series bold
Isograft
\series default
 is a tissue transplanted between two genetically identical individuals.
 In humans, it is usually homozygotic (identical) twins.
\end_layout

\begin_layout Itemize

\series bold
Allograft
\series default
 is a tissue transplanted between two genetically non-identical individuals
 of the same species.
\end_layout

\begin_layout Itemize

\series bold
Xenograft
\series default
 is a tissue transplanted between individuals of different species.
\end_layout

\begin_layout Standard
Autografts and isografts are generally accepted because they are genetically
 identical.
 Allografts, being genetically different, are typically identified as foreign
 by the immune system and rejected.
 Xenografts, which have the most significant genetic differences, are the
 most vigorously rejected by the immune system.
 
\end_layout

\begin_layout Subsection
Immune Response to the Graft
\end_layout

\begin_layout Standard
As detailed in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

, there are three reasons why the immune system may react to the allograft:
 damage due to ischemia, reaction to the incompatible blood group antigens,
 and reaction to major histocompatibility complex (MHC) and minor histocompatibi
lity (miH) antigens.
\end_layout

\begin_layout Paragraph
Ischemia-Reperfusion Injury
\end_layout

\begin_layout Standard
The transplantation process inevitably includes termination of blood flow
 and, as a result, oxygenation.
 This interruption in blood supply inhibits the cell's ability to generate
 sufficient energy to maintain homeostasis, leading to cellular damage or
 death.
 If this happens, the kidney is said to experience 
\emph on
ischemia-reperfusion injury (IRI)
\emph default
.
 IRI is a significant factor in the success of kidney transplantation.
 When the blood flow to the ischemic kidney is restored, dead cells trigger
 an innate immune response.
 It is associated with the release of 
\emph on
danger-associated molecular patterns (DAMP) 
\emph default
from the compromised cells
\emph on
,
\emph default
 serving as a critical alert mechanism.
 These DAMPs are recognized by both innate and adaptive immunity, although
 the former is predominately activated.
 Such an immune response might damage the graft even more and contribute
 to acute rejection.
 Because of that, the duration of ischemia is crucial in predicting the
 graft (and patient) survival.
\end_layout

\begin_layout Paragraph
Blood Group Compatibility
\end_layout

\begin_layout Standard
ABO blood group antigens, expressed by most cell types, play a crucial role
 in the compatibility of organ transplants.
 If the transplantation between incompatible pairs were performed, it would
 result in a severe and often immediate reaction known as hyperacute antibody-me
diated rejection (AMR or ABMR).
 The rejection risk is so high that the ABO blood group compatibility is
 the first thing checked before performing the transplant.
\end_layout

\begin_layout Standard
There are four primary blood groups: A, B, O, and AB.
 Within this system, individuals with blood group O are so-called ”universal
 donors.” Their organs can be transplanted to recipients with any ABO blood
 group.
 Whereas recipients in the AB group can safely receive organs from recipients
 with any ABO blood group and are called ”universal recipients.” In clinical
 practice, however, particularly with deceased donors, the preference is
 to match organ recipients with ABO-identical organs to prevent potential
 inequities in organ allocation and access.
 In the case of living donors, the principle of ABO-identity is somewhat
 more flexible, where organs from ABO-compatible donors are considered acceptabl
e for transplantation.
\end_layout

\begin_layout Paragraph
Histocompatibility
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, genetically similar tissues, known as 
\emph on
histocompatible
\emph default
, are less likely to trigger an immune response post-transplant, as the
 immune system does not recognize the transplanted tissue as foreign.
 Conversely, 
\emph on
histoincompatible
\emph default
 tissues, characterized by genetic differences, typically trigger an immune
 response.
 Although more than 40 distinct loci encode the antigens responsible for
 histocompatibility, the most vigorous allograft-rejection responses are
 attributed to loci within the 
\emph on
major histocompatibility complex (MHC)
\emph default
.
 The organization of MHC in humans is called 
\emph on
human leukocyte antigen (HLA)
\emph default
.
\end_layout

\begin_layout Standard
As detailed in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

, histocompatibility antigens, genetically encoded antigens that cover cell
 surfaces, play a crucial role in recognizing self versus non-self.
 In all vertebrates, histocompatibility antigens are categorized into a
 single 
\emph on
major histocompatibility complex (MHC) 
\emph default
and
\emph on
 
\emph default
numerous
\emph on
 minor histocompatibility (miH)
\emph default
 systems.
 Mismatches in either MHC or miH result in an immune reaction, most severe
 in the case of MHC.
 Rejection in MHC-compatible donor-recipient pairs is usually delayed and,
 in some cases, forever.
 However, the miH mismatch might be so drastic that it would be comparable
 to the MHC mismatch.
\end_layout

\begin_layout Standard
The MHC itself is divided into class I and class II antigens.
 MHC class I antigens cover the surfaces of most cells and can activate
 cytotoxic CD8 cells.
 MHC class II antigens, found on specific immune cells, play an essential
 role in immune response coordination.
 In humans, each MHC class is further divided into three subgroups, as illustrat
ed in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:MHC-class-division"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MHC class division
\begin_inset CommandInset label
LatexCommand label
name "tab:MHC-class-division"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DR
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DP
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DQ
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
However, 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

 remarks, that class I and II mismatches differ in their impact on graft
 survival.
 A mismatch of one or two class I antigens has little effect on graft survival,
 whereas a single mismatch in class II antigen is equivalent to a mismatch
 of three or four class I antigens.
 When there are mismatches in both class I and II antigens, the risk and
 severity of the rejection are notably increased.
\end_layout

\begin_layout Paragraph*
HLA Typing and Matching
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

 reports, in clinical practice, clinicians assess and try to match donors
 and recipients according to the number of HLA-A, -B, and -DR mismatches,
 ranging from zero mismatches (0-0-0) to a maximum of 6 mismatches (2-2-2).
 Generally, more emphasis is placed on DR loci due to the capability of
 CD4 T cell activation, which might trigger both humoral and cellular adaptive
 immune responses.
\end_layout

\begin_layout Standard
As detailed in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, HLA typing of potential donors and recipient is conducted with a 
\emph on
microcytotoxicity test
\emph default
.
 The subject's white blood cells are distributed into various wells on a
 microtiter plate, after which specific antibodies for various class I and
 class II MHC alleles are added to different wells.
 Following incubation, complement is introduced to the wells, and cytotoxicity
 is assessed by the cells' absorption of various dyes.
 If a cell has an MHC allele for which a particular antibody is specific,
 then the cell will die upon the addition of a complement, and these dead
 cells will take up a dye.
\end_layout

\begin_layout Standard
Even when a fully HLA-compatible donor is not available, the transplantation
 still can be successful.
 In this situation, a one-way mixed-lymphocyte reaction (MLR) test can be
 used to assess the degree of class II MHC compatibility.
 The irradiated (or treated with mitomycin C) potential donor's lymphocytes
 play the role of stimulator, and the unaltered recipient's lymphocytes
 take a responder role.
 Then, the proliferation of recipient cells is measured.
 The intense recipient leukocyte proliferation indicates a poor prognosis
 for graft survival.
 The MLR gives a better understanding of the degree of CD4 cell activation.
 However, it takes several days to run the assay, often making it less suitable
 in the case of cadaver transplantation compared to a quicker microcytotoxicity
 test, which takes only a couple of hours.
 Notably, even perfect HLA matching does not guarantee rejection-free transplant
ation.
 Minor histocompatibility loci mismatches can still lead to graft rejection,
 necessitating at least some level of immunosuppression even in HLA-identical
 pairings.
 It is particularly critical in kidney and bone marrow transplants, where
 HLA matching is vital, whereas heart and liver transplantation can withstand
 higher levels of mismatching.
 
\end_layout

\begin_layout Paragraph
Mechanisms of Graft Rejection
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

 further explains, graft rejection is mainly due to cell-mediated immune
 response to alloantigens, predominantly MHC antigens, present on the graft
 cells.
 The process of cell-mediated graft rejection is divided into two phases.
 The first is known as the sensitization phase, which involves the recognition
 of MHC and miH alloantigens by helper CD4 and killer CD8 cells, leading
 to their proliferation.
 The second stage, the effector stage, is where the actual destruction of
 the graft occurs.
 
\end_layout

\begin_layout Standard
A range of effector mechanisms are involved in graft rejection.
 Cell-mediated reactions involving delayed-type hypersensitivity and cytotoxic
 T lymphocyte-mediated cytotoxicity are the most prevalent.
 Less common mechanisms are antibody-plus-complement lysis (cell disintegration)
 and destruction by antibody-dependent cell-mediated cytotoxicity (ADCC).
 
\end_layout

\begin_layout Subsection
Clinical Manifestations of Graft Rejection
\end_layout

\begin_layout Standard
In continuation with 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, the onset and severity of graft rejection varies depending on the organ
 transplanted and the underlying immune response mechanisms involved.
 Rejection can be categorized into three types: hyperacute, acute, and chronic
 rejections.
\end_layout

\begin_layout Standard

\emph on
Hyperacute rejections
\emph default
 occur within the first 24 hours post-transplant.
 This type of rejection occurs due to the prior sensitization to graft antigens,
 with specific antibodies already present in the bloodstream.
 It may happen due to several reasons: recipients of repeated blood transfusions
 sometimes develop significant amounts of antibodies to MHC antigens present
 on white blood cells of the transfused blood; women, through repeated pregnanci
es, can become sensitized to paternal alloantigens of the fetus; individuals
 with previous grafts may have antibodies against alloantigens of that graft;
 and naturally occurring antibodies to blood group antigens are always present,
 although pre-transplant blood group matching has made rejections of this
 nature rare.
\end_layout

\begin_layout Standard

\emph on
Acute rejections
\emph default
 typically occur within the first few weeks after transplant.
 They are primarily mediated by adaptive immunity via T-cell responses.
\end_layout

\begin_layout Standard

\emph on
Chronic rejections
\emph default
, on the other hand, occur months to years after the transplantation and
 are mediated by both humoral and cell-mediated mechanisms.
 While the advancements in immunosuppression and tissue typing significantly
 improve short-term survival, little progress has been made in terms of
 long-term survival.
 Chronic rejections are hard to manage with immunosuppressants and may necessita
te another transplantation.
\end_layout

\begin_layout Subsection
Immunosuppression
\end_layout

\begin_layout Standard
As stated in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

, most of the available immunosuppression methods, while essential in preventing
 graft rejections, have the disadvantage of being non-specific, meaning
 they suppress immune reactions to all antigens, not only ones of the graft,
 placing the recipient at the risk of infection and, in some cases, more
 severe complications.
 For instance, some drugs target the speed of proliferation of activated
 lymphocytes.
 In doing so, they slow down the proliferation of all cells in the body.
 It can be particularly problematic for rapidly dividing cells, such as
 those of gut epithelia or bone marrow hematopoietic stem cells, placing
 patients at risk of severe or even life-threatening complications.
 Moreover, long-term use of immunosuppressive agents puts patients at risk
 of cancer, hypertension, and metabolic bone disease.
 In this section, we will delve into commonly used immunosuppressive methods.
\end_layout

\begin_layout Standard
A class of drugs known as 
\emph on
mitotic inhibitors
\emph default
 is used to inhibit rapid cell division across all cells.
 These are administered just before and after the transplantation to stop
 T-cell proliferation.
 The most commonly used mitotic inhibitors include 
\emph on
Azathioprine
\emph default
, 
\emph on
Cyclophosphamide
\emph default
, and 
\emph on
Methotrexate
\emph default
.
 However, their broad action on cell division can lead to significant adverse
 effects.
\end_layout

\begin_layout Standard

\emph on
Corticosteroids
\emph default
, another class of drugs, are often used in conjunction with mitotic inhibitors
 to prevent acute rejection.
 They have anti-inflammatory properties and act on different levels of immune
 response.
 Commonly used corticosteroids are 
\emph on
prednisone
\emph default
 and 
\emph on
dexamethasone
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe add a note about a rapamycin used in the life-extension and all that
 things from Outlive by peter attia - NO:)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Fungal metabolites, such as 
\emph on
Cyclosporine A (CsA)
\emph default
, 
\emph on
Tacrolimus
\emph default
, and 
\emph on
rapamycin,
\emph default
 are also widely used due to their potent immunosuppressive properties in
 heart, liver, kidney, and bone marrow transplants.
 A substantial drawback is their nephrotoxicity.
 While CsA has been widely used, Tacrolimus and rapamycin, being much more
 potent, might be administered at much lower doses, minimizing the side
 effects.
\end_layout

\begin_layout Standard
Radiation, known for its effectiveness in destroying lymphocytes, is another
 lymphocyte elimination method employed just before transplantation.
 In one such procedure, lymph nodes, spleen, and thymus are irradiated.
 Later, newer, more tolerant to the graft alloantigens lymphocytes will
 emerge, as the bone marrow was unaffected.
\end_layout

\begin_layout Standard
Ideal immunosuppression would be antigen-specific, targeting only alloantigens
 of the graft, preserving the recipient's ability to respond to infections.
 Such specific immunosuppression has not been achieved in humans yet, but
 animal models suggest its feasibility.
\end_layout

\begin_layout Subsection
Transplant Tolerance
\end_layout

\begin_layout Standard
Considering the detrimental effect of long-term immunosuppression, one of
 the primary objectives in transplantation is the induction of immunologic
 non-responsiveness (tolerance) to an allograft.
 The literature describes several pathways of immune non-responsiveness
 generation.
 However, it has yet to go further than animal models 
\begin_inset CommandInset citation
LatexCommand cite
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

.
 Tolerance is usually induced by prior exposure to donor antigens in a way
 that causes immune tolerance rather than sensitization in the recipient
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-32"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Future of Kidney Transplantation
\end_layout

\begin_layout Standard
Despite significant advances in recent years, the current approach to treating
 kidney failure with kidney transplantation has severe disadvantages.
 While it does save lives, its limitations make it an unsatisfactory long-term
 solution.
 Primary limitations are the limited lifespan of transplanted organs and
 a lifetime dependence on immunosuppressive drugs.
 These drugs come with their own set of problems, including an increased
 risk of infections and cancer.
 
\end_layout

\begin_layout Standard
Several solutions have been proposed over the years to address the limitations
 of kidney transplantation.
 The most promising solutions are 3D-printed organs, "ghost" organs, and
 pig-grown organs.
\end_layout

\begin_layout Paragraph
3D-printed Organs
\end_layout

\begin_layout Standard

\emph on
3D printing
\emph default
 is a manufacturing technology that has gained popularity in recent years.
 It has also found its way into the medical field, where it holds significant
 promise for solving the shortage of organs for transplantation.
 However, according to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-59"
literal "false"

\end_inset

, it faces the following challenges: 1) High cost in terms of time and money.
 3D printers, materials, and software remain very expensive.
 2) The limited ability of the material to mimic soft tissue.
 3) Poor production precision.
 It is caused by limited imaging and 3D printing resolution.
 The limited space in 3D printers and the resulting need to divide large
 models into a couple of smaller ones and the subsequent assembling also
 contribute to poor precision.
\end_layout

\begin_layout Paragraph
Lab-grown Organs
\end_layout

\begin_layout Standard
(Ott et al., 2008)
\begin_inset CommandInset citation
LatexCommand cite
key "key-57"
literal "false"

\end_inset

 and (Sánchez et al., 2015)
\begin_inset CommandInset citation
LatexCommand cite
key "key-58"
literal "false"

\end_inset

 removed cells from a human heart, leaving only an extracellular matrix
 (ECM), and then repopulated it with cells from another human.
 ECM is a structure that consists of connective tissue and gives the organ
 its form.
 The ECM does not have HLA or any other PAMPs, so it does not trigger an
 immune response.
 This approach will allow the use of organs that would otherwise be unusable
 due to incompatibilities and reduce waiting times from years to weeks or
 months.
 Also, the body will see the organ as its own, so immunosuppressants will
 not be needed.
 Theoretically, even non-human organ ECM can be utilized for transplantation
 to humans.
 Combined with recent advancements in stem cell production, this approach
 looks very promising.
\begin_inset Note Note
status open

\begin_layout Plain Layout
reference some paper that tells about making stem cells from fat
\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Pig-grown organs
\end_layout

\begin_layout Standard
Finally, pig-grown organs have also been researched as a potential solution
 to the organ shortage problem.
 Given the anatomical and physiological similarities between pigs and humans,
 xenotransplantation has emerged as a viable approach 
\begin_inset CommandInset citation
LatexCommand cite
key "key-60"
literal "false"

\end_inset

.
 For instance, (Nagashima & Hitomi Matsunari, 2016) injected pancreatic
 progenitor cells into a pig fetus with an apancreatic trait and observed
 successful fetal development and pancreatic function.
 Although the approach appears promising, how it would work with human organs
 is unknown.
\end_layout

\begin_layout Chapter
Machine Learning Background
\begin_inset CommandInset label
LatexCommand label
name "chap:Machine-Learning-Background"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
cit konkretnejic tim hur
\end_layout

\begin_layout Plain Layout
- long rank popsat
\end_layout

\begin_layout Plain Layout
na taky popsat
\end_layout

\begin_layout Plain Layout
vysvetit proc featuru josu ruzne v L Dc
\end_layout

\begin_layout Plain Layout
EDA ocekava porovnatelne a obecne data bez zamereni na finalni modely.
\end_layout

\begin_layout Plain Layout
rict o 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Machine learning is a subfield of computer science that consists of building
 algorithms capable of processing large amounts of data, finding patterns,
 and performing actions such as predictions or generating new data.
 It is an intersection of many fields of science, such as statistics, theory
 of probability, linear algebra, calculus, and certainly, computer science.
 
\end_layout

\begin_layout Standard
As detailed in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

, machine learning excels in problems that are either overly complex or
 have no known algorithm.
 It has the potential to help us create new knowledge by uncovering previously
 unknown correlations within the data.
 It might also make fewer errors in decision-making than humans.
\end_layout

\begin_layout Standard
Every field is affected by human errors, and medicine is no exception.
 Machine learning also makes mistakes, but achieving machine learning error
 even 1% below the human error rate would be a remarkable accomplishment.
 The human body is a complex system, and it is very difficult for the human
 mind to comprehend all processes and how they relate to each other.
 Machine learning might be able to do that better than any human could.
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

 further explains, based on the problem and an approach to building a dataset
 and the model, there are four types of learning: 
\emph on
supervised
\emph default
, 
\emph on
semi-supervised
\emph default
, 
\emph on
unsupervised
\emph default
, and 
\emph on
reinforcement learning
\emph default
.
 
\end_layout

\begin_layout Standard

\emph on
Supervised learning
\emph default
 deals with labeled data, meaning that training data contains the desired
 solutions, referred to as 
\emph on
labels
\emph default
.
\end_layout

\begin_layout Standard

\emph on
Semi-supervised
\emph default
 
\emph on
learning
\emph default
 deals with partially labeled data, which needs to be labeled fully either
 manually, or using techniques such as 
\emph on
clustering
\emph default
.
 
\end_layout

\begin_layout Standard

\emph on
Unsupervised learning
\emph default
 deals with unlabeled data.
\end_layout

\begin_layout Standard
In 
\emph on
reinforcement learning
\emph default
, we create an environment, set up rewards for performing certain actions
 and punishment for others, and let the machine (actor) perform actions
 that produce the highest reward.
\end_layout

\begin_layout Standard
In this chapter, we will cover all theoretical backgrounds that might prove
 useful for solving our problem, including classical machine learning, statistic
al survival analysis, basic steps that are required to create machine learning
 systems, and data preprocessing.
 We will begin by exploring supervised learning.
\end_layout

\begin_layout Section
Supervised Learning 
\end_layout

\begin_layout Standard
Drawing heavily on 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

 this section explores supervised learning and its algorithms.
 
\emph on
Supervised learning
\emph default
 is the process of training a model on data where the outcome is known,
 to make predictions for data where the outcome is not known 
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"
literal "false"

\end_inset

.
 
\emph on
Classification
\emph default
 and 
\emph on
regression
\emph default
 are common supervised learning tasks.
 In this section we will define these problems and the necessary terminology,
 and describe commonly used algorithms that are used to solve these types
 of problems.
\end_layout

\begin_layout Standard
In supervised learning the 
\emph on
dataset
\emph default
 is the collection of labeled samples 
\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, where each individual 
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is called a 
\emph on
feature vector
\emph default
.
 A feature vector is a vector that in each its dimension 
\begin_inset Formula $j=1,...,D$
\end_inset

 contains a value that describes a sample in some way.
 This value is called a 
\emph on
feature
\emph default
 and is denoted as 
\begin_inset Formula $x^{(j)}$
\end_inset

.
 The 
\emph on
label
\emph default
 
\begin_inset Formula $y^{i}$
\end_inset

 might be either a finite set of classes 
\begin_inset Formula $\{1,2,...,C\}$
\end_inset

, in case of a classification task, or a real number, a vector, a matrix
 or graph, in case of a regression.
 The goal of supervised learning algorithm is to create a model using the
 dataset that will take the feature vector as an input and produce a label
 or a more complex structure as an output.
\end_layout

\begin_layout Standard
Classification is a problem of assigning a label to an unlabeled sample.
 This problem is solved by a classification learning algorithm that takes
 a labeled set of samples as input and produces a model that takes an unlabeled
 sample as input and outputs a label.
 If the set of labels has only two classes we talk about 
\emph on
binary classification
\emph default
.
 Consequently, if the set of labels has three or more classes, it is a 
\emph on
multiclass classification
\emph default
.
 Some algorithms are binary classifiers by definition while others are multiclas
s classifiers.
 It is possible to create an 
\emph on
ensemble
\emph default
 out of binary classifiers that will be able to perform multiclass classificatio
n.
 An ensemble is a combination of algorithms that are connected to perform
 one task.
\end_layout

\begin_layout Standard
Regression is a problem of predicting a 
\emph on
target value
\emph default
 from an unlabeled example.
 The problem is solved by a regression learning algorithm that takes a set
 of labeled samples as input, and produces a model that takes an unlabeled
 sample as input and outputs a target value.
 
\end_layout

\begin_layout Standard
Classification and regression tasks are similar in many ways and often for
 each classifier there is an equivalent regressor, and vice versa.
 In the following subsections we are going to explore some techniques for
 supervised learning.
\end_layout

\begin_layout Subsection
Linear Regression 
\end_layout

\begin_layout Standard

\emph on
Linear regression 
\emph default
is a commonly used algorithm for regression learning.
 The resulting model is a linear combination of all features 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The problem formulation we are trying to solve is as follows:
\series bold
 
\series default
Given a collection of labeled samples
\series bold
 
\series default

\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, create a model 
\begin_inset Formula 
\begin{equation}
f_{\bar{w},b}(\overline{x})=\overline{w}\overline{x}+b,\label{eq:lin_reg}
\end{equation}

\end_inset

 where N is the size of the collection, 
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is a 
\emph on
feature vector 
\emph default
of D dimensions of sample 
\begin_inset Formula $i=1,…,N$
\end_inset

, every feature 
\begin_inset Formula $x_{i}^{(j)}\epsilon\mathbb{R},j=1,...,D,$
\end_inset

 
\begin_inset Formula $y_{i}\epsilon\mathbb{R}$
\end_inset

 is the 
\emph on
target value
\emph default
.
 
\begin_inset Formula $\overline{w}$
\end_inset

 is a D-dimensional vector of parameters and 
\begin_inset Formula $b\epsilon\mathbb{R}$
\end_inset

.
 The notation 
\begin_inset Formula $f_{\bar{w},b}(\overline{x})$
\end_inset

 means that 
\begin_inset Formula $f$
\end_inset

 is parametrized by 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
To train the linear regression means to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 In graphical terms, it means finding such a hyperplane that fits data points
 from the training set as well as possible, as shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linear-regression-for"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/linear_regression.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Linear regression for Two-Dimensional Data
\begin_inset CommandInset label
LatexCommand label
name "fig:Linear-regression-for"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
To find optimal parameters, we need to minimize
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{1}{N}\mathop{\underset{i=1...N}{\sum}(}f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}.\label{eq:mse}
\end{equation}

\end_inset

 This expression is called a 
\emph on
mean squared error (MSE), 
\emph default
the 
\emph on
loss function 
\emph default
that comprises of 
\emph on
squared error loss 
\emph default

\begin_inset Formula $(f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}$
\end_inset

, another loss function
\emph on
 
\emph default
that
\emph on
 
\emph default
evaluates individual predictions
\emph on
.
 
\emph default
The loss function measures the model's overall performance (MSE) or evaluates
 each prediction (square error loss) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
There is a closed-form solution for finding optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

.
 A 
\emph on
closed-form solution
\emph default
 is a simple algebraic expression that gives the result directly.
 In the case of linear regression, it is the 
\emph on
normal equation
\emph default
, and it is denoted by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{\overline{\mathbf{w}}^{*}=(x^{T}x)^{-1}x^{T}y},\label{eq:norm_eq}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $x^{T}$
\end_inset

means transposed feature matrix 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
We could select another loss function, but according to Andriy Burkov, it
 would be a different algorithm.
 For example, we could take the absolute difference between 
\begin_inset Formula $f(x_{i})$
\end_inset

 and 
\begin_inset Formula $y_{i}$
\end_inset

, but that would create problems as the derivative of absolute value is
 not continuous.
 Therefore, the function is not smooth, which might create unnecessary complicat
ions during optimization.
 
\end_layout

\begin_layout Standard
Linear models are usually resilient to overfitting because they are simple.
 The model overfits when it learns the intricacies of the training dataset
 so well that it remembers actual values instead of learning the underlying
 pattern.
 Such a model cannot make accurate predictions when confronted with unseen
 data.
 More on overfitting in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Training-and-Hyperparameter-tuning"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Logistic Regression 
\end_layout

\begin_layout Standard

\emph on
Logistic regression 
\emph default
is a binary classifier that estimates the probability of an example belonging
 to a particular class 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 If the predicted probability of the instance belonging to a class is greater
 than 50%, then the model concludes that it belongs to the class (referred
 to as positive class and labeled as 1).
 Otherwise, it predicts that the example does not belong to that class (but
 belongs to the negative class, labeled 0) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Logistic regression comes from statistics, where its mathematical formulation
 is similar to a regression, hence the name 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 Multiclass classification is available in softmax regression, a multiclass
 variant of logistic regression 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
As with linear regression, in logistic regression, we want to model 
\begin_inset Formula $y$
\end_inset

 as a linear combination of 
\begin_inset Formula $\overline{x}$
\end_inset

, but in this case, it is more complex 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The logistic regression model is denoted by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
f_{\bar{w},b}(\overline{x})\stackrel{def}{=}\frac{1}{1+e^{-(wx+b)}}.\label{eq:log_reg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Similar to linear regression, our task is to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Once we found 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

, in other words, we trained the model, we can apply the model 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

 on features 
\begin_inset Formula $x_{i}$
\end_inset

 from an example 
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

.
 The output value lies in the range 
\begin_inset Formula $0<p<1$
\end_inset

 .
 If 
\begin_inset Formula $y_{i}$
\end_inset

 is a positive class, the likelihood of 
\begin_inset Formula $y_{i}$
\end_inset

 being a positive class is given by 
\begin_inset Formula $p$
\end_inset

.
 Consequently, if 
\begin_inset Formula $y_{i}$
\end_inset

 is the negative class, the likelihood of it being the negative class is
 given by 
\begin_inset Formula $1-p$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/logistic_regression.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Logistic Function 
\begin_inset CommandInset label
LatexCommand label
name "fig:Logistic-function."

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-function."
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrates the logistic function.
 The 
\begin_inset Formula $y$
\end_inset

-axis is a decision boundary whose value corresponds to the predicted probabilit
y.
 The 
\begin_inset Formula $x$
\end_inset

-axis corresponds to a feature's value.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Logistic-function."
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see that when 
\begin_inset Formula $y$
\end_inset

 is less than 
\begin_inset Formula $\frac{1}{2}$
\end_inset

, the corresponding 
\begin_inset Formula $x$
\end_inset

 values are negative, classifying it as a negative class.
 Conversely, if 
\begin_inset Formula $y$
\end_inset

 is greater than 
\begin_inset Formula $\frac{1}{2}$
\end_inset

, it is classified as positive.
 However, the specific threshold may vary depending on the context.
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

, in logistic regression, instead of 
\emph on
minimizing
\emph default
 MSE, we try to 
\emph on
maximize
\emph default
 the 
\emph on
likelihood function
\emph default
.
 In statistics, the likelihood function tells how likely the example is
 according to our model.
 The objective function in logistic regression is called 
\emph on
maximum likelihood
\emph default
.
 It is given by: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L_{\bar{w},b}\stackrel{def}{=}\underset{i=1...N}{\prod}f_{\bar{w},b}(\overline{x}_{i})^{y_{i}}(1-f_{\bar{w},b}(\overline{x}_{i}))^{(1-y_{i})}.\label{eq:max_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
On the other hand, due to the exponential function in equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:max_likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

, it is better to use the 
\emph on
log-likelihood
\emph default
 instead to make calculations easier.
 As 
\begin_inset Formula $Log$
\end_inset

 is a strictly increasing function, maximizing it is the same as maximizing
 its argument.
 The solution to this optimization problem is the same as the solution to
 the original problem.
 The log-likelihood function is expressed as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
LogL_{\bar{w},b}\stackrel{def}{=}ln(L_{\overline{w},b}(\overline{x}))\stackrel[i=1]{N}{\sum}y_{i}lnf_{\bar{w},b}(\overline{x})+(1-y_{i})ln(1-f_{\bar{w},b}(\overline{x})).\label{eq:log_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Unfortunately, this optimization problem has no closed-form solution 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9,key-10"
literal "false"

\end_inset

.
 Nonetheless, the function is convex.
 Hence, gradient descent (or any other optimization algorithm) guarantees
 the finding of the global minimum, provided that the learning rate is not
 too large and enough time is given 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Subsection
Support Vector Machines 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Support-Vector-Machines"

\end_inset


\end_layout

\begin_layout Standard

\emph on
Support vector machine (SVM)
\emph default
 is a widely used and powerful machine learning algorithm that can perform
 a wide range of tasks, including linear and nonlinear classification, regressio
n, and outlier detection on small- to medium-sized datasets 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection*

\series bold
Linear SVM
\end_layout

\begin_layout Standard
In its classical formulation, the support vector machine is a binary classifier.
 Classes are called positive and negative and are labeled +1 and -1, respectivel
y.
 
\end_layout

\begin_layout Standard
The model is described by the equation 
\begin_inset Formula 
\[
f(x)=sign(\overline{w}\overline{x}-b).
\]

\end_inset


\end_layout

\begin_layout Standard
The function 
\begin_inset Formula $sign$
\end_inset

 returns +1 if the input is positive and -1 if negative.
 To train the SVM means to find optimal values 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters 
\begin_inset Formula $\overline{w}$
\end_inset

 and 
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 The process of finding 
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 is called training.
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SVM-demonstration-for"
plural "false"
caps "false"
noprefix "false"

\end_inset

 demonstrates the concept behind support vector machines.
 The image consists of two classes represented by red and blue dots, divided
 by a solid line termed the 
\emph on
decision boundary
\emph default
 
\begin_inset Formula $\overline{w}\overline{x}-b=0$
\end_inset

, with two dashed lines by its sides known as 
\emph on
support vectors
\emph default
 
\begin_inset Formula $\overline{w}\overline{x}-b=1$
\end_inset

 and 
\begin_inset Formula $\overline{w}\overline{x}-b=-1$
\end_inset

.
 Support vectors are defined by the closest instances of a class to the
 decision boundary.
 These instances are emphasized in the figure.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/svm1.pdf
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
SVM Demonstration for Two-Dimensional Dataset
\begin_inset CommandInset label
LatexCommand label
name "fig:SVM-demonstration-for"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The distance between the closest instances of two classes is called 
\emph on
margin 
\emph default
and is equal to 
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, where 
\begin_inset Formula $||\overline{w}||$
\end_inset

 is the Euclidean norm and 
\begin_inset Formula $\overline{w}$
\end_inset

 is a parameter vector of the same dimensionality as the feature vector.
 Thus, the smaller the norm, the larger the margin.
 The larger the margin, the better the model's generalization.
 The primary objective of the model is to find the largest possible margin
 
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, so, to do that, we need to 
\emph on
minimize 
\emph default
the Euclidian norm defined by the expression
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
||\overline{w}||=\sqrt{\stackrel[j=1]{D}{\sum}(w^{(j)})^{2}}.
\]

\end_inset

 
\end_layout

\begin_layout Standard
The fundamental assumption of support vector machines is that classes are
 linearly separable, implying their instances can be separated by a hyperplane
 (decision boundary) with no examples of one class lying among the ones
 of the opposite class.
 It is illustrated in the figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Linearly-non-separable"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In this case, the algorithm cannot find an optimal solution with no instances
 lying between the support vectors and the decision boundary.
 Consequently, the model is susceptible to outliers.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/svm2.pdf
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Linearly Inseparable Dataset 
\begin_inset CommandInset label
LatexCommand label
name "fig:Linearly-non-separable"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Every optimization problem requires constraints, and for the support vector
 machine, they are the following:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{w}\overline{x_{i}}-b\geqslant+1$
\end_inset

 if 
\begin_inset Formula $y_{i}=+1$
\end_inset


\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{w}\overline{x_{i}}-b\leqslant-1$
\end_inset

 if 
\begin_inset Formula $y_{i}=-1$
\end_inset

.
\end_layout

\begin_layout Standard
These two equations can be reduced to one 
\begin_inset Formula $y_{i}(\overline{w}\overline{x}-b)\geqslant1$
\end_inset

.
\end_layout

\begin_layout Standard
The optimization problem we want to solve is the following: Minimize 
\begin_inset Formula $||\overline{w}||$
\end_inset

 subject to constraint 
\begin_inset Formula $y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1$
\end_inset

 for 
\begin_inset Formula $i=1,...,N$
\end_inset

, where N is the number of features.
 This problem can be modified so that the quadratic programming techniques
 can be used in the optimization process.
 The modified formula is 
\begin_inset Formula $\frac{1}{2}||\overline{w}||^{2}$
\end_inset

, and minimization of it would also mean minimization of 
\begin_inset Formula $||\overline{w}||$
\end_inset

.
 The updated optimization problem looks like this:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
min\frac{1}{2}||\overline{w}||^{2}\text{such that }y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1,i=1,...,N\label{eq:svm_optimisation}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsubsection*

\series bold
Handling Noise
\end_layout

\begin_layout Standard
To introduce the ability of SVM to handle nonlinearly separable data (but
 not to the extreme), we define the hinge loss function: 
\begin_inset Formula $max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)).$
\end_inset

 It is zero if the constraints 1 and 2 are satisfied.
 If it is not, the data point does not lie on the right side of the decision
 boundary.
 The function value is proportional to the distance from the decision boundary.
 The resulting cost function looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
C||\overline{w}||^{2}+\frac{1}{N}\stackrel[j=1]{N}{\sum}max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)),\label{eq:svm_loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where C is the hyperparameter that determines the trade-off between increasing
 the size of the decision boundary and ensuring that each 
\begin_inset Formula $x_{i}$
\end_inset

 lies on the correct side of the decision boundary.
 Its value is chosen experimentally.
 C handles the trade-off between classifying the training data well and
 classifying future examples well (generalization).
 For higher values of C, the misclassification error will be almost negligible,
 so the algorithm will try to find the highest margin without considering
 it.
 For lower values of C, the algorithm will try to make fewer mistakes by
 sacrificing the margin size.
 (A larger margin is better for the generalization.) Lower values lead to
 wider margins and more margin violations.
 Higher values lead to narrower margins and fewer margin violations.
\end_layout

\begin_layout Standard
SVM with the hinge loss function is called 
\emph on
soft-margin SVM,
\emph default
 while the original formulation that optimizes the Euclidian norm is referred
 to as 
\emph on
hard-margin SVM
\emph default
.
 
\emph on
Soft margin classification
\emph default
 tries to mitigate the downsides of 
\emph on
hard margin classification
\emph default
 by finding a balance between keeping the margin as large as possible and
 mitigating margin outliers (instances that lie on the margin or on the
 opposite side).
\end_layout

\begin_layout Subsubsection*

\series bold
Handling Non-linearity
\series default
 
\end_layout

\begin_layout Standard
We can adapt SVM to work with nonlinearly separable datasets by applying
 the kernel trick.
 The kernel trick means transforming the original space to a higher dimensional
 one during the cost function optimization with the hope that, in higher
 dimensional space, it will become linearly separable.
 Mathematically, the kernel trick is mapping 
\begin_inset Formula $\varphi:\overline{x}\rightarrow\varphi(\overline{x})$
\end_inset

, where 
\begin_inset Formula $\varphi(\overline{x})$
\end_inset

 is a vector of higher dimensionality than 
\begin_inset Formula $\overline{x}.$
\end_inset

 The kernel trick allows us to save a lot of non-necessary computations.
 Linear, polynomial, and radial basis function (RBF) are the most widely
 used kernel functions.
\end_layout

\begin_layout Subsection
Decision Trees
\begin_inset CommandInset label
LatexCommand label
name "subsec:Decision-Trees"

\end_inset


\end_layout

\begin_layout Standard
A Decision Tree is a versatile and highly interpretable machine learning
 algorithm that can perform both classification and regression tasks.
 It is a critical component of 
\emph on
GradientBoostedSurvivalAnalysis
\emph default
.
 Its extension, the Survival Tree, is also an essential component of the
 
\emph on
Random Survival Forest
\emph default
.
 Both ensemble methods are used in this work and are discussed in Sections
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Gradient-Boosted-Survival"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Random-Survival-Forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

, respectively.
 Scikit-learn's 
\emph on
DecisionTreeRegressor
\emph default
 is utilized in 
\emph on
GradientBoostedSurvival Analysis,
\emph default
 so we will focus on regression rather than classification.
 Scikit-learn employs the 
\emph on
Classification and Regression Tree (CART)
\emph default
 algorithm to train decision trees, so let us look into this algorithm in
 more detail.
\end_layout

\begin_layout Standard
In each node, the dataset is recursively split in two based on a feature
 
\begin_inset Formula $k$
\end_inset

 and a threshold 
\begin_inset Formula $t_{k}.$
\end_inset

 Such pair 
\begin_inset Formula $(k,t_{k})$
\end_inset

 is chosen that produces the 
\begin_inset Quotes sld
\end_inset

purest
\begin_inset Quotes srd
\end_inset

 subsets.
 A node is pure if all training instances it applies to belong to the same
 class.
 The following impurity methods are common: Gini impurity and entropy for
 classification and MSE for regression.
\end_layout

\begin_layout Standard
Gini impurity is expressed as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
G_{i}=1-\sum_{k=1}^{n}p_{i,k}^{2}
\]

\end_inset

, where 
\begin_inset Formula $p_{i,k}$
\end_inset

 is the ratio of 
\begin_inset Formula $k$
\end_inset

 class instances among the training instances in the 
\begin_inset Formula $i^{th}$
\end_inset

 node.
 Entropy is a term originating from physics, where it is a measure of molecular
 disorder: it is zero if molecules are well-ordered and still.
 In machine learning, it is used as a measure of impurity and is denoted
 as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
H_{i}=-\stackrel[p_{i,k}\neq0]{n}{\sum_{k=1}}p_{i,k}\log_{2}(p_{i,k}).
\]

\end_inset


\end_layout

\begin_layout Standard
Consequently, the entropy is zero if all instances are of the same class.
\end_layout

\begin_layout Standard
In regression, CART tries to split the dataset in each node to minimize
 mean squared error (MSE) (seen previously in Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:mse"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 The cost function for the decision tree regression is denoted as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
J(k,t_{k})=\frac{m_{left}}{m}MSE_{left}+\frac{m_{right}}{m}MSE_{right}\text{ where }\begin{cases}
MSE_{node}=\sum_{i\in node}(\hat{y}_{node}-y^{(i)})^{2}\\
\hat{y}=\frac{1}{m_{node}}\sum_{i\in node}y^{(i)}
\end{cases}\label{eq:DT-regression-loss}
\end{equation}

\end_inset

, where 
\begin_inset Formula $m$
\end_inset

 is the number of instances in the dataset, and 
\emph on
left
\emph default
 and 
\emph on
right
\emph default
 mean that the value relates to the left or right node of the tree.
 In classification, the loss function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:DT-regression-loss"
plural "false"
caps "false"
noprefix "false"

\end_inset

 would contain Gini (G) or entropy (H) instead of MSE.
 
\end_layout

\begin_layout Standard
It is worth noting that the other algorithms train the Decision Tree without
 restrictions (for example.
 the approach used in Survival Trees).
 Then, during the 
\emph on
pruning
\emph default
 stage, all unnecessary nodes are removed.
 A node is considered redundant if its purity improvement is not statistically
 significant.
 Statistical tests such as 
\begin_inset Formula $\chi^{2}$
\end_inset

 test are used to estimate the probability of the improvement being the
 result of a chance (the null hypothesis).
 If the probability (p-value) exceeds a certain threshold (often 5%), the
 node is deemed unnecessary, and its children are deleted.
\end_layout

\begin_layout Standard
Linear algorithms, like linear regression, assume that data are linear.
 In contrast, decision trees make no such assumption about the data.
 As a result, Decision Trees are classified as nonparametric algorithms
 because the number of parameters is not predefined before training.
 This flexibility allows the algorithm to fit the data closely, possibly
 leading to overfitting.
 Consequently, the algorithm needs to be regularized to prevent overfitting.
 The following hyperparameters can help with regularization and preventing
 overfitting:
\end_layout

\begin_layout Itemize

\emph on
max_depth
\emph default
 - specifies the maximum depth of the decision tree.
 The most important hyperparameter.
\end_layout

\begin_layout Itemize

\emph on
min_samples_split
\emph default
 - sets the minimum number of samples a node must have before it can be
 split.
\end_layout

\begin_layout Itemize

\emph on
min_samples_leaf
\emph default
 - dictates the minimum number of samples a leaf node must have.
\end_layout

\begin_layout Itemize

\emph on
max_leaf_nodes
\emph default
 - determines the maximum number of leaf nodes.
\end_layout

\begin_layout Itemize

\emph on
max_features
\emph default
 - sets the maximum number of features evaluated for splitting at each node.
 Supports the following values: integer, float, square root of all features
 (
\begin_inset Quotes sld
\end_inset

sqrt
\begin_inset Quotes srd
\end_inset

), 
\begin_inset Formula $log_{2}$
\end_inset

 of the number of features (
\begin_inset Quotes sld
\end_inset

log2
\begin_inset Quotes srd
\end_inset

), and None (all features)
\end_layout

\begin_layout Standard
The most critical parameter to limit is 
\emph on
max_depth
\emph default
.
 Increasing 
\emph on
min_*
\emph default
 and decreasing 
\emph on
max_*
\emph default
 will help to regularize the model.
 There are other hyperparameters that are not related to regularization:
\end_layout

\begin_layout Itemize

\emph on
criterion
\emph default
 hyperparameter specifies the function to assess the quality of a split.
 In the scenario described above this is MSE.
\end_layout

\begin_layout Itemize

\emph on
splitter
\emph default
 defines the strategy for choose the split at each node.
 There are two available splitters: 
\begin_inset Quotes sld
\end_inset

best
\begin_inset Quotes srd
\end_inset

 to choose the best split and 
\begin_inset Quotes sld
\end_inset

random
\begin_inset Quotes srd
\end_inset

 to choose the random split.
 
\end_layout

\begin_layout Itemize

\emph on
random_state
\emph default
 defines the randomness of the estimator, as features are randomly permuted
 at each split.
\end_layout

\begin_layout Standard
For more information on the hyperparameters, refer to the scikit-learn documenta
tion 
\begin_inset CommandInset citation
LatexCommand cite
key "key-74"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Once the decision tree is built and we need to make a prediction, the data
 instance is passed through the tree.
 It begins at the top node, known as the root node, and then it moves to
 the left or right node based on the instance's value for the selected feature
 of the node.
 In this way, the instance traverses through all the descendant nodes until
 it arrives at the terminal node, also known as the leaf node.
 The target value at that node will serve as the prediction, whether for
 regression or classification.
 In the case of regression, this value represents the average of all the
 target values that led to the leaf node.
\end_layout

\begin_layout Standard
Now, let us discuss several limitations.
 The first one is that the Decision Tree is a greedy algorithm, which means
 that finding an optimal model is nearly impossible since it's an NP-complete
 problem.
 NP-complete problems are NP (solutions can be verified in polynomial time)
 and NP-hard (any NP problem can be reduced to it in polynomial time).
 Additionally, decision trees are sensitive to small changes in data.
 This problem can be addressed by using PCA or opting for the ensemble models
 mentioned earlier, which are generally able to mitigate this instability.
\end_layout

\begin_layout Subsection
Stochastic Gradient Boosting 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Stochastic-Gradient-Boosting"

\end_inset


\end_layout

\begin_layout Standard
Boosting is an approach to building an ensemble model, where weak learners
 are trained and sequentially added to the model, each trying to correct
 its predecessor.
 The most common boosting algorithms are AdaBoost and Gradient Boosting.
 Gradient Boosting is an
\begin_inset Note Note
status open

\begin_layout Plain Layout
a framework for building many algorithms by optimizing different loss functions
\end_layout

\end_inset

 ensemble machine learning method that uses Decision Trees as a base learner.
 Each subsequent learner is trained to fit the residuals of the previous
 model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Residuals are typically the difference between the predicted values and
 the actual values.
 In this context, however, they are called pseudo-residuals, as they represent
 the negative gradients of the loss function.
 This section will look into the modification of Gradient Boosting, called
 Stochastic Gradient Boosting, presented in Friedman's work 
\begin_inset CommandInset citation
LatexCommand cite
key "key-69"
literal "false"

\end_inset

.
 The only difference from the original Gradient Boosting is that it draws
 a subset of random instances from the training set to compute the negative
 gradient (step 2) and train the base learner (step 3) instead of using
 the whole training set.
 We have chosen it over the original to shorten the training time.
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $\{(\mathbf{x}_{i},y_{i})\}_{i=0}^{n}$
\end_inset

 be the training set, 
\begin_inset Formula $L(y_{i},F(\mathbf{x}))$
\end_inset

 be a differentiable loss function, and 
\begin_inset Formula $\{\pi(i)\}_{1}^{N}$
\end_inset

 be a random permutation of integers 
\begin_inset Formula $\{1,...,N\}$
\end_inset

.
 Then a subset of random instances of size 
\begin_inset Formula $\widetilde{N}<N$
\end_inset

 is given by 
\begin_inset Formula $\{(\mathbf{x}_{\pi(i)},y_{\pi(i)})\}_{i=0}^{\widetilde{N}}$
\end_inset

and the Stochastic Gradient Boosting is denoted as follows:
\end_layout

\begin_layout Standard
Initialize the model 
\begin_inset Formula $F_{0}(\mathbf{x})=\underset{\gamma}{\arg\min}\sum_{i=1}^{N}L(y_{i},\gamma)$
\end_inset

, where 
\begin_inset Formula $\gamma$
\end_inset

 is the predicted value that minimizes the total error.
\end_layout

\begin_layout Standard
For each tree 
\begin_inset Formula $m=1,...,M$
\end_inset

 (where 
\begin_inset Formula $M$
\end_inset

 is the number of treesspecified by the hyperparameter 
\emph on
n_estimators
\emph default
) perform the following steps:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\{\pi(i)\}_{1}^{N}=rand\_perm\{i\}_{1}^{N}$
\end_inset


\end_layout

\begin_layout Enumerate
Compute the negative gradient (pseudo-residual)
\begin_inset Formula 
\[
\tilde{y_{\pi(i),m}}=\left[\frac{\partial L(y_{\pi(i)},F(\mathbf{x}_{\pi(i)}))}{\partial F(\mathbf{x}_{\pi(i)})}\right]_{F(\mathbf{x)=F_{m-1}(}\mathbf{x})},i=1,...,\tilde{N}
\]

\end_inset


\end_layout

\begin_layout Enumerate
Fit a regression tree to the residuals 
\begin_inset Formula $\tilde{y}_{\pi(i),m}$
\end_inset

 and create terminal regions (leaves) 
\begin_inset Formula $R_{lm}$
\end_inset

 for 
\begin_inset Formula $l=1,...,L$
\end_inset

, where 
\begin_inset Formula $l$
\end_inset

 is each leaf in a tree 
\begin_inset Formula $m$
\end_inset

) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-70"
literal "false"

\end_inset

 
\end_layout

\begin_layout Enumerate
For 
\begin_inset Formula $l=1,...,L_{m}$
\end_inset

 compute 
\begin_inset Formula $\gamma_{l,m}=\underset{\gamma}{\arg\min}\sum_{\mathbf{x}_{\pi(i)}\in R_{l,m}}L(y_{\pi(i)},F_{m-1}(\mathbf{x}_{\pi(i)})+\gamma)$
\end_inset

 – determine the output value 
\begin_inset Formula $\gamma$
\end_inset

 for each leaf 
\begin_inset Formula $l$
\end_inset

 that minimizes the error.
 
\end_layout

\begin_layout Enumerate
Update 
\begin_inset Formula $F_{m}(x)=F_{m-1}(\mathbf{x})+\nu\cdot\gamma_{l,m}I(\mathbf{x}\in R_{l,m})$
\end_inset

.
 Where 
\begin_inset Formula $\nu\in[0,1]$
\end_inset

 is a learning rate (or shrinkage parameter) which regulates how much influence
 each trained tree has on the final result.
 Generally, lower values are recommended, necessitating the increase of
 the number of estimators.
 Such combination generally leads to better generalization.
 
\begin_inset Formula $I()$
\end_inset

 is an indicator function and it is equal to one, only if 
\begin_inset Formula $\mathbf{x}$
\end_inset

is in this region (leaf) 
\begin_inset Formula $R_{l,m}$
\end_inset

, and zero otherwise.
\end_layout

\begin_layout Standard
\begin_inset Formula $\widetilde{N}=N$
\end_inset

 will produce no randomness, effectively resulting in the original Gradient
 Boosting algorithm.
 A smaller fraction, 
\begin_inset Formula $\frac{\widetilde{N}}{N}$
\end_inset

, introduces a trade-off: it increases randomness and decreases training
 time but also leads to higher variance in the learner.
 So, this value must be chosen wisely.
 
\end_layout

\begin_layout Standard
Friedman suggested various algorithms based on this template with different
 loss functions for regression and classification.
 Ridgeway 
\begin_inset CommandInset citation
LatexCommand cite
key "key-70"
literal "false"

\end_inset

 extended this algorithm to survival analysis, a branch of statistics that
 studies time until event (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Survival-Analysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

 explains it in more detail), by using the Cox Proportional Hazards loss.
 More on that is Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Gradient-Boosted-Survival"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which is dedicated to the algorithm used in the practical part.
\end_layout

\begin_layout Section
Unsupervised Learning
\end_layout

\begin_layout Standard

\emph on
Unsupervised learning 
\emph default
deals with a dataset without labels 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 It has three main techniques: clustering, dimensionality reduction, and
 anomaly detection 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Clustering
\emph default
 is a method that identifies similar instances and groups them into sets.
 It has applications in data analysis, namely, 
\emph on
exploratory data analysis (EDA),
\emph default
 customer segmentation, dimensionality reduction, and anomaly detection
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Clustering might be either soft, where an instance has a score of belonging
 to a particular cluster, or hard, where an instance belongs to only one
 class.
 The score might be the distance from the cluster centroid or an affinity
 (similarity score) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\emph on
Dimensionality reduction
\emph default
 is useful for visualization and learning acceleration 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Datasets often have a lot of redundant data, or the task requires a lot
 of features.
 Many algorithms, such as linear models, SVMs, and decision trees, might
 have their performances compromised due to high-dimensional data.
 So-called 
\emph on
curse of dimensionality
\emph default
 states that high dimensional data can cause slow learning and prevent us
 from getting an optimal model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Consequently, the reduction of the data dimensionality might be a good
 idea.
 However, it is worth noting that a dimensionality reduction algorithm might
 lose some useful information.
 Many modern algorithms, such as neural networks or ensemble algorithms,
 handle high dimensional data very well, and dimensionality reduction techniques
 are used less than in the past 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 However, they are still used for data visualization and cases when we need
 to build an interpretable model while we are limited in the number of algorithm
s we can use 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Anomaly (outlier) detection
\emph default
 involves the detection of instances strongly deviating from the norm 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 These instances are called 
\emph on
outliers
\emph default
 or anomalies, while regular ones are referred to as 
\emph on
inliers
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 Anomaly detection has many applications.
 For example, it can be used as a data preprocessing step to remove outliers
 from the dataset, which might improve the performance of the resulting
 model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 In addition, it is used in the 
\emph on
fraud detection
\emph default
 task and the detection of faulty products in manufacturing facilities 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Novelty detection 
\emph default
is closely related to anomaly detection.
 The only difference is that novelty detection assumes that outliers did
 not contaminate the training dataset, while anomaly detection does not
 make this assumption 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Data Preparation
\end_layout

\begin_layout Standard
Due to factors such as the curse of dimensionality and inherent noise, we
 cannot load raw data to an algorithm and expect good performance 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 Most often, the raw data has too many features, and most of them have very
 little predictive power.
 We need to build a dataset first.
 
\emph on
Feature engineering
\emph default
 is responsible for transforming raw data into a dataset 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 It is a labor-demanding process that requires creativity and, most importantly,
 domain knowledge 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The objective of this stage is to create 
\emph on
informative
\emph default
 features or features with 
\emph on
high predictive power
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 For example, in our task of predicting survival time, donor-recipient blood
 group compatibility or recipient's age are likely to have much higher predictiv
e power than the donor's or recipient's citizenship.
 Blood group compatibility is directly linked to the successful transplantation,
 while the recipient's citizenship has little correlation with the genetic
 similarities, which we would expect from datasets from other countries,
 but we use the dataset from the United States - a country with very diverse
 population.
\end_layout

\begin_layout Standard
Moreover, it is possible to create new features with higher predictive power
 out of those with low predictive power 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 For example, the calculation of 
\emph on
estimated Glomerular Filtration Rate (eGFR)
\emph default
, the metric of kidney function estimated on a patient's age, gender, and
 serum creatinine level, could potentially give more information to the
 learning algorithm than all features separately.
 
\end_layout

\begin_layout Standard
In the following subsections, we will cover some popular feature engineering
 techniques.
\end_layout

\begin_layout Subsection

\series bold
Handling Categorical Features
\end_layout

\begin_layout Standard
The majority of machine learning algorithms primarily operate with numerical
 features 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
 To handle categorical features (the ones with only a few possible values),
 such as the age group or a blood group, we can use 
\emph on
one-hot encoding
\emph default
 to convert them to several binary ones 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 For instance, let us consider a blood group feature comprised of four primary
 blood groups: A, B, AB, and O.
 We can convert each blood group into a vector of four numerical values:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{align*}   
\end_layout

\begin_layout Plain Layout

A&=[1,0,0,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

B&=[0,1,0,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

AB&=[0,0,1,0] 
\backslash

\backslash
   
\end_layout

\begin_layout Plain Layout

O&=[0,0,0,1]
\end_layout

\begin_layout Plain Layout


\backslash
end{align*}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
This technique will increase the dimensionality of the dataset, but this
 is a trade-off we have to make because assigning a number to each group
 (1 to A, 2 to B, and so on) would imply gradation or ranking among these
 categories, while there is none 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
However, if the categorical feature does suggest some gradation, for example,
 university marks as ”fail”, ”average”, ”good”, or ”excellent”, an enumeration
 of each value will be appropriate.
 This practice of assigning a number to categories that have ranking is
 called 
\emph on
ordinal encoding
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9,key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\emph on
Binning
\emph default
 (or 
\emph on
bucketing
\emph default
) is the technique used for converting numerical values into multiple binary
 features called 
\emph on
bins
\emph default
 or 
\emph on
buckets
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 For example, a patient's age can be transformed into age-range bins: 0
 to 18, 18 to 25, 25 to 40, and so on.
 This technique might help the algorithm to learn better, particularly with
 smaller datasets 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection

\series bold
Feature Scaling
\end_layout

\begin_layout Standard
Different ranges of feature values might pose a problem to some machine
 learning algorithms
\series bold
 
\series default
as they do not handle them very well 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9,key-10"
literal "false"

\end_inset

.
 It might result in a slower training time or a poorer performance 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\emph on
Normalization
\emph default
 and 
\emph on
standardization
\emph default
 scaling techniques solve this problem.
\end_layout

\begin_layout Standard

\emph on
Normalization
\emph default
 (also known as 
\emph on
min-max scaling
\emph default
) is a technique of converting an actual range of numerical feature values
 into a standard range of values: 
\begin_inset Formula $[-1,1]$
\end_inset

 
\series bold

\begin_inset Note Note
status collapsed

\begin_layout Plain Layout

\series bold
pridat vzorec pro '-1
\end_layout

\end_inset


\series default
 or 
\begin_inset Formula $[0,1]$
\end_inset

 without losing any information 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 The 
\begin_inset Formula $[0,1]$
\end_inset

 normalization formula for value 
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature 
\begin_inset Formula $j$
\end_inset

 is expressed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{x}^{(j)}=\frac{x^{(j)}-min(j)}{max(j)-min(j)},
\]

\end_inset

where 
\begin_inset Formula $min(j)$
\end_inset

 and 
\begin_inset Formula $max(j)$
\end_inset

 are minimal and maximal values of feature 
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Standardization
\emph default
 is a scaling technique that scales numerical data so that after scaling,
 it has the 
\emph on
standard normal distribution 
\emph default
properties with the mean µ=0 (average value) and the standard deviation
 from the mean 
\begin_inset Formula $\sigma=1$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 The standardization formula for value 
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature 
\begin_inset Formula $j$
\end_inset

 is given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{x}^{(j)}=\frac{x^{(j)}-\mu^{(j)}}{\sigma^{(j)}}.
\]

\end_inset


\end_layout

\begin_layout Standard
Typically, standardization is used for supervised learning in cases where
 feature values are formed by a standard distribution (bell curve) or a
 feature has outliers.
 In other cases, normalization is preferred 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection

\series bold
Handling Missing Feature Values
\begin_inset CommandInset label
LatexCommand label
name "subsec:Handling-Missing-Feature"

\end_inset


\end_layout

\begin_layout Standard
Datasets frequently have missing values and, according to Andrii Burkov
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

, to handle them, we have one of the following options:
\end_layout

\begin_layout Enumerate

\series bold
Removal of rows with missing values
\series default
 is the most straightforward approach to managing missing data.
 If missing values are sparse or the dataset is large enough, this technique
 would be appropriate.
\end_layout

\begin_layout Enumerate

\series bold
Feature removal
\series default
.
 If a feature in the dataset has excessive missing values relative to its
 size, it is better to remove it.
\end_layout

\begin_layout Enumerate

\series bold
Regression imputation
\series default
.
 This technique implies filling in a missing feature value with regression
 algorithm predictions.
\end_layout

\begin_layout Enumerate

\series bold
Mean/median imputation
\series default
.
 This method involves filling missing feature values with their mean or
 median value.
\end_layout

\begin_layout Enumerate

\series bold
Constant value imputation
\series default
.
 This technique entails filling the missing values with clearly too-high
 or too-low values.
 Because of that, the algorithm will be able to discern the value as an
 outlier while considering other features.
 This method is not recommended as it can introduce bias.
\end_layout

\begin_layout Enumerate

\series bold
Adding a binary indicator for each feature with missing values
\series default
.
 If features with missing values are few and the dataset is large, we can
 add a binary indicator for each feature with missing values.
 The missing value can be replaced with 0 or any other number.
\end_layout

\begin_layout Standard
It is often impossible to tell which data imputation method would work best,
 so it should be checked experimentally.
\end_layout

\begin_layout Section
Model Training and Hyperparameter Tuning
\begin_inset CommandInset label
LatexCommand label
name "sec:Model-Training-and-Hyperparameter-tuning"

\end_inset


\end_layout

\begin_layout Standard
It is a common practice to divide a dataset into three parts:
\end_layout

\begin_layout Itemize
Training set (70% of the dataset)
\end_layout

\begin_layout Itemize
Validation set (15% of the dataset)
\end_layout

\begin_layout Itemize
Test set (15% of the dataset) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The training set, being the largest, is employed to train the machine learning
 model.
 Validation and test sets, which are identical in size and often called
 hold-out sets, are used in subsequent stages of model evaluation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The rationale behind using separate training and validation sets is to prevent
 
\emph on
overfitting
\emph default
 - when the model performs well on the training data but poorly on the unseen
 data 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 Overfitting can occur if the model is tested and evaluated on the same
 dataset.
 As a result, the model may memorize the training examples and fail to make
 accurate predictions on the unseen data.
 To alleviate this, we use the validation set to fine-tune the model and
 the test set to assess its performance before deploying it to production
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
A typical workflow involves training the model on the training set, validating
 it on the validation set using the selected metric, and then adjusting
 the model's parameters to improve its performance.
 This process is repeated until no substantial improvement is observed.
 Finally, the model's performance is assessed on the test set 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
 This iterative process is referred to as 
\emph on
hyperparameter tuning.
\end_layout

\begin_layout Standard
An alternative to the three-set technique is 
\emph on
k-fold cross-validation
\emph default
.
 This technique involves splitting the dataset into k subsets, or folds,
 of equal size.
 One fold is used as a validation set, while the other k-1 folds constitute
 a training set.
 The model is trained exactly k times, with each fold serving as a validation
 set only once.
 The only drawback is that it is highly computationally demanding, particularly
 with a high k value and larger datasets, as the model will be trained k
 times, but with a small dataset, it is a necessity 
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
A 
\emph on
hyperparameter
\emph default
 is a parameter specified before model training, unlike regular parameters
 calculated during training.
 Each model possesses a different set of hyperparameters, and they profoundly
 influence the model's performance.
 The number of trees in Random Forest (described in Subseection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Random-Survival-Forest"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and the C hyperparameter in Support Vector Machines (described in Subsection
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Support-Vector-Machines"
plural "false"
caps "false"
noprefix "false"

\end_inset

) are examples of hyperparameters.
 The task of finding the optimal combination of hyperparameters is called
 
\emph on
hyperparameter tuning
\emph default
.
 One strategy might be to select hyperparameters manually and observe their
 impact on performance.
 However, utilizing the grid search is a better way 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Grid search
\emph default
 is a standard way of performing hyperparameter fine-tuning.
 It includes defining hyperparameters to experiment with, providing values
 for each to be tested, and training a model for each possible combination
 of hyperparameters.
 The performance of each model is assessed using k-fold cross-validation,
 and the best combination of hyperparameters is selected.
 This approach is used in scikit-learn's implementation - GridSearchCV 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Grid search proves effective when dealing with relatively few hyperparameter
 combinations.
 However, with a larger number of hyperparameter combinations, it is advisable
 to use RandomizedSearch (RandomizedSearchCV in scikit-learn 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

).
 This method is very similar to grid search, but instead of trying every
 possible combination of provided values, it tests only a specified number
 of randomly selected hyperparameter combinations.
 The primary advantage of this method over grid search lies in more control
 over computational power and the time dedicated to hyperparameter tuning
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Section
Survival Analysis 
\begin_inset CommandInset label
LatexCommand label
name "sec:Survival-Analysis"

\end_inset


\end_layout

\begin_layout Standard

\emph on
Survival analysis (SA), 
\emph default
often referred to as
\emph on
 time-to-event analysis, 
\emph default
is a set of statistical techniques employed to analyze and predict the time
 until an event of interest occurs.
 Its name originates from clinical and biological research, where these
 methods are used to analyze survival time.
 These methods, however, found their use in areas far beyond clinical settings:
 in business to predict the time until the customer ”churns” from a subscription
; in engineering to estimate the product longevity or the longevity of its
 parts; in social sciences to estimate the longevity of a marriage; or to
 estimate a student dropout rate in an academic setting.
\end_layout

\begin_layout Standard
In the context of survival analysis, 
\emph on
time
\emph default
 (also 
\emph on
survival time
\emph default
) refers to the duration from the start of an individual's follow-up to
 the occurrence of an event.
 It is usually measured in days, weeks, months, or years 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
 The term 
\emph on
event
\emph default
 (also referred to as 
\emph on
deat
\emph default
h or 
\emph on
failure
\emph default
) encompasses any occurrence that permanently changes the subject's state.
 It can be death, the onset of a disease, a relapse from remission, a recovery,
 or any other specified experience of interest that an individual might
 encounter 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA,key-30"
literal "false"

\end_inset

.
 Usually, only one event of interest is considered.
 When evaluating multiple events, the problem is categorized as 
\emph on
competing risks
\emph default
 or 
\emph on
recurrent events
\emph default
 problem 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
This section will explain the fundamental survival analysis terminology,
 such as censoring and survival function.
 We will explore the taxonomy of survival analysis methods, highlighting
 the most popular statistical methods and one machine learning method tailored
 to the needs of survival analysis.
 Additionally, we will discuss ways to evaluate survival models effectively.
 Unless referenced otherwise, the contents of this section are drawn from
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Essential Concepts
\end_layout

\begin_layout Standard
In this subsection, we will cover the essential terminology required for
 survival analysis such as censoring, censoring assumptions, survival, and
 hazard functions.
 
\end_layout

\begin_layout Subsubsection*
Censoring
\end_layout

\begin_layout Standard
The most distinct feature of survival analysis methods is the ability to
 handle censored data.
 
\emph on
Censoring
\emph default
 refers to a circumstance when the information about survival time is only
 partially known.
 For example, the dataset utilized in our research has 370,000 censored
 instances out of 500,000 performed transplantations.
 These patients were either alive at the last observation date or lost to
 follow-up.
 This lack of complete information indicates 
\emph on
censoring
\emph default
 in survival analysis.
 Censoring makes the application of standard statistical and machine-learning
 approaches to survival data impractical 
\begin_inset CommandInset citation
LatexCommand cite
key "key-27-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/censoring_ill.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
The Illustration of Censoring 
\begin_inset CommandInset label
LatexCommand label
name "fig:Censoring-illustration"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Look at the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Censoring-illustration"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 On the y-axis, we can see individual patients, while the x-axis corresponds
 to the study timeline (the right side is the end of the study).
 Cross (X) denotes an occurrence of the event, and circle (O) corresponds
 to the patient's exit from the study.
 No sign and the line ceasing at the end of the study means the patient
 did not experience the event.
\end_layout

\begin_layout Standard
There are three types of censoring: 
\emph on
left
\emph default
, 
\emph on
right
\emph default
, and 
\emph on
interval
\emph default
 censoring.
 
\emph on
Right censoring
\emph default
, which is more common, occurs when we are sure that the event did not happen
 by a specific time and we do not know when it will happen.
 The situation arises when the patient drops out of a study, or the study
 ends when they are still alive, as illustrated with patients 2, 3, and
 4 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Censoring-illustration"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard

\emph on
Left censoring
\emph default
 is less common and happens when the event occurs before the study begins
 or before the initial observation.
 We know the event happened before a specific time, but the exact time is
 unknown.
 This type is typical in cases where a patient has already experienced the
 event (e.g., developed a disease) before enrolling in the study.
\end_layout

\begin_layout Standard

\emph on
Interval censoring
\emph default
 happens when the event occurs within a particular timeframe, but the exact
 time is unknown.
 This can be the case in studies involving periodic patient follow-up, where
 the event can happen at any point between two visits.
\end_layout

\begin_layout Standard
Understanding right, left, and interval censoring is essential in survival
 analysis.
 We will next turn our attention to the assumptions associated with these
 censoring types.
 These assumptions are inherent to many survival analysis methods and are
 critical in selecting an appropriate technique.
\end_layout

\begin_layout Subsubsection*
Censoring Assumptions
\end_layout

\begin_layout Standard
There are three types of censoring assumptions: 
\emph on
random
\emph default
, 
\emph on
independent
\emph default
, and 
\emph on
non-informative
\emph default
.
 Each shares certain similarities but also possesses unique distinctions,
 which we will explore in detail.
 Censoring assumptions are how censoring is managed.
\end_layout

\begin_layout Enumerate

\series bold
Random Censoring
\series default
: Subjects censored at time 
\emph on
t
\emph default
 are assumed to have the same failure rate as the remaining subjects, provided
 they have the same survival experience.
 The censored subjects are selected randomly, meaning the study does not
 influence or bias which participants are censored.
\end_layout

\begin_layout Enumerate

\series bold
Independent Censoring
\series default
: Independent censoring occurs when the censoring is random within certain
 subgroups defined by specific covariates.
 If no covariates are present, it defaults to random censoring.
 This distinction might not be apparent when examining a single subgroup.
\end_layout

\begin_layout Enumerate

\series bold
Non-Informative Censoring
\series default
: Non-informative censoring occurs when censored instances do not provide
 any information on the patient's survival prospects.
 In other words, whether or not the patient is censored, has no influence
 on experiencing the event.
\end_layout

\begin_layout Standard
Generally, it is safe to assume 
\emph on
non-informative
\emph default
 censoring when censoring is 
\emph on
independent
\emph default
 and/or 
\emph on
random
\emph default
.
 However, these assumptions are not equivalent.
 Let us examine some examples to better understand these concepts.
\end_layout

\begin_layout Standard
Consider a three-year disease occurrence study with 100 subjects at risk
 (group A).
 By the end of the study, 20 of them contracted the disease, giving a 20%
 three-year disease risk.
 Suppose we want to extend the study for another two years on the remaining
 80 individuals.
 However, 40 refused to continue in the study and were, therefore, lost
 to follow-up (censored).
 Of the remaining 40, five contracted the disease.
 Assuming those who left were representative of the remaining subjects (random
 and independent censoring), another five among the censored would have
 contracted it.
 Consequently, the five-year risk is 30%, and the five-year survival is
 70% under random and independent censoring assumptions.
 In this case, random and independent censoring is the same, as no predictor
 variables are considered.
\end_layout

\begin_layout Standard
To illustrate the difference between random and independent censoring, let
 us introduce another group to the study: group B with 100 individuals.
 In the first three years, 40 contracted the disease, and 10 left the study.
 So, the calculated three-year risk for group B is 40%.
 In the next two years, 10 out of 50 get the disease, yielding 20% risk
 for years between 3 and 5.
 Under the independent censoring assumption, we assume that out of 10 censored,
 2 contracted the disease.
 The five-year risk for group B is 52%, with 48% survival under independent
 censoring assumptions.
\end_layout

\begin_layout Standard
As we can see, the five-year risk in the two groups differs significantly
 (30% against 52%), and the censoring proportion is also very different
 (50% against 17%).
 Hence, the overall censoring is not random.
 However, it is random within each group, so the censoring is independent.
 Conversely, if in group B, 30 subjects out of 60 were censored at the three-yea
r mark, the censoring proportion would be the same in both groups, and the
 overall censoring would be random as those censored would be the representative
s of those who remained at risk.
\end_layout

\begin_layout Standard
To best illustrate non-informative censoring, let us demonstrate informative
 censoring.
 Let us take a group of subjects under random and independent censoring
 assumptions.
 Every time subject A gets an event, subject B leaves the study (e.g., B is
 A's relative).
 If the censored subjects are representative of subjects at risk, it would
 be random and independent censoring.
 Here, the censoring mechanism is directly related to the event occurrence,
 so the censoring is informative.
 
\end_layout

\begin_layout Subsubsection*
Survival Function
\end_layout

\begin_layout Standard
The 
\emph on
survival function
\emph default
, also known as the 
\emph on
survivor function
\emph default
, denoted by 
\begin_inset Formula $S(t)$
\end_inset

, represents the probability that the patient survives, in other words,
 does not experience the event of interest beyond a given time 
\begin_inset Formula $t$
\end_inset

.
 Mathematically, it is denoted by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S(t)=P(T>t),\label{eq:f_surv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $P$
\end_inset

 represents the probability, 
\begin_inset Formula $t$
\end_inset

 is any specific time of interest, and 
\begin_inset Formula $T$
\end_inset

 is the random variable for the subject's survival time.
 For instance, if we want to know the likelihood that a patient will live
 for more than five years after a kidney transplant, we set 
\begin_inset Formula $t$
\end_inset

 equal to 5 and evaluate 
\begin_inset Formula $S(t)$
\end_inset

 to determine the probability that 
\begin_inset Formula $T$
\end_inset

, actual survival time, is greater than 5 years.
 
\end_layout

\begin_layout Standard
The survival function has several key characteristics:
\end_layout

\begin_layout Enumerate
It continually decreases or maintains its value over time, theoretically
 extending from 0 to infinity.
 If the study lasted indefinitely, the survival function would eventually
 fall to 0.
 In practical research scenarios, studies do not last forever, and not every
 patient experiences an event by the end of the study.
\end_layout

\begin_layout Enumerate
As it represents a probability, the function value ranges from 0 to 1.
 At the beginning of the observation period, it starts at 1, indicating
 100% survival probability, and declines over time, potentially reaching
 0 as the probability of survival decreases.
\end_layout

\begin_layout Enumerate
Although the survival function graph is smooth by definition, in reality,
 it is a step function.
 This stepwise representation is due to the nature of real-world data, where
 events are recorded at specific, discrete time points rather than continuously
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/survival_f.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Survival Function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Hazard Function
\end_layout

\begin_layout Standard
The survival function provides the probability of an individual surviving
 at each given point in time.
 This function is often preferred over the 
\emph on
hazard function
\emph default
 due to its more intuitive appeal: it directly communicates the chance of
 survival.
 However, there are scenarios where knowing the risk at each point in time
 is necessary, entailing the use of the hazard function.
 
\end_layout

\begin_layout Standard
The 
\emph on
hazard function
\emph default
, denoted as 
\begin_inset Formula $h(t)$
\end_inset

, represents the instantaneous potential per unit of time for the event
 to occur, provided the subject's survival up to time 
\begin_inset Formula $t$
\end_inset

.
 It is expressed as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t)=\underset{\Delta t\rightarrow0}{lim}\frac{P(t\leq T<t+\Delta t|T\geq t)}{\Delta t}.\label{eq:f_hazard}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here, 
\begin_inset Formula $\Delta t$
\end_inset

 represents an infinitesimally small increment of time.
 The function 
\begin_inset Formula $h(t)$
\end_inset

 is equal to the limit, as 
\begin_inset Formula $\Delta t$
\end_inset

 approaches zero, of a conditional probability, divided by 
\begin_inset Formula $\Delta t$
\end_inset

.
 The conditional probability statement gives the probability that a person’s
 survival time 
\begin_inset Formula $T$
\end_inset

 will lie in the interval between 
\begin_inset Formula $t$
\end_inset

 and 
\begin_inset Formula $t+\Delta t$
\end_inset

, given that the survival time is greater than or equal to 
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
Occasionally, due to its structure, the hazard function is referred to as
 a 
\emph on
conditional failure rate
\emph default
.
 It is a rate because it represents a conditional probability per unit of
 time 
\begin_inset Formula $\Delta t$
\end_inset

, and it is conditional on the subject surviving until time 
\begin_inset Formula $t$
\end_inset

.
 Unlike probability, this rate has a scale from 0 to infinity — depending
 on the measure of time in days, weeks, or years.
 By considering the limit as the 
\begin_inset Formula $\Delta t$
\end_inset

 approaches zero, we get the instantaneous potential of failing at time
 
\begin_inset Formula $t$
\end_inset

, given survival until that moment.
\end_layout

\begin_layout Standard

\emph on
The cumulative hazard function 
\emph default
further extends our understanding by quantifying the accumulated risk over
 time.
 It is the area under the hazard function that allows us to say which group
 has a greater risk.
 It is defined by the following function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
H(t)\overset{def}{=}\int_{0}^{t}h(u)du,\text{ where }t>0.\label{eq:cumulative-hazard-function}
\end{equation}

\end_inset

Where 
\begin_inset Formula $h(u)$
\end_inset

 represents a hazard function.
 This integral measure enables a comprehensive comparison of risk between
 groups, showing which has a greater risk from the perspective of accumulated
 potential for the event to occur over time.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/hazard_f.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of Different Hazard Function Types
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*

\series bold
The Relationship Between the Survival and Hazard Functions
\end_layout

\begin_layout Standard
Some models, such as Cox Proportional Hazards, are written in terms of the
 hazard function, and it is necessary to be able to estimate the survival
 function out of the hazard function to make the model more flexible and
 accessible to a larger audience.
 Fortunately, there are ways to convert one into the other.
 In this subsection, we are going to describe techniques for these conversions.
 
\end_layout

\begin_layout Standard
Let us start by considering how to obtain the survival function 
\begin_inset Formula $S(t)$
\end_inset

 from the hazard function 
\begin_inset Formula $h(t)$
\end_inset

.
 The relationship is captured in the following equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
S(t)=exp\left[-\intop_{0}^{t}h(u)du\right].\label{eq:haz_to_surv}
\end{equation}

\end_inset

Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:haz_to_surv"
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrates that the survival function 
\begin_inset Formula $S(t)$
\end_inset

 is equal to the exponential of the negative cumulative hazard function
 from zero to 
\begin_inset Formula $t$
\end_inset

.
 As we can see, the integral in the equation is the 
\begin_inset Formula $H(t)$
\end_inset

 from the 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cumulative-hazard-function"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 We can simplify our equation to:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
S(t)=e^{-H(t)}
\]

\end_inset


\end_layout

\begin_layout Standard
On the other hand, to obtain the hazard function from the survival function,
 we can utilize the following relationship: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t)=-\frac{dS(t)/dt}{S(t)}.\label{eq:surv_to_haz}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Here, Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:surv_to_haz"
plural "false"
caps "false"
noprefix "false"

\end_inset

 denotes that the hazard function 
\begin_inset Formula $h(t)$
\end_inset

 is the negative derivative of the survival function 
\begin_inset Formula $S(t)$
\end_inset

 with respect to time 
\begin_inset Formula $t$
\end_inset

 divided by 
\begin_inset Formula $S(t)$
\end_inset

.
\end_layout

\begin_layout Standard
Considering the fact that the survival function describes the probability
 of a patient surviving up to time 
\begin_inset Formula $t$
\end_inset

, and the hazard function shows the instantaneous risk of a person dying
 at a specific instant 
\begin_inset Formula $t$
\end_inset

, we can say that they provide complementary information about survival
 and risk over time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
 Of the two discussed functions, the survival function is used more often
 as it is more intuitive.
 In the practical part, we will estimate the survival function as well.
 Fortunately, we will not convert them manually.
 The 
\emph on
scikit-survival
\emph default
 library will do that for us.
 
\end_layout

\begin_layout Subsection
Taxonomy of Survival Analysis Methods
\end_layout

\begin_layout Standard
Survival analysis methods can be broadly categorized into 
\emph on
statistical
\emph default
 and 
\emph on
machine learning-based
\emph default
 methods 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
 Both aim to estimate the survival time and the survival probability at
 that time 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
 Statistical methods primarily characterize the distributions of the event
 times and the statistical properties of the parameter estimation by estimating
 the survival curves.
 Typically, statistical methods are employed for low-dimensional data 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
On the other hand, machine learning methods focus on predicting the occurrence
 of events at a given time.
 They harness the strengths of traditional survival analysis while integrating
 different machine learning techniques, leading to more potent algorithms.
 Machine learning methods are mostly used with high-dimensional data 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Statistical methods can be further subdivided based on their assumptions
 and parameter usage into parametric, non-parametric, and semi-parametric
 methods.
 Machine learning methods, encompassing methods like survival trees, ensembles
 (random survival forests), neural networks, and support vector machines,
 form a distinct category.
 Advanced machine learning techniques, such as active learning, transfer
 learning, and multitask learning, are also included 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In the following sections, we will cover selected statistical methods and
 one machine learning technique.
\end_layout

\begin_layout Subsection
Statistical Methods
\end_layout

\begin_layout Standard
This section provides a concise overview of statistical techniques.
 Here, we introduce three different types of statistical methods that are
 commonly used to estimate the survival and hazard functions: 
\emph on
non-parametric
\emph default
, 
\emph on
semi-parametric
\emph default
, and 
\emph on
parametric methods
\emph default
.
\end_layout

\begin_layout Standard

\emph on
Non-parametric methods
\emph default
 are preferred in situations where the event time does not adhere to any
 known distribution or when the proportional hazards assumption is not met
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
 There are three main non-parametric methods: the Kaplan-Meier (KM) method,
 the Nelson-Aalen (NA) estimator, and the Life-Table (LT) method.
 In the next section, we will cover Kaplan-Meier in more detail.
 The Life-Table method is more convenient than Kaplan-Meier for the estimation
 of survival curves when data subjects are segmented into distinct time
 intervals when dealing with an extensive number of subjects or a broad
 population scope.
 On the other hand, the Nelson-Aalen method is used to estimate hazard functions
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Semi-parametric models
\emph default
 offer a middle ground between fully parametric models, which make specific
 distributional assumptions, and non-parametric models, which make very
 few assumptions.
 The Cox model is the most frequently employed model for survival regression
 analysis among semi-parametric methods.
 It is semi-parametric, as the distribution of the outcome is unknown.
 Unlike other approaches, this method is based on the proportional hazards
 assumption and uses partial likelihood for parameter estimation (more on
 that in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Cox-Proportional-Hazards"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 There are a couple of variants of the basic Cox model: the penalized Cox
 model, which will be used in the practical part of this work, the CoxBoost
 algorithm, the Time-Dependent Cox model and many more 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Parametric methods
\emph default
 shine in their accuracy and efficiency when the time to the event conforms
 to a known distribution that can be specified in terms of certain parameters.
 With parametric models, estimating the time to the event is straightforward,
 whereas the Cox model can make this task somewhat cumbersome or unfeasible.
 In the domain of parametric models, linear regression is central.
 However, the Tobit model, Buckley-James regression, and penalized regression
 are the most favored.
 Beyond this, other parametric models, like the Accelerated Failure Time
 (AFT), have gained traction.
 The AFT model represents survival time as a function of covariates 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
To conclude, statistical methods in survival analysis can be broadly categorized
 into non-parametric, semi-parametric, and parametric techniques, each with
 its unique strengths and applications.
 Among non-parametric methods, Kaplan-Meier stands out as particularly significa
nt.
 It provides a robust and intuitive way to estimate survival functions.
 
\end_layout

\begin_layout Subsubsection

\series bold
Kaplan-Meier Survival Curves
\end_layout

\begin_layout Standard

\emph on
Kaplan-Meier
\emph default
 is a non-parametric method of creating survival functions..
 It is 
\emph on
non-parametric
\emph default
 because it does not consider any covariates or parameters and requires
 only the survival time and the censoring indicator.
 It works under an independent censoring assumption.
 The general Kaplan-Meier formula for plotting the survival function is
 the following: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{S}(t_{(f)})=\hat{S}(t_{(f-1)})\times\hat{P}(T>t_{(f)}|T\geq f_{(f)}).\label{eq:KM}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
That can be read as the survival function 
\begin_inset Formula $\hat{S}$
\end_inset

 of time 
\begin_inset Formula $t_{(f)}$
\end_inset

 is equal to the probability of surviving past the previous time point 
\begin_inset Formula $t_{(f-1)}$
\end_inset

 times the conditional probability of surviving past the time 
\begin_inset Formula $t_{(f)}$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
 This function can also be expressed as a product limit:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{S}(t_{(f)})=\prod_{i=1}^{f-1}\hat{P}(T>t_{(i)}|T\geq f_{(i)}).
\]

\end_inset


\end_layout

\begin_layout Standard
In the Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:KM"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we replaced 
\begin_inset Formula $\hat{S}(t_{(f-1)})$
\end_inset

 with the product of all fractions estimating the conditional probabilities
 for failure times 
\begin_inset Formula $t_{f-1}$
\end_inset

 and those preceding it 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
 Because of that, the Kaplan-Meier estimator is sometimes referred to as
 the 
\emph on
product-limit method
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Survival-curve-estimated"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the survival curve created with the KM method on the dataset used
 in this work.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/example-km.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Survival Curve Estimated with KM on the training dataset 
\begin_inset CommandInset label
LatexCommand label
name "fig:Survival-curve-estimated"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
These survival curves are often compared using the log-rank test.
 The 
\emph on
log-rank test
\emph default
 is a way to compare two survival functions.
 It is often used in studies, with a target group and a placebo (control)
 group to assess the efficacy of the treatment or intervention in the study
 by comparing the survival curves of the two groups 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
is discussed in the following section.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To summarize, the Kaplan-Meier method provides a robust nonparametric approach
 for estimating survival functions, emphasizing its independence from covariates.
 While the log-rank test offers a mechanism to compare the survival curves
 and assess treatment efficacy, some situations still require considering
 covariates.
 That leads us to the Cox proportional hazards model, which inherently handles
 covariates.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
rewrite
\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Log-rank Test
\begin_inset CommandInset label
LatexCommand label
name "subsec:Log-rank-test"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
- it is essentially a 
\begin_inset Formula $\chi^{2}$
\end_inset

test (already seen in DTs).
 calculates 
\begin_inset Formula $\chi^{2}$
\end_inset

for each event time for each group and sums the results
\end_layout

\begin_layout Plain Layout
LR - comparison between observed and expected 
\begin_inset Formula $\frac{(\text{observed}-\text{expected})^{2}}{\text{expected}}$
\end_inset


\end_layout

\begin_layout Plain Layout
expected num of events assumes that the Null hypothesis is true (groups
 are identical)
\end_layout

\begin_layout Plain Layout
- LRT compares the distribution of the time until an event occurs of two
 or more independent samples.
\end_layout

\begin_layout Plain Layout
the null hypothesis: groups have identical distribution curves.
\end_layout

\begin_layout Plain Layout
alternative hypothesis: the groups have different distribution curves.
\end_layout

\begin_layout Plain Layout
as with all statistical hypothesis test we get a p value at the end of the
 LRT.
 (is it greater than the signifficance level of 0,05? if yes then null hypothesi
s is retained and both groups have identical distribution curves.
 if no the null hypothesis is rejected)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Log-rank Test is essentially a 
\begin_inset Formula $\chi^{2}$
\end_inset

test for two KM survival curves that can evaluate whether or not survival
 (KM) curves for two or more groups are statistically equivalent (
\begin_inset Formula $\chi^{2}$
\end_inset

 test was already touched upon in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Decision-Trees"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 Two curves are statistically equivalent if the produced p-value is lower
 than the level of significance (
\begin_inset Formula $0.05$
\end_inset

 or 
\begin_inset Formula $0.01$
\end_inset

).
 It is formed by the sum of the observed minus expected over all failure
 times for one of the two groups 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The Null hypothesis 
\begin_inset Formula $H_{0}$
\end_inset

is being tested in the log-rank test, which states that groups have identical
 distribution curves.
 If the p-value from the 
\begin_inset Formula $\chi^{2}$
\end_inset

 distribution table associated with the produced log-rank statistic is lower
 than the level of significance, then the Null hypothesis is rejected, and
 the curves are deemed statistically inequivalent.
 Under the Null hypothesis, the log-rank statistic is approximately 
\begin_inset Formula $\chi^{2}$
\end_inset

 with 
\begin_inset Formula $n-1$
\end_inset

 degrees of freedom for 
\begin_inset Formula $n$
\end_inset

 groups 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The log-rank statistic for two groups, 
\begin_inset Formula $i=1,2$
\end_inset

, is denoted as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\text{Log-rank statistic}=\frac{(O_{i}-E_{i})^{2}}{Var(O_{i}-E_{i})},
\]

\end_inset

where 
\begin_inset Formula $Var()$
\end_inset

 is variance.
 The formula for more than two groups is much more complex and involves
 the calculation of covariates.
 Fortunately, the log-rank can be approximated without the need for the
 calculation of variance and covariance with the following formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\chi^{2}\approx\sum_{i}^{\#\text{of groups}}\left(\frac{O_{i}-E_{i}}{E_{i}}\right)
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $i$
\end_inset

 is the group, 
\begin_inset Formula $O_{i}$
\end_inset

 are summed observed, and 
\begin_inset Formula $E_{i}$
\end_inset

 are summed expected 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

..
 
\end_layout

\begin_layout Subsubsection
Nelson-Aalen
\end_layout

\begin_layout Subsubsection
Cox Proportional Hazards Method 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Cox-Proportional-Hazards"

\end_inset


\end_layout

\begin_layout Standard
The 
\emph on
Cox proportional hazards
\emph default
 (also Cox PH) model is a widely used in survival analysis semi-parametric
 model.
 This section explores its formulation, key properties, and reasons for
 its widespread use in research.
 
\end_layout

\begin_layout Standard
The Cox proportional hazards model is defined in terms of a hazard at time
 
\begin_inset Formula $t$
\end_inset

 for a subject with a given vector of explanatory variables 
\begin_inset Formula $\mathbf{X}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
h(t,\mathbf{X})=h_{0}(t)\exp\left[\sum_{i=1}^{p}\beta_{i}X_{i}\right],\label{eq:cox_ph}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\series bold

\begin_inset Formula $h_{o}(t)$
\end_inset


\series default
 stands for the 
\emph on
baseline hazard function
\emph default
.
 Coefficients 
\begin_inset Formula $\beta$
\end_inset

 are the parameters of interest in the model.
 Since the exponential function has no 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is called 
\emph on
time-independent.

\emph default
 The exponential function ensures that the function is non-negative, satisfying
 the definition of the hazard function
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Even though Equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:cox_ph"
plural "false"
caps "false"
noprefix "false"

\end_inset

 contains the baseline hazard function, the function is not specified.
 Fortunately, we can calculate the hazard ratio, a measure of effect, without
 having to estimate the baseline hazard function.
 Similarly, the hazard function 
\begin_inset Formula $h(t,\mathbf{X})$
\end_inset

 and the survival function 
\begin_inset Formula $S(t,\mathbf{X})$
\end_inset

 can be estimated without the baseline function.
 So, with minimal assumptions, we can estimate everything we need (h, S,
 and HR).
\end_layout

\begin_layout Standard
The 
\emph on
hazard ratio
\emph default
 (HR) is a measure of the influence of an intervention on the outcome.
 A hazard ratio is defined as the hazard for one individual divided by the
 hazard for the other, as illustrated with the following formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{HR}=\frac{h(t,X^{*}\mathbf{)}}{h(t,X)}=\exp\left[\sum_{i=1}^{p}\hat{\beta_{i}}(X^{*}-X)\right]\label{eq:cox_ph_hazard_ratio}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
As can be seen, the equation does not contain 
\begin_inset Formula $t$
\end_inset

 and the basic hazard function, as they are canceled out, making it a 
\emph on
proportional hazard assumption
\emph default
.
\end_layout

\begin_layout Standard
Like logistic regression, the CoxPH uses the 
\emph on
maximum likelihood 
\emph default
function 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:max_likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

 to calculate its parameters (
\begin_inset Formula $\hat{\beta_{i}}$
\end_inset

).
 However, since the maximal likelihood considers only a part of patients,
 namely those who experienced an event, the formula is called 
\series bold
\emph on
partial
\series default
\emph default
 
\emph on
likelihood
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-13-SA"
literal "false"

\end_inset


\emph on
.

\emph default
 
\end_layout

\begin_layout Standard
Let us define the partial likelihood.
 The subject's survival can be defined by 
\begin_inset Formula $h(t,X)dt$
\end_inset

 with 
\begin_inset Formula $dt→0$
\end_inset

.
 Consider 
\begin_inset Formula $J$
\end_inset

 (where 
\begin_inset Formula $J\leq N$
\end_inset

) as the total number of events of interest observed for 
\begin_inset Formula $N$
\end_inset

 instances.
 
\begin_inset Formula $T_{1}<T_{2}<\cdots<T_{J}$
\end_inset

 represents the unique and sequentially ordered times to the event of interest.
 Let 
\begin_inset Formula $X_{j}$
\end_inset

 be the vector of covariates for a subject who experiences the event at
 time 
\begin_inset Formula $T_{j}$
\end_inset

.
 
\begin_inset Formula $R_{j}$
\end_inset

 is the set of risk subjects at 
\begin_inset Formula $T_{j}$
\end_inset

.
 Given that the event happens at time 
\begin_inset Formula $T_{j}$
\end_inset

, the individual probability associated with 
\begin_inset Formula $X_{j}$
\end_inset

 can be described as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{h(T_{j},X_{j})dt}{\sum_{i\epsilon R_{j}}h(T_{j},X_{i})dt}.
\]

\end_inset


\end_layout

\begin_layout Standard
By taking the product across all subjects' probabilities, we get partial
 likelihood.
 Based on Cox's assumption and the presence of censoring, partial likelihood
 is defined as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
PL(\beta)=\prod_{j=1}^{N}\left[\frac{exp(X_{j}\beta)}{\sum_{i\epsilon R_{j}}exp(X_{i}\beta)}\right]^{\delta_{J}}.\label{eq:partial-likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\delta_{j}$
\end_inset

 is the indicator of censoring of a given instance (1 for event occurrence,
 0 for censoring).
 Therefore, if the subject is censored, 
\begin_inset Formula $\delta=0$
\end_inset

, the individual probability is equal to 1, and the subject does not affect
 the result.
 The vector of coefficients 
\begin_inset Formula $\hat{\beta}$
\end_inset

 is estimated by either maximizing partial likelihood, defined above, or
 maximizing the negative 
\emph on
log-partial likelihood
\emph default
 to improve the efficiency:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
LL(\beta)=-\sum_{j=1}^{N}\delta_{j}\{X_{j}\beta-log[\sum_{i\epsilon R_{j}}exp(X_{i}\beta)]\}\label{eq:neg-partial-log-likelihood}
\end{equation}

\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-23-wang"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Penalized Cox Models
\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

, the Cox proportional hazards model is often chosen since its coefficients
 can be interpreted in terms of hazard ratios, offering meaningful insights.
 However, when estimating coefficients for many features, the standard Cox
 model collapses due to matrix inversion getting disrupted by correlations
 between features.
 Feature (column) correlations make a matrix singular, i.e., impossible to
 revert.
 In this section, we aim to explore the 
\emph on
Ridge regression
\emph default
, 
\emph on
LASSO
\emph default
, and 
\emph on
Elastic Net
\emph default
 methods as extensions or modifications to the Cox model.
 These techniques address the inherent challenges in the standard Cox model,
 offering solutions for regularization and feature selection.
\end_layout

\begin_layout Paragraph
Ridge
\end_layout

\begin_layout Standard
As 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

 further explains, we can avoid the problem of the inability to revert singular
 matrix by incorporating an 
\begin_inset Formula $l_{2}$
\end_inset

 penalty term on the coefficients, shrinking them to zero.
 Consequently, our objective has the following form: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\frac{\alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2}.
\]

\end_inset


\end_layout

\begin_layout Standard
In the equation, 
\begin_inset Formula $PL(\beta)$
\end_inset

 denotes the partial likelihood of the Cox model (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:partial-likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

), terms 
\begin_inset Formula $\beta_{1},\ldots,\beta_{p}$
\end_inset

represent the coefficients corresponding to 
\begin_inset Formula $p$
\end_inset

 features, 
\begin_inset Formula $\alpha\geq0$
\end_inset

 is a hyper-parameter that controls the amount shrinkage.
 The resulting objective is referred to as 
\emph on
ridge regression.

\emph default
 If 
\begin_inset Formula $\alpha$
\end_inset

 is set to zero, we get the regular Cox model 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
LASSO
\end_layout

\begin_layout Standard
While the 
\begin_inset Formula $l_{2}$
\end_inset

 ridge penalty solves the mathematical problem of fitting the Cox model,
 we would still need to take into account all features, no matter how many
 there are.
 Preferably, we would like to select a small subset of the most predictive
 features and ignore the rest, as too many features might result in overfitting.
 
\emph on
LASSO
\emph default
 (Least Absolute Shrinkage and Selection Operator) does exactly that.
 Rather than merely shrinking the coefficients to zero, it performs feature
 selection as a part of the optimization process, where a subset of coefficients
 is set to zero and is, therefore, excluded, reducing the number of features
 we need for prediction.
 Mathematically, we would replace the 
\begin_inset Formula $l_{2}$
\end_inset

 penalty with the 
\begin_inset Formula $l_{1}$
\end_inset

 penalty, leading to the following optimization problem: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\alpha\sum_{j=1}^{p}|\beta_{j}|.
\]

\end_inset


\end_layout

\begin_layout Standard
The main drawback is that we cannot directly control the number of features
 selected.
 However, the value of 
\begin_inset Formula $\alpha$
\end_inset

 essentially determines the number of selected features.
 To achieve a refined model that requires fewer features, we need a data-driven
 way to determine the appropriate 
\begin_inset Formula $\alpha$
\end_inset

.
 This can be accomplished by first determining the 
\begin_inset Formula $\alpha$
\end_inset

 that would ignore all features (coefficients are set to zero) and then
 gradually decreasing its value, possibly down to 1% of its starting value.

\emph on
 
\emph default
We would need to set 
\emph on
l1_ratio=1.0
\emph default
 and 
\emph on
alpha_min_ratio=0.01
\emph default
 to search for 100 
\begin_inset Formula $\alpha$
\end_inset

 values up to 1% of the estimated maximum.
 Fortunately, it is implemented in scikit-survival's 
\emph on
sksurv.linear_model.CoxnetSurvivalAnalysis
\emph default
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Elastic Net
\end_layout

\begin_layout Standard
The LASSO method is effective for selecting a subset of discriminative features.
 However, it has its shortcomings.
 The first is that LASSO cannot select more features than there are instances
 in the training data set.
 The second is its tendency to randomly select only one feature out of a
 set of highly correlated ones.
 The 
\emph on
Elastic Net
\emph default
 alleviates these issues by incorporating the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalties in a weighted manner, as is shown in the following optimization
 problem:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
arg\max_{\beta}\log PL(\beta)-\alpha\left(r\sum_{j=1}^{p}|\beta_{j}|+\frac{1-r}{2}\sum_{j=1}^{p}\beta_{j}^{2}\right),
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $r\epsilon[0;1]$
\end_inset

 is the relative weight of the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalty (
\begin_inset Formula $r=1$
\end_inset

 is a LASSO penalty, 
\begin_inset Formula $r=0$
\end_inset

 is a ridge penalty).
 The Elastic Net penalty combines the LASSO's feature selection capability
 and Ridge's regularization power.
 As a result, it is more stable than LASSO, and in a situation of highly
 correlated features, it would select them all, while LASSO would randomly
 select only one.
 Usually, it is sufficient to give the 
\begin_inset Formula $l_{2}$
\end_inset

 penalty only a small weight to improve the stability of the LASSO; for
 example, 
\begin_inset Formula $r=0.9$
\end_inset

 might suffice.
\end_layout

\begin_layout Standard
Similar to LASSO, the weight 
\begin_inset Formula $\alpha$
\end_inset

 inherently dictates the size of the chosen subset.
 Its optimal value is typically estimated in a data-driven way 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

.
 More on that in Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Coxnet"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where we will choose the best 
\begin_inset Formula $\alpha$
\end_inset

 for our data set.
\end_layout

\begin_layout Standard
To conclude, Ridge, LASSO, and Elastic Net are potent extensions of the
 Cox PH model, especially when dealing with high-dimensional datasets and
 highly correlated features.
 They help to regularize the Cox model and select only the most significant
 features, avoiding overfitting and assuring a more stable and interpretable
 model.
 Nevertheless, it is important to acknowledge their limitations.
 As linear models, they capture only linear relationships, and non-linear
 relationships remain uncaptured.
 That leads us to the next section dedicated to the Random Survival Forests,
 a machine learning approach.
 Machine learning techniques are known for their ability to catch complex
 non-linear relationships, often outperforming traditional statistical methods
 in predictive accuracy.
\end_layout

\begin_layout Subsection
Machine Learning Methods
\end_layout

\begin_layout Subsubsection
Survival Tree
\end_layout

\begin_layout Standard
Survival Tree is an adaptation of a Decision Tree (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Decision-Trees"
plural "false"
caps "false"
noprefix "false"

\end_inset

) to Survival Analysis, presented by LeBlanc 
\begin_inset CommandInset citation
LatexCommand cite
key "key-71"
literal "false"

\end_inset

.
 Similarly to the CART algorithm, it recursively divides the dataset at
 each node based on one feature that maximizes the dissimilarity in the
 survival distribution instead of minimizing the MSE.
 The dissimilarity is measured with a log-rank test (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Log-rank-test"
plural "false"
caps "false"
noprefix "false"

\end_inset

) or any other statistic test that can handle survival data.
 LeBlanc shows that log-rank performs well for splitting survival data.
 The scikit-survival implementation of the Survival Tree also uses log-rank.
 In the original paper by LeBlanc, pruning was suggested as a way to deal
 with overfitting.
 However, after reviewing the source code for the scikit-survival's SurvivalTree
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-72"
literal "false"

\end_inset

 and Scikit-learn's DepthFirstTreeBuilder 
\begin_inset CommandInset citation
LatexCommand cite
key "key-73"
literal "false"

\end_inset

 (used in SurvivalTree), we saw that they did not use pruning.
 The model should be regularized manually with hyperparameter tuning.
 All key regularization hyperparameters are the same as in Decision Trees
 (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Decision-Trees"
plural "false"
caps "false"
noprefix "false"

\end_inset

), namely max_depth, min_samples_split, and others.
 
\end_layout

\begin_layout Subsubsection
Gradient Boosted Survival Analysis 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Gradient-Boosted-Survival"

\end_inset


\end_layout

\begin_layout Standard
Gradient Boosted Survival Analysis (GBSA) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-56"
literal "false"

\end_inset

 is a Gradient Boosting algorithm (discussed in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stochastic-Gradient-Boosting"
plural "false"
caps "false"
noprefix "false"

\end_inset

) adapted for survival analysis and implemented within the Scikit-survival
 package.
 Friedman's Gradient Boosting is adapted to survival analysis by setting
 the partial log-likelihood as the loss function and following the algorithm
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-70"
literal "false"

\end_inset

.
 The partial log-likelihood allows for the inclusion of censored instances
 without introducing bias.
 GBSA shares many hyperparameters with the Decision Trees (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Decision-Trees"
plural "false"
caps "false"
noprefix "false"

\end_inset

), such as 
\emph on
max_depth
\emph default
, 
\emph on
min_samples_split
\emph default
 and others, for the apparent reason of trees being a base learner.
\end_layout

\begin_layout Standard
In addition to Cox Proportional Hazards (”coxph” value of 
\emph on
loss
\emph default
 hyperparameter), GBSA also allows for choosing other loss functions, such
 as squared regression loss (”squared”) that ignores predictions beyond
 the time of censoring and inverse-probability of censoring weighted least
 squares error (”ipcwls”).
 In this work, we will use the Cox Proportional Hazard loss.
 Other hyperparameters that we have used are:
\end_layout

\begin_layout Itemize

\emph on
learning_rate
\emph default
 - hyperparameter sets the weight of each learner's contribution.
\end_layout

\begin_layout Itemize

\emph on
n_estimators
\emph default
 - dictates how many estimators must be trained.
 It is the value of 
\begin_inset Formula $M$
\end_inset

 from the Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Stochastic-Gradient-Boosting"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Itemize

\emph on
subsample
\emph default
 - sets the fraction of the dataset to be used to train each individual
 learner.
 Values span 
\begin_inset Formula $(0,1]$
\end_inset

, where 
\begin_inset Formula $1.0$
\end_inset

 denotes Gradient Boosting, while values less than 
\begin_inset Formula $1.0$
\end_inset

 result in the Stochastic Gradient Boosting algorithm.
\end_layout

\begin_layout Itemize

\emph on
random_state
\emph default
 - allows us to control the randomness during training to ensure that any
 performance improvements are due to deliberate changes rather than random
 fluctuations.
\end_layout

\begin_layout Standard
For more information on the numerous hyperparameters, you can refer to the
 GBSA documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-56"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Random Survival Forest 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Random-Survival-Forest"

\end_inset


\end_layout

\begin_layout Standard

\emph on
Random Survival Forest
\emph default
 (RSF) is an ensemble machine learning method tailored for survival analysis.
 It is derived from the original Random Forest method developed by 
\begin_inset CommandInset citation
LatexCommand cite
key "key-28-breiman"
literal "false"

\end_inset

.
 Random Forests have gained popularity in the machine learning community
 due to their effectiveness in classification and regression machine learning
 tasks.
 Random Forest is built from multiple decision trees.
 Averaging their predictions allows for more accurate predictions than any
 single tree can provide.
 Similar to Random Forest, Random Survival Forest leverages the power of
 multiple survival trees, making its predictions more robust.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
rf does not use survival trees...
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
It differs from gradient boosting in a way the trees are combined: in GB
 they are connected sequentially, each trying to fix errors the previous
 one, while Random Forest just trains 
\begin_inset Formula $n$
\end_inset

 number of trees and averages their predictions.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Randomness
\end_layout

\end_inset


\end_layout

\begin_layout Standard
A Random Survival Forest is comprised of 
\emph on
Survival Trees
\emph default
, discussed in 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
.
 A Survival Tree is essentially a binary tree.
 It is developed by iteratively splitting children nodes into two nodes
 until a certain criterion is met.
 An optimal node split is the one maximizing the survival difference between
 the children nodes.
 It is done by iterating through all features and finding such a value for
 a given feature that maximizes the survival difference 
\begin_inset CommandInset citation
LatexCommand cite
key "key-27-wang"
literal "false"

\end_inset

.
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Random Survival Forest training is comprised of the following steps:
\end_layout

\begin_layout Enumerate
Randomly draw 
\begin_inset Formula $L$
\end_inset

 bootstrap samples of size 
\begin_inset Formula $n$
\end_inset

 with replacement from the training dataset.
 
\begin_inset Formula $n$
\end_inset

 is about two-thirds of the original data.
 The remaining instances, about one-third, are termed out-of-bag (OOB) observati
ons and are not included in the bootstrap sample.
\end_layout

\begin_layout Enumerate
For each sample, develop a full-grown survival tree based on the selected
 splitting criterion.
 At each node, randomly select 
\begin_inset Formula $\sqrt{p}$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
max_features
\end_layout

\end_inset

 (or other number) covariates, where 
\begin_inset Formula $p$
\end_inset

 is the total number of covariates.
 Stop developing when a certain condition is met (for example, when the
 terminal node has fewer observations than a predetermined threshold or
 the node reaches purity).
 
\end_layout

\begin_layout Enumerate
For each tree, calculate the cumulative hazard function with the Nelson-Aalen
 estimator.
 Find a mean of all trees to find ensemble CHF.
\end_layout

\begin_layout Enumerate
Use OOB data to determine the prediction error of the ensemble 
\begin_inset CommandInset citation
LatexCommand cite
key "key-26-Ishwaran"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In summary, the Random Survival Forest is a robust and powerful survival
 analysis approach that harnesses the collective power of multiple survival
 trees.
 Having explored several survival analysis methods, it is crucial to evaluate
 them properly since the standard machine learning performance metrics are
 not always applicable to survival analysis due to the presence of censoring.
 The next section will introduce essential criteria for evaluating survival
 analysis models and the primary performance metrics created for this purpose.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
Survival prediction models play a vital role in healthcare.
 They are often used to estimate the risk of developing a particular disease
 and are crucial in guiding the clinical management of patients.
 It is, therefore, essential to assess their performance accurately.
 Similar to machine learning, this process of model evaluation is referred
 to as model validation.
 There are three aspects we can asses our model on:
\end_layout

\begin_layout Enumerate

\series bold
Overall performance, 
\series default
which is the distance between the predicted and observed survival time.
\end_layout

\begin_layout Enumerate

\series bold
Discrimination,
\series default
 or the model's ability to distinguish between high- and low-risk patients.

\series bold
 
\end_layout

\begin_layout Enumerate

\series bold
Calibration
\series default
 is the agreement between the observed and predicted survival times 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The absence of bias when the validation set contains censored instances
 indicates a good performance measure.
 Otherwise, in the presence of high levels of censoring, the evaluation
 would be unreasonably optimistic 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In this section, we will cover three measures of discrimination and one
 measure that assesses both discrimination and calibration (overall performance).
\end_layout

\begin_layout Subsubsection
Harrel's and Uno's Concordance Indices
\end_layout

\begin_layout Standard
One way to measure discrimination is through 
\emph on
concordance
\emph default
.
 A pair of patients is 
\emph on
concordant
\emph default
 if the subject who experiences an event earlier has a greater estimated
 risk 
\begin_inset CommandInset citation
LatexCommand cite
key "key-30"
literal "false"

\end_inset

.
 
\emph on
Measures of concordance
\emph default
 quantify the rank correlation between the predicted risk and the observed
 survival times.
 Typically, their values range between 0.5 and 1.
 A value of 0.5 indicates no discrimination, while 1 corresponds to the ideal
 discrimination 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The 
\emph on
concordance index
\emph default
, or 
\emph on
C-index
\emph default
, is the most widely used performance metric in survival analysis.
 It is defined through the concordance probability.
 The
\emph on
 concordance probability
\emph default
 is the probability that from the arbitrarily selected pair of patients
 
\begin_inset Formula $(i,j)$
\end_inset

, the one with a shorter survival time, 
\begin_inset Formula $T$
\end_inset

 has the higher predicted risk, 
\begin_inset Formula $M$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

.
 Mathematically, this is expressed as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C=P(M_{i}>M_{j}|T_{i}<T_{j}).
\]

\end_inset


\end_layout

\begin_layout Standard

\emph on
Harrell's concordance index
\emph default
 is the most widely used implementation of the concordance index.
 To compute Harrel's concordance index 
\begin_inset Formula $C_{H}$
\end_inset

, we consider every comparable pair of patients where the one with the shorter
 time failed.
 Pair is 
\begin_inset Quotes sld
\end_inset

comparable
\begin_inset Quotes srd
\end_inset

 if we can determine which patient experienced the event first 
\begin_inset CommandInset citation
LatexCommand cite
key "key-30"
literal "false"

\end_inset

.
 
\begin_inset Formula $C_{H}$
\end_inset

 is estimated as the proportion of these pairs in which the subject with
 the shorter survival time has a higher estimated risk.
 A modified version of this estimator, 
\begin_inset Formula $C_{H}(\tau)$
\end_inset

, only considers patients with 
\begin_inset Formula $T_{i}<\tau$
\end_inset

 and may provide more stable estimates 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Mathematically, Harrell's C-index is defined as a ratio between the number
 of concordant and comparable pairs:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\hat{C}=\frac{\sum_{i=1}^{N}\Delta_{i}\sum_{j=i+1}^{N}\left[I(T_{i}^{obs}<T_{j}^{obs})+(1-\Delta_{j})I(T_{i}^{obs}=T_{j}^{obs})\right]\left[I(M_{i}>M_{j})+\frac{1}{2}I(M_{i}=M_{j})\right]}{\sum_{i=1}^{N}\Delta_{i}\sum_{j=i+1}^{N}\left[I(T_{i}^{obs}<T_{j}^{obs})+(1-\Delta_{j})I(T_{i}^{obs}=T_{j}^{obs})\right]}.\label{eq:c-index}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $I(\cdot)$
\end_inset

 is the indicator function.
 It is equal to 1 if its argument is true and 0 if it is not.
 
\begin_inset Formula $T^{obs}$
\end_inset

 is the observation time.
 
\begin_inset Formula $\Delta_{i}$
\end_inset

 is a binary variable, and 
\begin_inset Formula $\Delta_{i}=1$
\end_inset

 if the subject 
\begin_inset Formula $i$
\end_inset

 experienced an event during the time of observation and 
\begin_inset Formula $\Delta_{i}=0$
\end_inset

 if they did not.
\end_layout

\begin_layout Standard
According to Scikit-survival's documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22-docs"
literal "false"

\end_inset

 and Rahman et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21-rahman"
literal "false"

\end_inset

, Harrell's concordance index becomes biased in the presence of censoring.
 The bias increases with the level of censoring.
 Uno et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-31"
literal "false"

\end_inset

 introduced a modified concordance index, 
\begin_inset Formula $C_{U}(\tau)$
\end_inset

, which incorporates weights based on the probability of being censored.
 While their estimator proved robust to the choice of 
\begin_inset Formula $\tau$
\end_inset

, they noted that the error of the estimate might be quite large if there
 are too few instances beyond this time point 
\begin_inset CommandInset citation
LatexCommand cite
key "key-21,key-22"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Having explored the concordance index and its modifications, it is evident
 that it provides valuable insight into the model's discriminatory abilities.
 However, it offers a singular value that characterizes a model's performance
 across different time points without considering fluctuations in concordance
 that tend to happen over time.
 To enhance our evaluation of model performance in terms of discrimination
 across varying time points, we turn our attention to another popular metric
 in survival analysis: the ROC AUC.
 The subsequent section explores it in detail.
\end_layout

\begin_layout Subsubsection
Time-dependent Area under the ROC
\end_layout

\begin_layout Standard
The 
\emph on
area under the receiver operating characteristic curve
\emph default
 (ROC AUC) is a popular performance measure for binary classification tasks.
 In survival analysis, it is used to determine how well estimated risk scores
 can distinguish diseased patients from healthy ones 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22-docs"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In binary classification, the
\emph on
 receiver operating characteristic (ROC)
\emph default
 is a curve that plots the 
\emph on
true positive rate (TPR or sensitivity)
\emph default
 against the 
\emph on
false positive rate (FPR)
\emph default
.
 The FPR is the ratio of negative instances falsely classified as positive.
 It is equal to 
\begin_inset Formula $1-$
\end_inset

 the 
\emph on
true negative rate
\emph default
 (the ratio of negative instances that are correctly classified, often referred
 to as 
\emph on
specificity
\emph default
).
 TPR, or sensitivity, represents the ratio of positive instances classified
 as positive 
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In survival analysis, we extend the ROC to continuous outcomes, where a
 patient is alive at the start of the observation but might experience an
 event later.
 Specificity and sensitivity, therefore, become time-dependent measures.
 It is especially important, as model accuracy tends to be different at
 different points in time.
 Here, we consider 
\emph on
cumulative cases
\emph default
 and 
\emph on
dynamic controls
\emph default
 at any given time 
\begin_inset Formula $t$
\end_inset

.
 
\emph on
Cumulative cases
\emph default
 are all subjects who experienced an event before or at time 
\begin_inset Formula $t$
\end_inset

, while 
\emph on
dynamic controls
\emph default
 are those who have yet to experience the event after time 
\begin_inset Formula $t$
\end_inset

.
 By calculating the ROC AUC for any given time point 
\begin_inset Formula $t$
\end_inset

, we assess the model's ability to differentiate between patients.
 Specifically, how well the model can distinguish patients who fail by a
 given time 
\begin_inset Formula $t_{i}<t$
\end_inset

 from subjects who fail after this time 
\begin_inset Formula $t_{i}>t$
\end_inset

.
 The time-dependent ROC AUC is especially helpful when we want to predict
 an event happening in a period up to time 
\begin_inset Formula $t$
\end_inset

, rather than at a specific time-point 
\begin_inset Formula $t$
\end_inset

 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22-docs"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
While the time-dependent ROC AUC provides a valuable measure of a model's
 discrimination ability at various time points, it falls short in providing
 insight into the accuracy of individual predictions – calibration.
 To address this limitation and provide a more comprehensive model assessment,
 we turn our attention to the time-dependent Brier score.
\end_layout

\begin_layout Subsubsection
Time-dependent Brier Score 
\end_layout

\begin_layout Standard
Time-dependent ROC AUC and concordance index are great for assessing the
 overall discrimination among all time points (mean AUC and c-index) and
 the discrimination at any individual time point (the ROC graph), but they
 tell us nothing about the accuracy of individual predictions 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22-docs"
literal "false"

\end_inset

.
 A metric analogous to regression performance measures used in machine learning
 would be ideal.
 Fortunately, such a metric exists.
 
\emph on
Time-dependent Brier score
\emph default
 is a modification of mean squared error (MSE) that handles right censored
 data.
\end_layout

\begin_layout Standard
While the concordance index and time-dependent ROC AUC measure only discriminati
on, the time-dependent Brier score measures both discrimination and calibration,
 making it a metric of 
\begin_inset Quotes sld
\end_inset

overall performance
\begin_inset Quotes srd
\end_inset

.
 It is defined by the following equation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
BS^{c}(t)=\frac{1}{n}\sum_{i=1}^{n}I(y_{i}\leq t\wedge\delta_{i}=1)\frac{(0-\hat{\pi}(t|\mathbf{x}_{i}))^{2}}{\hat{G}(y_{i})}+I(y_{i}>t)\frac{(1-\hat{\pi}(t|\mathbf{x}_{i}))^{2}}{\hat{G}(t)}
\]

\end_inset


\end_layout

\begin_layout Standard
where 
\series bold

\begin_inset Formula $\hat{\pi(t|\mathbf{x})}$
\end_inset


\series default
is a model’s predicted probability of remaining event-free up to the time
 point 
\begin_inset Formula $t$
\end_inset

 for feature vector 
\series bold
x
\series default
, and 
\begin_inset Formula $\frac{1}{\hat{G}(t)}$
\end_inset

 is the inverse probability of censoring weight 
\begin_inset CommandInset citation
LatexCommand cite
key "key-22-docs"
literal "false"

\end_inset

.
 
\begin_inset Formula $I(\cdot)$
\end_inset

 is the indicator function.
 
\begin_inset Formula $y_{i}$
\end_inset

 is the subject's survival time, 
\begin_inset Formula $\delta_{i}$
\end_inset

 is the event/censoring indicator.
\end_layout

\begin_layout Standard
The time-dependent Brier Score's limitation is that it applies exclusively
 to models capable of estimating the survival function.
 The 
\emph on
Integrated Brier Score
\emph default
 provides a scalar value for general model evaluation It is beneficial for
 model comparison, as time-dependent measures are a bit harder to compare
 than scalar values, and for the model fine-tuning process.
\end_layout

\begin_layout Standard
\begin_inset space \space{}
\end_inset


\end_layout

\begin_layout Standard
In this chapter, we covered various approaches for training machine learning
 models.
 We discussed the training process, data preparation, handling missing values,
 and finding optimal hyperparameters for our models.
 Additionally, we discussed survival analysis, which offers a range of technique
s for predicting the time to an event.
 It encompasses various models and performance metrics capable of handling
 censored data during training, prediction, and model evaluation.
 Now, we are well-equipped to perform our analysis.
 However, before we proceed, let us ensure that we have a dataset to work
 with and a solid understanding of the data .
\end_layout

\begin_layout Chapter
Data Preparation and Analysis
\begin_inset CommandInset label
LatexCommand label
name "chap:Data-Preparation-and"

\end_inset


\end_layout

\begin_layout Standard
In this chapter, we will examine the study's dataset.
 We will explore its most important features and their influence on survival
 time.
\end_layout

\begin_layout Section
Data Acquisition
\end_layout

\begin_layout Standard
The dataset provided by the IKEM (Institute of Clinical and Experimental
 Medicine in Prague) that we initially had was unsuitable for any meaningful
 analysis.
 It had very few records, an insufficient number of features, many missing
 values, and quite outdated follow-ups.
 That is why it was decided to look for data elsewhere.
\end_layout

\begin_layout Standard
A number of institutions were contacted, including the Scientific Registry
 of Transplant Recipients (SRTR), the Australia and New Zealand Dialysis
 and Transplant Registry (ANZDATA), the National Health Service (NHS), the
 Center for Research in Transplantation and Translational Immunology (CR2TI),
 and other organizations from Spain, Germany and Canada.
 However, we faced the following obstacles:
\end_layout

\begin_layout Itemize
High cost of acquiring the data
\end_layout

\begin_layout Itemize
The need for association with a local research group
\end_layout

\begin_layout Itemize
The need for local citizenship.
\end_layout

\begin_layout Standard
Fortunately, the United Network for Organ Sharing (UNOS), a US organization,
 agreed to provide their data for free upon signing the data use agreement.
 The database consists of 1,108,884 records for all kinds of transplants.
 The table that contained data crucial for our analysis encompassed both
 kidney and pancreas transplants, had 993 806 records and 457 columns for
 both transplanted patients and ones from the waiting list.
 Data span from October 1, 1987, to September 2022.
 Kidney transplants account for 490 172 records.
\end_layout

\begin_layout Section
Data Loading
\end_layout

\begin_layout Standard
The data were provided in the form of a MongoDB database dump.
 Unfortunately, the database dump alone was insufficient for data analysis.
 It was necessary to run the database, import the database dump, and export
 data in the appropriate format.
 We set up the MongoDB database in a Docker container (Docker explained
 in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

) running locally on our PC, as we were not allowed to install Docker in
 our university cluster.
 The corresponding table was then exported to CSV, compressed into a zip
 archive, and uploaded to the cluster.
 
\end_layout

\begin_layout Standard
The process of loading the data using the 
\emph on
read_csv()
\emph default
 method of the Pandas DataFrame was taking too long (about 5 minutes) due
 to the large size of the CSV file (80GB).
 The nature of data science is quite iterative, so we had to optimize this
 inefficiency.
 As a result, it was decided to store data in a Parquet file.
 Parquet is an efficient cloud computing format that works on the principles
 of databases, allowing for more efficient data loading.
 The 
\emph on
DataFrame.to_parquet()
\emph default
 method was used to dump the pandas DataFrame into the Parquet database
 file.
 This significantly reduced the loading time of the entire dataset to just
 38 seconds.
 Moreover, it allows for specifying the columns to load, which further reduces
 the data loading time to a mere 21 seconds.
 Therefore, using this technology has significantly improved the workflow.
\end_layout

\begin_layout Section
Data Preprocessing Pipeline
\end_layout

\begin_layout Standard
This section will describe the data pipeline used to create the dataset
 from the raw data.
 The pipeline can be found in the GitHub repository of this paper: 
\begin_inset CommandInset href
LatexCommand href
name "survival_pipeline.py"
target "https://github.com/krllstdn/BachelorProject/blob/main/Code/surv_data_pipeline/survival_pipline.py"
literal "false"

\end_inset

.
\begin_inset Note Note
status open

\begin_layout Plain Layout
Make sure it works
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The work with the pipeline is pretty straightforward: we initialize the
 class and call the 
\emph on
load()
\emph default
 method.
 As is shown in the following block of Python code:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "language=Python,numbers=left,basicstyle={\ttfamily},breaklines=true,tabsize=4"
inline false
status open

\begin_layout Plain Layout

from surv_data_pipeline.survival_pipeline import ScikitSurvivalDataLoader
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

loader = ScikitSurvivalDataLoader()
\end_layout

\begin_layout Plain Layout

X, y = loader.load()
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Two main class constants are categorical_values and numerical_values, whose
 names speak for themselves.
 The primary class method is 
\emph on
load()
\emph default
.
 This method loads the data into the pandas DataFrame, processes them, and
 returns 
\emph on
X
\emph default
 and 
\emph on
y
\emph default
.
 
\emph on
X
\emph default
 represents data transformed into numbers, while 
\emph on
y
\emph default
 is a tuple consisting of two elements: PSTATUS and PTIME.
 PSTATUS is the boolean censoring indicator (True - the event happened,
 False - otherwise), PTIME is the number of days survived.
 All Scikit-survival estimators require this format of the target value.
\end_layout

\begin_layout Standard
The first step is to load the data from the parquet file into Pandas DataFrame.
 It is done using the Pandas method 
\emph on
read_parquet(path, engine, columns)
\emph default
.
 In the 
\emph on
path
\emph default
 property, we need to specify the path to the parquet file; the 
\emph on
engine
\emph default
 parameter specifies the data loading scheme.
 We use 
\emph on
'auto'
\emph default
, which tries PyArrow first; if that does not work, it uses FastParquet.
 PyArrow and FastParquet are just interfaces for reading data from Parquet
 files.
 In 
\emph on
columns
\emph default
, we need to specify the columns we want to load.
 
\end_layout

\begin_layout Standard
Now, we need to filter out all records that might add noise to the data.
 Firstly, we remove patients who died of unrelated causes, such as accidents.
 Next, we filter out all recipients from the pediatric group, as the transplanta
tion at that age differs significantly from that of adults.
 Finally, depending on the donor type for which we are training a model,
 we will include only living or deceased patients.
 
\end_layout

\begin_layout Standard
The next step is to handle missing (NaN) values.
 It is done with _handle_nan() method.
 We tried using mean and median imputation techniques from 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Handling-Missing-Feature"
plural "false"
caps "false"
noprefix "false"

\end_inset

, but that only led to data leakage and suboptimal performance, so we just
 dropped all rows with empty columns.
 Fortunately, the size of the dataset allowed us to do that.
\end_layout

\begin_layout Standard
At this point, data were pickled and downloaded, and the next steps were
 carried out locally.
 It is more comfortable to work locally and it is good to have a control
 over the later stages of data processing in each iteration of training.
 This helped to iterate faster and to get the results quicker.
\end_layout

\begin_layout Standard
After the NaN handling step, the training set is sent to the method 
\emph on
_get_X_y()
\emph default
 (or the same steps are done locally) where the numerical columns are scaled
 (with 
\emph on
StandardScaler()
\emph default
) and categorical is one-hot encoded (with 
\emph on
OneHotEncoder()
\emph default
) in the Scikit-learn transformer pipeline.
 The pipeline is pickled for later use in the application to transform the
 user input into data digestible by the model.
 Numerical and categorical values then comprise the 
\emph on
X
\emph default
 set.
 The target value set is constructed with the 
\emph on
Surv.from_arrays()
\emph default
 utility that accepts event and survival time and builds the y value acceptable
 to scikit-survival algorithms.
\end_layout

\begin_layout Standard
At this stage, the pipeline is divided in two: for models trained locally
 and models trained on the university cluster.
 For models trained locally, we pickle and download the data.
 And perform scaling for numerical features, one-hot encoding for categorical
 ones and For models trained on the cluster, data are sent to the method
 
\emph on
_get_X_y().
 
\emph default
In both cases similar steps are taken: numerical columns are standardized
 (with 
\emph on
sksurv.column.standardize()
\emph default
), categorical ones are one-hot encoded (with 
\emph on
sksurv.preprocessing.OneHotEncoder()
\emph default
), and target value is constructed with the 
\emph on
Surv.from_arrays()
\emph default
 method that accepts event and survival time and builds the y value acceptable
 to scikit-survival algorithms.
 
\end_layout

\begin_layout Standard
There is only one distinction in the pipeline for models used in the application.
 We used custom transformers built with Scikit-learn to have the ability
 to pickle the pipeline used for training and use it later in the application
 to avoid the complexity and potential errors of developing a custom pipeline.
 This approach ensures consistency and minimizes mistakes, thereby enhancing
 the reliability of our model deployment process.
 Models, unfortunately, need to be retrained with data from these transformers
 for use in the application, as our chosen approach does not preserve the
 column names, and we cannot assess feature importance with certainty.
 The approach with scikit-survival methods described before allows for preservin
g the column names and accurate feature importance assessment.
\end_layout

\begin_layout Standard
At this point we have the dataset to work with.
\end_layout

\begin_layout Section
Exploratory Data Analysis
\end_layout

\begin_layout Standard
In this section, we will comprehensively cover the most significant features.
 It is important to note that the data were not adjusted to limit the influence
 of other factors.
 As a result, the correlations we are attempting to make here may not be
 entirely accurate; however, some have been confirmed in the existing literature.
 All plots are based on the training dataset.
 Features in DDT/LDT sections differ because different features were deemed
 signifficant by the permutation importance algorithm for the given transplantat
ion type.
\end_layout

\begin_layout Subsection
General Data
\end_layout

\begin_layout Standard
Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:feature_table}
\end_layout

\end_inset

 lists the features used in this work along with their descriptions and
 types.
 
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
check if there are some new ones
\end_layout

\end_inset

 The features used in each model differ from model to model and can be found
 in the feature importance figures of each model in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Machine-Learning-Models"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
label{tab:feature_table}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{lll}
\end_layout

\begin_layout Plain Layout


\backslash
toprule
\end_layout

\begin_layout Plain Layout

        Feature &                                        Description & 
        Type 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
midrule
\end_layout

\begin_layout Plain Layout

    ON
\backslash
_DIALYSIS &    The recipient's pre-transplant dialysis status.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   PRE
\backslash
_TX
\backslash
_TXFUS &  The recipient's history of pre-transplant bloo...
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

         GENDER &                                   Recipient gender & 
 Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

         ETHCAT &                                Recipient ethnicity & 
 Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

           DIAB &  Recipient diabetes status.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

 HCV
\backslash
_SEROSTATUS &  Recipient Hepatitis C status &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   DIABETES
\backslash
_DON &  Donor diabetes status.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

     ETHCAT
\backslash
_DON &                                    Donor ethnicity.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

        ABO
\backslash
_MAT &                   Donor recipient blood type match type.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

       HBV
\backslash
_CORE &  Recipient Hepatitis B status.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

     LIV
\backslash
_DON
\backslash
_TY &                                         Living donor type.
 &  Categorical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

            AGE &                        Recipient age at transplant.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

       BMI
\backslash
_CALC &                        Recipient BMI at transplant.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

        AGE
\backslash
_DON &                            Donor age at transplant.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

      CREAT
\backslash
_TRR &                 Recipient serum creatinine at transplant.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

          NPKID &                     Number of previous transplants.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   COLD
\backslash
_ISCH
\backslash
_KI &  The duration kidneys are preserved at low temperature.
 &    Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

 KI
\backslash
_CREAT
\backslash
_PREOP &  Living donor preoperative serum creatinine (mg/dL) &    Numerical
 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    HGT
\backslash
_CM
\backslash
_CALC &                    Recipient height in centimeters &    Numerical
 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   BMI
\backslash
_DON
\backslash
_CALC &                                          Donor BMI &    Numerical
 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

	DIALYSIS
\backslash
_TIME &  Days on dialysis.
  & Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    KDPI &  Kidney Donor Profile Index.
 & Numerical 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
bottomrule
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Survival Data
\end_layout

\begin_layout Standard
In this subsection, we will explore the y-axis that will be used for the
 survival estimator training.
 As mentioned earlier, the 
\emph on
y
\emph default
 value consists of boolean censoring status (PSTATUS) and time (PTIME),
 a numerical value representing the survival time or the time of censoring.
 The 
\emph on
y
\emph default
 value is called the 
\emph on
survival data
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/EDA/survival-box.pdf
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Box Plots for the Survival Time for Deceased and Living Donor Groups
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "box_surv_time"

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "box_surv_time"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the survival time box plot (PTIME column) for living and deceased
 donor transplantations.
 The first quantile (
\begin_inset Formula $Q_{1}$
\end_inset

) of the deceased donor subgroup is equal to 1770 days (4.85 years), the
 median is 2926 days (8 years), and the third quantile (
\begin_inset Formula $Q_{3}$
\end_inset

) is equal to 3828 days (11.2 years).
 The interquartile range (
\emph on
IQR
\emph default
) is 2058 days, which forms the box.
 The whiskers extend from 0 days to 7397.3 (20.3 years) days.
 Any value above 7397.3 is an outlier, as it is above the 99th percentile.
 The statistics for the living are generally better: 
\begin_inset Formula $Q_{1}$
\end_inset

corresponds to 2287.75 days (6.3 years), the median is 3280 days (9 years),
 and 
\begin_inset Formula $Q_{3}$
\end_inset

is equal to 4426 days (12.1 years).
 However, the 99th percentile is much closer to the value in the deceased
 subgroup compared to the quantiles: 7623 days (20.9 years), which is only
 226 days of difference.
 As can be seen the deceased donor subset has more outliers than the living
 donor one.
 This might be attributed to the larger number of instances.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/censoring-pie.pdf

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Charts of Censored/Non-censored Values for Deceased and Living Donor
 Groups 
\begin_inset CommandInset label
LatexCommand label
name "fig:bar_pstatus"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:bar_pstatus"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows pie charts of the PSTATUS column values for living and deceased donor
 groups.
 As can be seen, the percentage of censoring in deceased donor transplantation
 subset is 51.6%, and in living donor transplantation 68.6%.
 This difference might be attributed to the fact that the number of LDT
 has started to grow only relatively recently (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Histogram-of-Deceased/Living"
plural "false"
caps "false"
noprefix "false"

\end_inset

) and less recipients experiences the event, and that LDT tends to be slightly
 more successful 
\begin_inset CommandInset citation
LatexCommand cite
key "key-16,key-17"
literal "false"

\end_inset

, as also illustrated in 
\begin_inset CommandInset ref
LatexCommand ref
reference "box_surv_time"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Now, let us talk in more detail about the influence of the donor type on
 survival.
\end_layout

\begin_layout Subsubsection*
Donor Type
\end_layout

\begin_layout Standard
In this subsection, we will explore the influence of donor type (living
 or diseased) on survival.
 It is a well-established fact that recipients who receive kidneys from
 living donors have higher life expectancy
\begin_inset CommandInset citation
LatexCommand cite
key "key-16,key-17"
literal "false"

\end_inset

.
 Let us confirm that on the data.
 Please refer to Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_donor_types"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which plots two Kaplan-Meier survival curves for all patients from the
 dataset.
 On the graph, we can see that the LDT survival probability is indeed higher
 than the DDT survival probability.
 This is the case because often, there is no time to make full, in-depth
 HLA screening, allowing for some HLA mismatches.
 Additionally, deceased transplants may suffer from mild kidney damage due
 to the delay in transplantation.
 Living donor transplants are most often performed between blood relatives
 who share similar HLA, or there is more time to find a compatible not related
 donor, resulting in better compatibility.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/donor-type-km.pdf
	scale 85

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meier Survival Curves for Donor Types
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_donor_types"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/gender-km.pdf
	scale 85

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir Survival Curves for Genders
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_sex"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Please take a look at Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pie-Don-Ty"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which illustrates the distribution of donor types in a pie chart.
 As can be seen, deceased donor transplantation is much more prevalent.
 Now let us look at the number of deceased/living donor transplants by year
 on Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Histogram-of-Deceased/Living"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Although the number of living transplantations mostly grows each year,
 deceased transplantations still prevail.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/donor_ty_by_year.png
	scale 45

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Histogram-of-Deceased/Living"

\end_inset

Histogram of Deceased/Living Donor Transplantations by Year
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Recipient Gender
\end_layout

\begin_layout Standard
In this subsection, we will explore the gender distribution in our dataset
 and its influence on survival.
 Let us start with the distribution of gender in our dataset.
 Please look at Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pie_gender"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As can be seen, there are more males than females.
 Even though chronic kidney disease is more common in women, end-stage kidney
 disease and, therefore, the need for kidney transplants is more common
 in men 
\begin_inset CommandInset citation
LatexCommand cite
key "key-14"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/don-type-pie.pdf
	scale 90

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Chart of Donor Type Destribution
\begin_inset CommandInset label
LatexCommand label
name "fig:Pie-Don-Ty"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/gender-pie.pdf
	scale 90

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie chart for gender 
\begin_inset CommandInset label
LatexCommand label
name "fig:pie_gender"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Charts of Donor Type (Living/Deceased) and Gender
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Now, let us take a look at gender's influence on survival.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_sex"
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrates the Kaplan-Meier survival curves for men and women on the whole
 dataset.
 As can be seen from the graph, females generally have a lower risk than
 their male counterparts.
 Women usually live longer 
\begin_inset CommandInset citation
LatexCommand cite
key "key-14"
literal "false"

\end_inset

.
 Quite a significant factor is the difference between male and female immune
 responses – males have a greater risk of getting an infection than females,
 and the intensity of the infection is higher 
\begin_inset CommandInset citation
LatexCommand cite
key "key-15"
literal "false"

\end_inset

.
 Furthermore, the influence of immunosuppressants makes the risk of infection
 even worse.
\end_layout

\begin_layout Subsubsection*
The Recipient Use of Dialysis
\end_layout

\begin_layout Standard
In this subsection, we will explore the influence of dialysis on survival.
 In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_dialysis"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see two survival curves for the patients who were on dialysis and
 who were not.
 As can be seen, the patients who were on dialysis before the transplantation
 have a greater risk than those who were not.
 It agrees with 
\begin_inset CommandInset citation
LatexCommand cite
key "key-20,key-48-aufhauser"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/dialysis-km.pdf
	scale 85

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir Survival Curves for the Pre-transplant Usage of Dialysis or
 not
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_dialysis"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection*
Recipient Ethnicity
\end_layout

\begin_layout Standard
This subsection explores the survival curves of different ethnic groups.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "km_race"
plural "false"
caps "false"
noprefix "false"

\end_inset

 plots seven survival curves for various ethnicities are plotted.
 As can be seen in the image, five ethnicities share about the same survival
 probability, while three groups slightly differ from the rest other.
 Multiracial has the best survival curve, yet later falls under the other
 curves.
 Native and Native Hawaiian have curves slightly below others.
 For these three ethnicities, these survival curves are not enough to make
 any conclusions, as these are also the least populated groups.
 As for other ethnicities, they have mostly similar survival curves with
 the exception of Asian group, which is slightly higher than the rest, yet
 later converges with them.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/eth-km.pdf
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Kaplan-Meir Survival Curves for Ethnicities
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "km_race"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deceased Group
\end_layout

\begin_layout Standard
In Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:desc_deceased}
\end_layout

\end_inset

, we can see the numerical value statistics for the deceased subset, which
 includes a total of 117 568 records.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout

    
\backslash
centering
\end_layout

\begin_layout Plain Layout

    
\backslash
caption{Descriptive Statistics for Numerical Features for Deceased Donors}
\end_layout

\begin_layout Plain Layout

    
\backslash
label{tab:desc_deceased}
\end_layout

\begin_layout Plain Layout

    
\backslash
begin{tabular}{lrrrrrrrr}
\end_layout

\begin_layout Plain Layout

    
\backslash
toprule
\end_layout

\begin_layout Plain Layout

    {}  & mean & std & min & 25
\backslash
% & 50
\backslash
% & 75
\backslash
% & max 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
midrule
\end_layout

\begin_layout Plain Layout

    AGE           & 51.88 & 13.11 & 18.0 & 43.00 & 53.00 & 62.0 & 90.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    BMI
\backslash
_CALC      & 27.77 & 5.42 & 15.0 & 23.80 & 27.30 & 31.3 & 72.2 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    AGE
\backslash
_DON       & 38.10 & 16.59 & 0.0 & 24.00 & 40.00 & 51.0 & 88.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    CREAT
\backslash
_TRR     & 8.252 & 3.51 & 0.1 & 5.70 & 7.82 & 10.3 & 28.2 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    NPKID         & 0.12 & 0.36 & 0.0 & 0.00 & 0.00 & 0.0 & 5.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    COLD
\backslash
_ISCH
\backslash
_KI  & 18.02 & 8.98 & 0.0 & 11.87 & 17.00 & 23.0 & 99.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
bottomrule
\end_layout

\begin_layout Plain Layout

    
\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Moving on to Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:yn_deceased}
\end_layout

\end_inset

, we see the distribution of categorical Yes / No features.
 As we can see, the majority of kidney recipients from deceased donors were
 on dialysis and did not receive blood transfusions.
 The fact that the majority of recipients from the deceased donor subset
 were on dialysis may be attributed to the fact that the suitable living
 donor was not found in time, and the deceased donor's kidney was received
 more or less as a last resort.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
caption{Frequency of Yes/No Variables in Deceased Donor Group}
\end_layout

\begin_layout Plain Layout


\backslash
label{tab:yn_deceased}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{llll}
\end_layout

\begin_layout Plain Layout


\backslash
toprule
\end_layout

\begin_layout Plain Layout

Value &     ON
\backslash
_DIALYSIS &    PRE
\backslash
_TX
\backslash
_TXFUS &     DIABETES
\backslash
_DON 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
midrule
\end_layout

\begin_layout Plain Layout

  Yes &  25881 (22.01
\backslash
%) &  88215 (75.03
\backslash
%) &  109920 (93.49
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   No &  91687 (77.99
\backslash
%) &  29353 (24.97
\backslash
%) &     7648 (6.51
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
bottomrule
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:ETHCAT-deceased-pie"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the ethnicity distribution of individuals who received a kidney from
 a deceased donor.
 It is worth noting that Black recipients make up 31.1% despite constituting
 only 12.6% of the US population.
 White recipients account for 46.7% of the total despite representing 58.9%
 of the US population.
 Hispanic/Latino recipients represent 14.8% of the total, while 19.1% of the
 overall US population.
 The only group with similar percentages are Asians/non-Hispanic, who make
 up 6.1% of total deceased donor kidney recipients, which is almost equivalent
 to their representation in the population at 5.6%.
 The remaining ethnicities constitute only 1.5%.
 US demographics data were taken from 
\begin_inset CommandInset citation
LatexCommand cite
key "key-68"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/ethncat-deceased-pie.pdf
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Chart of Ethnicity Destributions for the Deceased Donor Subset 
\begin_inset CommandInset label
LatexCommand label
name "fig:ETHCAT-deceased-pie"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pie-deceased-features"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows distributions for donor diabetes, and recipient hepatitis C status.
 65.8% of the recipients do not have diabetes.
 Hepatitis C is a viral infection causing liver inflammation that can lead
 to severe liver damage.
 33.5% have different forms of diabetes.
 The diabetes status of the remaining 0.8% is unknown.
 Almost 90% of the recipients did not have Hepatite C, while 6.4% tested
 positive.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/diab-hcv-deceased-pie.pdf
	scale 40

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Charts of Diabetes Status and Hepatite C Serological Status for the
 Deceased Donor Subset
\begin_inset CommandInset label
LatexCommand label
name "fig:pie-deceased-features"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Living Group
\end_layout

\begin_layout Standard
Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:desc_living}
\end_layout

\end_inset

 illustrates the basic statistics for the numerical values for the living
 subset that contains 52420 records.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout

    
\backslash
centering
\end_layout

\begin_layout Plain Layout

    
\backslash
caption{Descriptive Statistics for Numerical Variables in Living Donor Group}
\end_layout

\begin_layout Plain Layout

    
\backslash
label{tab:desc_living}
\end_layout

\begin_layout Plain Layout

    
\backslash
begin{tabular}{lrrrrrrrr}
\end_layout

\begin_layout Plain Layout

    
\backslash
toprule
\end_layout

\begin_layout Plain Layout

    {}  & 
\backslash
multicolumn{1}{c}{mean} & 
\backslash
multicolumn{1}{c}{std} & 
\backslash
multicolumn{1}{c}{min} & 
\backslash
multicolumn{1}{c}{25
\backslash
%} & 
\backslash
multicolumn{1}{c}{50
\backslash
%} & 
\backslash
multicolumn{1}{c}{75
\backslash
%} & 
\backslash
multicolumn{1}{c}{max} 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
midrule
\end_layout

\begin_layout Plain Layout

    KI
\backslash
_CREAT
\backslash
_PREOP  & 0.87 & 0.38 & 0.10 & 0.70 & 0.80 & 1.00 & 25.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    SERUM
\backslash
_CREAT  & 1.78 & 1.42 & 0.10 & 1.10 & 1.40 & 1.90 & 21.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    NPKID  & 0.10 & 0.33 & 0.00 & 0.00 & 0.00 & 0.00 & 4.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    AGE  & 47.46 & 13.78 & 18.00 & 37.00 & 48.00 & 58.00 & 85.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    HGT
\backslash
_CM
\backslash
_CALC  & 171.11 & 10.67 & 109.00 & 162.60 & 170.20 & 178.00 & 213.4 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    BMI
\backslash
_DON
\backslash
_CALC  & 26.90 & 4.37 & 15.01 & 23.71 & 26.60 & 29.71 & 71.9 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    AGE
\backslash
_DON  & 41.34 & 11.55 & 15.00 & 32.00 & 41.00 & 50.00 & 77.0 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
bottomrule
\end_layout

\begin_layout Plain Layout

    
\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Moving on, Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:ethcat_living}
\end_layout

\end_inset

 presents the ethnicity distributions of donors and recipients in the living
 donor group.
 Notably, Whites constitute 65-68% of donors and recipients.
 It is higher than the corresponding percentage in the deceased donor group
 and in the US population, which may be attributed to the larger pool of
 potential donors and the resulting relative ease of finding a match.
 On the other hand, Blacks are close to their percentage in the population,
 while the percentage of Hispanics/Latinos is lower than their percentage
 in the population.
 Other ethnicities constitute less than 2%.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout

    
\backslash
centering
\end_layout

\begin_layout Plain Layout

    
\backslash
caption{Frequency Table for ETHCAT and ETHCAT
\backslash
_DON in Living Donor Group}
\end_layout

\begin_layout Plain Layout

    
\backslash
label{tab:ethcat_living}
\end_layout

\begin_layout Plain Layout

    
\backslash
begin{tabular}{lrr}
\end_layout

\begin_layout Plain Layout

    
\backslash
toprule
\end_layout

\begin_layout Plain Layout

    Category &  ETHCAT (
\backslash
%) &  ETHCAT
\backslash
_DON (
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
midrule
\end_layout

\begin_layout Plain Layout

    White, non-Hispanic &  34127 (65.1
\backslash
%) &  35778 (68.25
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    Black, non-Hispanic &  7664 (14.6
\backslash
%) &  7386 (14.09
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    Hispanic/Latino      &  7565 (14.43
\backslash
% &  6639 (12.67
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    Asian, non-Hispanic  &   2340 (4.46
\backslash
%) &   1958 (3.74
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    American Indian/Alaska Native, non-Hispanic &   365 (0.7
\backslash
%) &   291 (0.56
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    Native Hawaiian/Other Pacific Islander, non-Hispanic &   185 (0.35
\backslash
%) &  216 (0.41
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    Multiracial, non-Hispanic &   174 (0.33
\backslash
%) &  152 (0.29
\backslash
% 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

    
\backslash
bottomrule
\end_layout

\begin_layout Plain Layout

    
\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:yn_living}
\end_layout

\end_inset

 provides information on the distribution of two Yes/No categories in the
 living donor subset, namely ON_DYALISIS and PRE_TX_TXFUS.
 The distribution of the ON_DYALISIS feature differs from that of the deceased
 donor subset.
 Here, the distributions of ”Yes” and ”No” are much closer to each other.
 This can be attributed to the fact that chronic kidney disease was caught
 in time, and the appropriate living donor was found before the use of dialysis
 was needed.
 Furthermore, the data indicate that the receivers of blood transfusions
 (PRE_TX_FUS) are in the minority.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
caption{Frequency of Yes/No Variables in Living Donor Group}
\end_layout

\begin_layout Plain Layout


\backslash
label{tab:yn_living}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{lll}
\end_layout

\begin_layout Plain Layout


\backslash
toprule
\end_layout

\begin_layout Plain Layout

Value &    ON
\backslash
_DIALYSIS (
\backslash
%)  &    PRE
\backslash
_TX
\backslash
_TXFUS (
\backslash
%)  
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
midrule
\end_layout

\begin_layout Plain Layout

  Yes &  22488 (42.9
\backslash
%) &  42388 (80.86
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

   No &  29932 (57.1
\backslash
%) &  10032 (19.14
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
bottomrule
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:np_living}
\end_layout

\end_inset

 illustrates the distribution of recipient hepatitides.
 Recipients tested negative for hepatitis C and B are in the majority.
 Categories ”Not Done” and ”Positive” are in the minority; however, the
 amount of ”Not Done” for hepatitis B is rather significant.
 
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{table}[htbp]
\end_layout

\begin_layout Plain Layout


\backslash
centering
\end_layout

\begin_layout Plain Layout


\backslash
caption{Frequency of N/P Variables in Living Donor Group}
\end_layout

\begin_layout Plain Layout


\backslash
label{tab:np_living}
\end_layout

\begin_layout Plain Layout


\backslash
begin{tabular}{llll}
\end_layout

\begin_layout Plain Layout


\backslash
toprule
\end_layout

\begin_layout Plain Layout

{} & Value &  HCV
\backslash
_SEROSTATUS (
\backslash
%)  &        HBV
\backslash
_CORE (
\backslash
%)  
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
midrule
\end_layout

\begin_layout Plain Layout

N  &     N &  50237 (95.84
\backslash
%) &  43276 (82.56
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

ND &     P &     801 (1.53
\backslash
%) &   6429 (12.26
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

P  &    ND &    1382 (2.64
\backslash
%) &    2715 (5.18
\backslash
%) 
\backslash

\backslash

\end_layout

\begin_layout Plain Layout


\backslash
bottomrule
\end_layout

\begin_layout Plain Layout


\backslash
end{tabular}
\end_layout

\begin_layout Plain Layout


\backslash
end{table}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pie-GENDER-DIAB-ABO"
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrates the distributions of GENDER, DIAB, and ABO_MAT in the living
 donor subset.
 The gender distribution is the same as in the full dataset.
 Furthermore, 71.8% of recipients do not have diabetes; the remaining patients
 have some form of it.
 Additionally, three-quarters of all transplants are of identical blood
 groups, while 24% of living donor transplants are of compatible blood groups,
 and 1.4% are of incompatible blood groups.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/diab-abo-living-pie.pdf
	scale 60

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Chart of Diabetes Status and ABO Match in Living Donor Subgroup
\begin_inset CommandInset label
LatexCommand label
name "fig:Pie-GENDER-DIAB-ABO"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally, Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Pie-Living-donor"
plural "false"
caps "false"
noprefix "false"

\end_inset

 shows the distribution of the living donor type feature that describes
 a recipient's biological/social relationship to a donor.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/EDA/liv-don-ty-pie.pdf
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Pie Chart for Living Donor Type for the Living Donor Subset
\begin_inset CommandInset label
LatexCommand label
name "fig:Pie-Living-donor"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Dataset building, Exclusion Criteria and Noise Reduction
\end_layout

\begin_layout Paragraph
Exclusion Criteria
\end_layout

\begin_layout Standard
Certain exclusion criteria were applied to the datasets to ensure better
 performance.
 Individuals under the age of 18 or those deceased from unrelated causes
 were removed.
 While censoring such deceased patients may have been an option, this was
 deemed unnecessary.
 Additionally, records with categorical values such as "Unknown" or "Not
 Done" were removed as they added noise to the models and hindered performance.
 However, these values were still used for training application models.
\end_layout

\begin_layout Paragraph
Handling of missing values
\end_layout

\begin_layout Standard
When handling missing values, it was decided to remove entire rows rather
 than use mean imputation, which introduced bias to the model.
 Fortunately, due to the abundance of available data, this approach did
 not negatively impact the results.
\end_layout

\begin_layout Section
Feature Engineering
\end_layout

\begin_layout Standard
We attempted to do feature engineering with donor-recipient height, weight,
 BMI, and age differences, as was done in 
\begin_inset CommandInset citation
LatexCommand cite
key "key-47-naqvi"
literal "false"

\end_inset

.
 However, these features had an adverse effect on performance, leading us
 to exclude them.
 The engineered feature that actually improved performance was the number
 of days the patient was on dialysis (DIALYSIS_TIME).
 The inspiration was drawn from 
\begin_inset CommandInset citation
LatexCommand cite
key "key-48-aufhauser,key-61"
literal "false"

\end_inset

.
\end_layout

\begin_layout Chapter
Machine Learning Models 
\begin_inset CommandInset label
LatexCommand label
name "chap:Machine-Learning-Models"

\end_inset


\end_layout

\begin_layout Section
Problem Formulation
\end_layout

\begin_layout Standard
Predicting the survival time after a successful kidney transplant can be
 approached in three ways: as a regression problem, classification problem,
 or survival analysis.
 
\end_layout

\begin_layout Standard
A 
\emph on
regression
\emph default
 model may seem an intuitive choice, as we want to predict a numerical value
 – the survival time.
 However, it is not the best option for the following reasons:
\end_layout

\begin_layout Standard
1.
 
\series bold
The censored dataset
\series default
.
 The dataset has a high level of censoring – 76%.
 Including living and deceased patients would introduce too much noise to
 the model, making it highly inaccurate because the survival time is unknown
 for the living recipients.
\end_layout

\begin_layout Standard
2.
 
\series bold
Censoring removal would produce bias
\series default
 
\series bold
and significantly reduce the dataset
\series default
.
 We could remove all censored instances, but that would significantly reduce
 the dataset and introduce significant bias, as the dataset would contain
 only deceased patients, and most of them passed away before the introduction
 of modern techniques for treating rejection.
 Additionally, the more recent transplants are all unsuccessful, that would
 contribute to the bias.
 As a result, the model trained on such a dataset would be highly inaccurate.
\end_layout

\begin_layout Standard
3.
 
\series bold
Regression predicts only one single number.

\series default
 It poses a problem, especially over extended time frames, as there are
 too many factors that we cannot account for, leading to incorrect predictions.
 
\end_layout

\begin_layout Standard
Another way of formulating the problem is classification.
 We can theoretically divide the dataset into groups: ”less than one year”,
 ”one to five years”, ”five and more,” or even more groups and train a classifie
r based on them, as Naqvi et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-47-naqvi"
literal "false"

\end_inset

 did.
 Again, we would face the problems of censoring and bias mentioned above,
 so classification is not the best option.
 
\end_layout

\begin_layout Standard
A more appropriate way of problem formulation is survival analysis.
 Survival analysis methods handle censoring and provide a better form of
 prediction: the survival function or hazard function, which represents
 survival probability or the failure rate at each moment in time, respectively.
 This formulation is superior to others because it acknowledges the inherent
 uncertainty in predicting life expectancy.
 Instead of providing overly specific predictions, survival analysis allows
 for a probabilistic approach that accommodates natural variability.
 By focusing on survival probability or hazard, we can make more meaningful
 and realistic predictions.
\end_layout

\begin_layout Section
Model Selection and Training Approach
\end_layout

\begin_layout Standard
In our study, we used the scikit-survival library algorithms to train our
 models.
 When choosing the models we would fully train and fine-tune in the end,
 we tried most of the models that scikit survival offers on a small subset
 of data and chose the ones that perform best while taking adequate time.
 Despite SVM-based survival models demonstrating superior performance on
 the small subset of data, we decided against using them due to their inherent
 inability to handle large datasets, which results in extended training
 times and frequent crashes when attempting to train on the slightly larger
 than 1000 samples subset, not even mentioning the entire training set.
 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
show the results
\end_layout

\end_inset


\end_layout

\begin_layout Standard
After rigorous experimentation, we ultimately selected CoxnetSurvivalAnalysis
 (Cox Elastic Net, later referred to as coxnet) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-54"
literal "false"

\end_inset

, Random Survival Forest (RSF) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-55"
literal "false"

\end_inset

, and Gradient Boosted Survival Analysis (GBSA) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-56"
literal "false"

\end_inset

 as our preferred models.
 The latter two are ensemble models widely recognized for their high accuracy
 and robustness despite prolonged training time.
 The former demonstrated a notably low training time but worse results than
 the ensemble models.
 It allowed us to experiment with less time invested than the other models.
 Further research led us to discover an approach to training the Coxnet
 in the scikit survival documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

, which offered comparable performance to other models, requiring only a
 tiny fraction of the training time and computational resources.
 This showed that the Cox Elastic Net is more than a viable alternative
 to powerful and robust ensemble models, especially when time and computational
 resources are limited.
 
\end_layout

\begin_layout Standard
However, we encountered some challenges due to the relatively large dataset,
 even after all exclusion criteria and the removal of NaN values.
 RSF and GBSA demonstrated a prolonged training time compared to Coxnet
 (hours instead of minutes), and RSF demonstrated high computational demands.
 The latter resulted in frequent kernel crashes in our Jupyter notebooks
 after long hours of training.
 To mitigate these issues, we divided our dataset based on living and deceased
 donors and reduced the number of features used.
 These subsets are later referred to by the name of the type of transplantation,
 namely the deceased donor transplantation (DDT) and the living donor transplant
ation (LDT).
 The models were trained for each subset separately.
 To reduce the number of features, we trained models on smaller subsets
 of data and removed ones that showed negative or zero feature importance.
 This approach allowed us to account for features unique to a particular
 subset (KDPI for deceased and LIV_DON_TY for living) and to tailor our
 models to a specific type of transplantation.
 Sizes of the resulting datasets are as follows: 53,082 instances in living
 (42,465 in training) and 117,536 instances in deceased (94,028 training).
 By training separate models for each subset, we were able to develop potentiall
y more accurate and effective models.
 
\end_layout

\begin_layout Standard
After the removal of missing values, the combined datasets contain 170,618
 instances, which is a significant reduction from the initial dataset of
 nearly 500,000 samples.
 Despite this reduction, the remaining samples are likely representative
 of the omitted data, and the dataset size remains sufficient for effective
 model training.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
add features used in the living and deceased as new table (but they are
 already shown for each model in feature importance......) just mention then that
 features are there.
\end_layout

\end_inset


\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
In this study, six models were fine-tuned and trained, three for each subset.
 We utilized the Cox Elastic Net, Random Survival Forest, and GradientBoostedSur
vivalAnalysis models from the scikit-survival library.
 Two models were trained locally (Coxnet and GBSA) on Apple MacBook Air
 16GB RAM, and one was trained in a cluster (RSF).
 The preferred model in this study is Coxnet due to its impressive performance
 and short training time of 30 to 180 seconds compared to other models that
 can take several hours to train.
 
\end_layout

\begin_layout Standard
As numerical performance measures, the Uno concordance index (c-index),
 integrated Brier score (IBS), and the mean cumulative/dynamic area under
 curve (AUC) were chosen.
 The performance of trained models for living and deceased datasets can
 be observed in Tables 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:living-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:deceased-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 As can be seen, the DDT models perform poorer than the living ones.
 This is likely related to the inherent donor-recipient compatibility limitation
s within the DDT dataset and the presence of older transplants performed
 with less refined technniques.
 These shortcomings are reflected in the models.
 Moreover, models have more or less the same performance for the same dataset,
 which might mean that models capture data well, and the only bottleneck
 is the dataset.
 Consequently, the only potential way to improve the performance is to expand
 the dataset with further feature engineering.
 For more detailed information on the training of each model, please refer
 to the corresponding section from the following sections.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.136
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.743
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gradient Boosting
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.722
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.136
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.742
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.139
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.744
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model comparison for living dataset
\begin_inset CommandInset label
LatexCommand label
name "tab:living-comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.695
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.163
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.729
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Gradient Boosting
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.699
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.162
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.734
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Random Survival Forest
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.691
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.164
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.724
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model comparison for deceased dataset
\begin_inset CommandInset label
LatexCommand label
name "tab:deceased-comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For LDT, RSF shows slightly superior discrimination (AUC), but the overall
 prediction accuracy (BS) is inferior to Coxnet and GBSA.
 For the deceased, GBSA shows superior discrimination to Coxnet and RSF,
 but the prediction accuracy (BS) is the very similar for GBSA and Coxnet,
 and worse for RSF.
 Coxnet is preferred for both living and deceased due to its low training
 time.
 However, if training time is not a concern, GBSA may be preferred for the
 deceased cohort due to its slightly higher discriminative ability.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align left
\begin_inset Graphics
	filename Images/Results/AUC_living.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
AUC for all living models
\begin_inset CommandInset label
LatexCommand label
name "fig:AUC-living"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align right
\begin_inset Graphics
	filename Images/Results/BS_living.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Time-dependent Brier Score for all living models
\begin_inset CommandInset label
LatexCommand label
name "fig:BS-living"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Time dependent Brier and AUC scores for all living models
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe two figures in a row not the best for such graphics.
 Try using that in data analysis section.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
As time-dependent metrics, ROC AUC and Brier Score were chosen.
 Please look at Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:AUC-living"
plural "false"
caps "false"
noprefix "false"

\end_inset

, where we can see the plot of AUC over time for all living models.
 Even though the mean AUCs are almost identical, values vary over time.
 From day 1000 to 2800, RSF performs better, then from day 2900 to day 4000,
 coxnet and GBSA overperform RSF.
 All models performed similarly from day 4000 to 5000 except for a slight
 rise in coxnet and GBSA, which later converged with RSF.
 Day 5000 to 6000 RSF outperforms the two models again.
 RSF outperforms Coxnet and GBSA on 3/5 of the timeline, underperforms on
 1/5, and is mostly similar on 1/5.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/AUC_deceased.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
AUC deceased
\begin_inset CommandInset label
LatexCommand label
name "fig:AUC-deceased"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/BS_deceased.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
BS deceased
\begin_inset CommandInset label
LatexCommand label
name "fig:BS-deceased"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Time dependent Brier and AUC scores for all deceased models
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:BS-living"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see time-dependent Brier scores for all living models.
 Coxnet and GBSA are identical here despite having different learners.
 We had to adjust the visualization settings for the other to be seen.
 The RSF curve is slightly higher than the two, meaning overall performance
 is slightly worse despite mostly superior discrimination in the previous
 graph.
 
\end_layout

\begin_layout Standard
In Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:AUC-deceased"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can see the AUC graph for the deceased models.
 GBSA shows superior discriminative ability, Coxnet is in the middle, and
 RSF shows the worst performance.
 Although all similarly rise over time.
 
\end_layout

\begin_layout Standard
Please look at Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:BS-deceased"
plural "false"
caps "false"
noprefix "false"

\end_inset

, which plots the Brier Score for the deceased models.
 Similarly to LDT, the performance of Coxnet and GBSA is identical, but
 RSF is slightly worse, although by a small margin.
 
\end_layout

\begin_layout Subsection
Coxnet 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Coxnet"

\end_inset


\end_layout

\begin_layout Standard
Coxnet, also known as a Cox elastic net or Cox regularized regression, is
 a linear model that efficiently handles large datasets and performs well.
\end_layout

\begin_layout Standard
As Tables 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:living-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:deceased-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show, Coxnet performed better for the living group and worse for the deceased.
\end_layout

\begin_layout Paragraph
Hyperparameters
\end_layout

\begin_layout Standard
The coxnet has only two hyperparameters: 
\begin_inset Formula $L_{1}$
\end_inset

 ratio and 
\begin_inset Formula $\alpha$
\end_inset

.
 The 
\begin_inset Formula $L_{1}$
\end_inset

 ratio defines the relative weight of the 
\begin_inset Formula $l_{1}$
\end_inset

and 
\begin_inset Formula $l_{2}$
\end_inset

 penalty.
 We have found that the choice of 
\begin_inset Formula $L_{1}$
\end_inset

 ratio did not significantly impact the c-index, IBS, or mean AUC values.
 At both high (0.9) and low (<0.1) values of the L1 ratio, those values were
 about the same.
 However, a higher value of 
\begin_inset Formula $L_{1}$
\end_inset

 ratio (0.9) was selected for stability purposes, as suggested by the documentati
on of the scikit survival library 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
To find the best 
\begin_inset Formula $\alpha$
\end_inset

, we used the approach from the documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-25-docs"
literal "false"

\end_inset

.
 In short, it is calculated by fitting the model and choosing 
\begin_inset Formula $\alpha$
\end_inset

 that generalizes best from the resulting list 
\emph on
CoxnetSurvivalAnalysis._alphas
\emph default
.
 As was covered in 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Training-and-Hyperparameter-tuning"
plural "false"
caps "false"
noprefix "false"

\end_inset

, the hyperparameter tuning is usually performed with either GridSearch
 or RandomizedSearch.
 As there are few alphas, we used GridSearchCV from scikit-learn to find
 the best one.
 The best 
\begin_inset Formula $\alpha$
\end_inset

 for the DDT was 0.000772369005962216, while 0.00021510636871239242 was the
 best for LDT.
\end_layout

\begin_layout Paragraph
Feature importance
\end_layout

\begin_layout Standard
The importance of more than 60 features was examined with the permutation
 importance method.
 Only a fraction of them had any importance.
 Those of negative and zero importance were removed, increasing model performanc
e.
 Interestingly, HLA had little influence on long-term survival, similar
 to what Terasaki noted in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Tissue-Typing"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 So, they were removed, and the model performance improved. The coefficients
 from the trained models, which represent feature importance, are visualized
 in Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FI-forCoxnet-Living"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:FI-for-Coxnet-Deceased"
plural "false"
caps "false"
noprefix "false"

\end_inset

 (negative values also indicate non-negative importance).
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Results/Coxnet_FI_living.pdf
	scale 80

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Feature Coefficient Importance for Coxnet Living
\begin_inset CommandInset label
LatexCommand label
name "fig:FI-forCoxnet-Living"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Results/Coxnet_FI_deceased.pdf
	scale 55

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Feature Coefficient Importance for Coxnet Deceased
\begin_inset CommandInset label
LatexCommand label
name "fig:FI-for-Coxnet-Deceased"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As we can see, the most predictive features are the patient’s diabetes status,
 age, ethnicity, hepatitis C status, and dialysis.
\end_layout

\begin_layout Paragraph
Feature Engineering
\end_layout

\begin_layout Standard
We attempted to do feature engineering with donor-recipient height, weight,
 BMI, and age differences inspired by 
\begin_inset CommandInset citation
LatexCommand cite
key "key-47-naqvi"
literal "false"

\end_inset

.
 The introduction of these features worsened the performance, so they were
 excluded.
 The engineered feature that improved the performance is the number of days
 the patient was on dialysis (DIALYSIS_TIME), inspired by 
\begin_inset CommandInset citation
LatexCommand cite
key "key-48-aufhauser"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Data Pipelines
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
expanding on ...
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Data preprocessing is a critical step in machine learning, particularly
 for improving the accuracy and reliability of models.
 Depending on the context, one of two ways of data preprocessing is employed:
 scikit-learn pipeline transformers or the scikit-survival functions.
 The former is used for KidneyLife production, while the latter is used
 for training and analyzing feature importances.
\end_layout

\begin_layout Standard
Scikit-learn transformers produce a numpy array, while scikit-survival functions
 output pandas data frame, which means that the column names are not preserved
 with the former.
 Moreover, the way of processing and the number of columns in outputs are
 different, so we could not just copy the column names from one to the other.
 As a result, analyzing the importance of features with transformers is
 virtually impossible.
\end_layout

\begin_layout Standard
We employed scikit-learn pipeline transformers because we needed a pipeline
 that produces consistent results in the production environment.
 Rather than developing a pipeline from scratch, the transformer pipeline
 used during model training is pickled and then used in production.
 However, this also means that if the model is to be used in production,
 it needs to be retrained using scikit-learn pipeline transformers.
 It is worth noting that the approach to preprocessing does not affect model
 performance.
\end_layout

\begin_layout Subsection
Random Survival Forest
\end_layout

\begin_layout Standard
The random survival forest is a robust ensemble machine-learning algorithm.
 However, its training time is longer than the other methods, and it can
 be challenging to work with, especially during hyperparameter tuning.
 Moreover, RSF is known to be highly memory-hungry during training and predictio
n, making it unsuitable for deployment.
 
\end_layout

\begin_layout Standard
On the positive side, RSF can learn well from smaller datasets.
 Although Coxnet performs well on large datasets, it does worse than RSF
 on smaller ones.
 It was observed during hyperparameter tuning, where we used a subset of
 1000 transplantations from the training dataset.
 RSF fitted them well, and when evaluated on the whole test set, the performance
 was comparable to that of the entire dataset.
 In fact, the difference in performance between the subset and the whole
 dataset was minimal, as evidenced by the integrated Brier score (IBS) of
 0.13906552 (1000 instances) compared to 0.139001 (whole dataset).
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Dataset
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno c-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Living
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.139
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.744
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Deceased
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.691
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.164
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.724
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Kaplan-Meier (for IBS reference)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.247
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
RSF.
\begin_inset CommandInset label
LatexCommand label
name "tab:RSF-result"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Tables 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:living-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:deceased-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 present RSF's performance.
 Similarly to Coxnet and GBSA, it performs worse for the deceased than for
 the living.
 However, overall, the performance is quite similar between RSF and the
 other two methods.
 
\end_layout

\begin_layout Paragraph
Hyperparameters
\end_layout

\begin_layout Standard
The hyperparameters were finetuned with a script that mimicked GridSearchCV
 without cross-validation for fine-tuning.
 The table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:RSF-hyperparameters"
plural "false"
caps "false"
noprefix "false"

\end_inset

shows the fine-tuned hyperparameters, as well as the values that were tested
 and ultimately selected.
 After experimenting with higher values (80), we kept the number of estimators
 at 50 since we did not observe any noticeable improvement beyond that point,
 and the training time increased significantly from 60 to 101 minutes.
 Nonetheless, it is generally accepted that the greater the number of estimators
, the better the results.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Hyperparameter
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Values Tried
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Chosen Value
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Number of Estimators
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1, 5, 10, 30, 50, 70, 80
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Max Depth
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2,4, 6, 8, 10, 12, 14
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
12
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Min Samples Split
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2, 4, 6, 8 10, 12, 14, 16, 18
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
16
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
RSF hyperparameters
\begin_inset CommandInset label
LatexCommand label
name "tab:RSF-hyperparameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Feature Importance
\end_layout

\begin_layout Standard
Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Feature-Importance-for-RSF-living"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:RSF-feature-importance-deceased"
plural "false"
caps "false"
noprefix "false"

\end_inset

 illustrate the feature importances.
 The feature selection process can be found at ”FeatureSelection/rsf_living.ipynb
” and ”FeatureSelection/rsf_living.ipynb” (calculated in cluster file deceased.
 ipynb).
 For the living, the most important ones are, as before, recipient age,
 diabetes status, dialysis status, and dialysis time.
 The situation is similar for the deceased, but the recipient's serum creatinine
 before transplant is higher in the rating.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/RSF_FI_living.pdf
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Feature Importance for RSF LDT
\begin_inset CommandInset label
LatexCommand label
name "fig:Feature-Importance-for-RSF-living"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/RSF_FI_deceased.pdf
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
RSF Feature Importance DDT
\begin_inset CommandInset label
LatexCommand label
name "fig:RSF-feature-importance-deceased"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Training Time
\end_layout

\begin_layout Standard
The LDT trained for 65 minutes, and the DDT for about 80 minutes.
 Both are on the 50 estimators.
 Upping the estimators is a potential avenue for improvement, but one would
 need to be careful not to overfit the model.
 Moreover, the training would take a long time: 200 estimators would require
 4 hours of training, and one might overfit the model or run out of RAM.
 Overfitting can be avoided with regularization, but it is unlikely that
 it would surpass the performance of Coxnet survival by a large margin.
 
\end_layout

\begin_layout Subsection
Gradient Boosting Survival Analysis
\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
subsamples are 0.5 because Friedman.
 (it is roughly bootstrap samples and if it would be lower the variance
 would increase.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
CoxnetSurvivalAnalysis is a model with gradient-boosted Cox proportional
 hazard loss with regression trees as the base learner.
 It is a relatively slow algorithm, as it can only run on one CPU core and
 trains only one estimator at a time.
 The training time might be considerable as we cannot fully utilize the
 cluster computing power.
 However, that can also be advantageous, as we can train the model on a
 PC.
 Additionally, the algorithm provides valuable insights during training,
 such as the predicted training time and loss and out-of-bag improvement
 for each estimator.
 This information is logged regularly throughout the training process, approxima
ting the model's accuracy well at that point of training.
\end_layout

\begin_layout Standard
Tables 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:living-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:deceased-comparison"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show the performance of two GBSA models for living and deceased subsets.
 As with the two previous models, the living model performs better than
 the deceased model.
 It is the best for the deceased with c_index 0.699 and IBS 0.162.
\end_layout

\begin_layout Paragraph
Hyperparameters
\end_layout

\begin_layout Standard
We used three primary hyperparameters: the number of estimators, learning
 rate, and max_depth.
 Grid search estimated the best max_depth to be 6.
 However, experimentally, we found that 4 gives better results.
 Through trial and error, we selected a learning rate of 0.2, which resulted
 in the best outcomes.
 With lower values (0.05, 0.1, 0.15), the model was underfitted, with higher
 (0.5, 1) overfitted.
 The number of estimators was determined using an early-stopping monitor,
 a widely used approach for GB algorithms.
 The monitor analyzes the average improvement of the last 25 iterations
 and stops training if negative for the last 50.
 The monitor implementation and further explanation can be found in the
 documentation 
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"
literal "false"

\end_inset

.
 The optimal number of estimators for the deceased was 110, and for the
 living, it was 94.
 Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Living-GBSA-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Deceased-GBSA-training"
plural "false"
caps "false"
noprefix "false"

\end_inset

 show the plot of OOB improvement per learner, with the red dashed line
 representing the cutoff point at which performance begins to decline.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/gbsa_early_stopping_living.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Living GBSA training process with early stopping
\begin_inset CommandInset label
LatexCommand label
name "fig:Living-GBSA-training"

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
don't forget to remake the image
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/gbsa_early_stopping_deceased.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout
Deceased GBSA training process with early stopping
\begin_inset CommandInset label
LatexCommand label
name "fig:Deceased-GBSA-training"

\end_inset


\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
don't forget to remake the image
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GBSA training process with early stopping
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Training Time
\end_layout

\begin_layout Standard
The models were trained locally, and the training time was 75 minutes for
 the living and 60 minutes for the deceased.
 Using a lower learning rate resulted in a significantly longer training
 time (396 minutes for living at a 0.05 learning rate).
\end_layout

\begin_layout Paragraph
Feature Selection
\end_layout

\begin_layout Standard
Initially, we performed feature selection on models with a small number
 of learners, and it produced an interesting result: only a small selection
 of features was deemed to have any importance.
 The result did not seem accurate.
 Since we already had several models trained with many more learners, we
 decided to assess their feature importance.
 Subsequently, we discovered that many features initially deemed irrelevant
 were, in fact, relevant.
 Therefore, this particular feature selection approach is unsuitable for
 GBSA, and feature importance must be evaluated on a fully trained model.
 For instance, RSF is unaffected by this issue, as feature importance remains
 more or less consistent for both trained and under-trained models.
\end_layout

\begin_layout Paragraph
Feature Importance 
\end_layout

\begin_layout Standard
Figures 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:GBSA-living-feature"
plural "false"
caps "false"
noprefix "false"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:GBSA-deceased-feature"
plural "false"
caps "false"
noprefix "false"

\end_inset

 display the estimated feature importances for living and deceased GBSA
 using permutation importance.
 Notably, in the living GBSA, a few features such as PRE_TX_TXFUS, KI_KREAT_PREO
P, HCV_SEROSTATUS, and GENDER have negative importance.
 However, their removal only results in a minor performance decrease from
 0.722 to 0.720.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/GBSA_FI_living.pdf
	scale 70

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
GBSA Feature Importance LDT
\begin_inset CommandInset label
LatexCommand label
name "fig:GBSA-living-feature"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/Results/GBSA_FI_deceased.pdf
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
GBSA Feature Importance DDT
\begin_inset CommandInset label
LatexCommand label
name "fig:GBSA-deceased-feature"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Scoring algorithm
\end_layout

\begin_layout Standard
Various methods have been proposed to evaluate donor-recipient compatibility,
 such as the Estimated Post-Transplant Survival Score (EPTS) score 
\begin_inset CommandInset citation
LatexCommand cite
key "key-60"
literal "false"

\end_inset

 , which is presently utilized in the US kidney allocation system and is
 based on survival analysis techniques.
 (Mark et al., 2019) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-64"
literal "false"

\end_inset

 mentioned other scoring systems, such as Life Years from Transplant (LYFT)
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-63"
literal "false"

\end_inset

 and Recipient Risk Score (RRS) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-62"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
Our bachelor project task included the development of a new scoring technique
 for TX Matching.
 Even though we diverged from integration with it, we still thought developing
 a scoring technique was worthwhile.
 TX Matching (discussed in Subsection 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:TX-Matching"
plural "false"
caps "false"
noprefix "false"

\end_inset

) uses the following formula for scoring:
\end_layout

\begin_layout Quotation
score = 1 * (number of A and B antigen matches) 
\end_layout

\begin_deeper
\begin_layout Quotation
+ 10 (number of DR antigen matches) 
\end_layout

\begin_layout Quotation
+ (bonus for compatible blood group) * (indicator of compatible blood group)
 
\end_layout

\end_deeper
\begin_layout Standard
It focuses on histo- and blood group compatibility but does not consider
 other factors, such as potential life expectancy.
 We wanted to address this.
\end_layout

\begin_layout Standard
We aimed to develop a scoring technique based on the hazard predicted by
 the Coxnet model.
 We believed that patients who lived the longest would have the lowest hazard.
 Based on this hazard, we would create a formula that would produce a score
 ranging from 0 to 100, with 0 being the worst possible match and 100 being
 the best possible match.
 However, despite the strong performance on the c-index and integrated Brier
 score, we discovered a lack of correlation (-0.262) between the days lived
 and the predicted hazard, as depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:predicted-hazard-over-days-lived"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/Results/scoring.pdf
	scale 60

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Plot of Predicted Hazard Over Days Survived for LDT with Event Happening.
\begin_inset CommandInset label
LatexCommand label
name "fig:predicted-hazard-over-days-lived"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
add some concluding paragraph
\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion 
\end_layout

\begin_layout Standard
Our goal was to create a model that could estimate the life expectancy of
 transplant recipients for subsequent use in our application.
 We developed six models: three for living donor transplantation (LDT) and
 three for deceased donor transplantation (DDT).
 Our models' performance was comparable to the state of the art, with the
 highest c-index of 0.699 achieved for DDT using GBSA and 0.723 for LDT using
 Coxnet over a 15-year period.
 However, we found that the DDT model performance was inferior to that of
 LDT, likely due to intrinsic limitations in donor-recipient compatibility
 present in the DDT dataset.
 Another reason might be related to the data itself.
 The DDT dataset might have more of older records when the techniques of
 transplantation and immunosuppression were not that refined, which might
 negatively influence the prediction.
 The distributions of living/deceased transplantations by year can be seen
 in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Histogram-of-Deceased/Living"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Mark et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-49"
literal "false"

\end_inset

 achieved 0.724 Harrel's c index over a 5-year period with RSF and CoxPH
 on the UNOS dataset.
 Paquette et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-50"
literal "false"

\end_inset

 achieved c index of 0.65, 0.661, and 0.659 for the neural network–based models
 (DeepSurv, DeepHit, and RNN, respectively) on the SRTR dataset.
 Senanayake et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-51"
literal "false"

\end_inset

 achieved a concordance index of 0.67 with a random survival forest on the
 ANZDATA dataset.
 Other SA studies from the Ravindhran et al.
 
\begin_inset CommandInset citation
LatexCommand cite
key "key-52"
literal "false"

\end_inset

 meta-analysis showed similar results of sub-0.70 c-index.
 Only one other study 
\begin_inset CommandInset citation
LatexCommand cite
key "key-53"
literal "false"

\end_inset

 surpassed it and achieved a 0.80 concordance index with survival trees,
 but with post-transplant 3-month serum creatinine and on a smaller dataset
 (3,117 instances).
\end_layout

\begin_layout Standard
While the importance of individual features varied between models, some
 were consistently deemed more important than others.
 Across all models, recipient diabetes, age, and dialysis status or time
 were consistently among the most important features.
 Surprisingly, HLA features only hindered the performance or were unimportant
 enough.
\end_layout

\begin_layout Standard
We tried to design and implement a scoring algorithm focused on the life
 expectancy.
 However the assumption of a high correlation between the predicted hazard
 and the days survived was wrong, which did not allow us to go any further.
 However, there are working algorithms, such as the Estimated Post-Transplant
 Survival Score (EPTS) score 
\begin_inset CommandInset citation
LatexCommand cite
key "key-60"
literal "false"

\end_inset

, which is used in the US kidney allocation system and bases its formula
 on the SA model prediction and several features.
 The reason their scoring works might be that they add a lot of other factors
 besides the model prediction to the formula.
\end_layout

\begin_layout Paragraph
Limitations and Further Work
\end_layout

\begin_layout Standard
The inherent models' assumption is the compatibility of donor and recipient.
 Consequently, models should be used after the donor-recipient compatibility
 is confirmed.
 Otherwise, the predictions might be inaccurate.
\end_layout

\begin_layout Standard
We achieved good performance for the pre-transplant prediction, possibly
 close to the best possible.
 However, it might not be enough.
 Therefore, we suggest training additional models for post-transplant observatio
n and follow-up to predict the lifespan of the transplant recipient with
 higher accuracy.
 Similar to 
\begin_inset CommandInset citation
LatexCommand cite
key "key-53"
literal "false"

\end_inset

, which trained model with the post-transplant 3-month serum creatinine,
 but we advise using more features.
\end_layout

\begin_layout Standard
Our models achieved performance very close to each other for each transplantatio
n type, which means that models fitted data reasonably well, and the performance
 is limited only by the data.
 We have observed minor improvements after introducing the engineered feature
 DIALYSIS_TIME, so one could try to engineer better features.
 For example, calculate eGFR from age, serum creatinine, and gender for
 both recipient and donor and see how it influences performance.
 This feature can be more informative than all those features combined and
 can have less noise.
\end_layout

\begin_layout Standard
Training and testing were performed on data from the same UNOS dataset.
 We do not know how models will perform on other datasets, so evaluating
 the model performance on another dataset would be beneficial.
 Ideally, even to train a model on data from multiple datasets.
\end_layout

\begin_layout Standard
Upping the estimators in random survival forest is a potential avenue for
 improvement, but one would need to be careful not to overfit the model.
 That would take a long time: 200 estimators would require 4 hours of training,
 and we might overfit the model or run out of RAM.
 Overfitting can be avoided with regularization, but it is unlikely to surpass
 the achieved performance by a large margin.
\end_layout

\begin_layout Standard
Initially, we considered training a survival neural network.
 However, after reviewing the literature, we found that the models we had
 already used had provided similar or better performance than neural networks.
 Furthermore, neural networks perform best with a large amount of highly
 complex data, such as images or text.
 Our dataset is relatively simple, consisting of tabular data with no more
 than 40 features after one-hot preprocessing.
 Therefore, we believe that the algorithms we use should be at least as
 effective.
 However, training a neural network may still be a viable option for those
 willing to invest the time necessary to potentially achieve better performance.
\end_layout

\begin_layout Chapter
Applications 
\begin_inset CommandInset label
LatexCommand label
name "chap:Applications"

\end_inset


\end_layout

\begin_layout Standard
This chapter focuses on the software used in kidney transplantation.
 We will start by exploring k
\emph on
idney paired donation
\emph default
 (KPD), with its intricacies and necessary terminology.
 Then, we will examine three solutions for KPD.
 Finally, we will introduce KidneyLife - a survival estimation application
 to complement KPD software to aid medical professionals in decision-making.
\end_layout

\begin_layout Section
Existing Solutions
\end_layout

\begin_layout Standard
To the best of our knowledge, there is no survival estimation software for
 kidney transplantation as of yet.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REMOVE OR REWRITE THE SENTENCE
\end_layout

\end_inset

So, let us review other adjacent solutions: kidney pair matching software.
 We will review three options: TX Matching, KidneyMatch, and KPDGUI.
 It is worth noting that there is limited information available online about
 alternatives.
 This shortage may stem from the fact that such software is developed for
 each transplantation program individually or from the preference of some
 developers to market their solutions directly rather than on the Internet.
 Nevertheless, some hospitals still rely on more traditional and manual
 matching techniques with no aid from specialized software.
\end_layout

\begin_layout Standard
When finding a compatible donor within the recipient's social circle is
 not feasible, the patient has an option to register in the 
\emph on
kidney paired donation
\emph default
 (KPD) program, also known as the 
\emph on
kidney exchange program
\emph default
 (KEP).
 KPD is an approach to living donor transplantation, where exchanges between
 pairs with incompatible donors are arranged in a way that allows each recipient
 to obtain a compatible kidney.
 While, theoretically, a large number of pairs can participate in the exchange,
 the higher the number of patients in the sequence of transplantations,
 the higher the risk of someone refusing to donate their kidney or being
 unable to due to a medical condition, which could compromise the entire
 sequence.
 So, the number of pairs in one sequence is usually limited to some number
 that makes sense for the center.
 
\end_layout

\begin_layout Standard
An example of how KPD works is as follows.
 Suppose A, B, and C are incompatible donor-recipient pairs, and a recipient
 is compatible with the donor from the next pair.
 Donor A donates their kidney to recipient B; donor B donates theirs to
 recipient C; and donor C donates theirs to recipient A.
 Such a sequence of transplantations is called a loop.
 An illustration of that can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:KPD-loop"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="1" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top" width="0pt">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/kpd_loop.png
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:KPD-loop"

\end_inset

KPD loop 
\begin_inset CommandInset citation
LatexCommand cite
key "key-43"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/kpd_chain.png
	scale 50

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Domino-Chain"

\end_inset

Domino Chain 
\begin_inset CommandInset citation
LatexCommand cite
key "key-43"
literal "false"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In some cases, altruistic or non-directed donors (NDD) are available.
 These donors are willing to donate a kidney to anyone compatible without
 being bound to a specific recipient.
 In such instances, a sequence of transplantations called a chain is formed,
 progressing forward from one recipient to the next, culminating in the
 recipient from a regular waiting list, as can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Domino-Chain"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
Such software also performs so-called virtual cross-match.
 It is a digital simulation of a test that predicts hyperacute rejection
 (briefly explained in 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Tissue-Typing"
plural "false"
caps "false"
noprefix "false"

\end_inset

).
 However, a virtual cross-match test is not a substitution for a regular
 one and must be complemented by a physical test, as occasional disparities
 may arise between the two.
 
\end_layout

\begin_layout Subsection
TX Matching
\begin_inset CommandInset label
LatexCommand label
name "subsec:TX-Matching"

\end_inset


\end_layout

\begin_layout Standard
TX Matching is open-source software that allows KPD centers to find optimal
 matches from a pool of incompatible donor-recipient pairs and altruistic
 donors.
 It utilizes one of two algorithms (solvers): integer linear programming
 (ILP) solver and All Solution Solver to find the best possible loops and
 chains.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/txmatching.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Txmatching
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
One of its core features is multi-national support that allows for collaboration
 in the organ exchange between the Czech Republic, Austria, and Israel.
 Additionally, it offers customizable pools of patients, referred to as
 ”TXM events”, allowing for the segregation of patients based on many factors,
 including logistic feasibility.
 This feature ensures that matches are not only biologically compatible
 but also feasible according to other factors.
\end_layout

\begin_layout Standard
Furthermore, TX Matching has a wide range of configurable settings, allowing
 users to adjust parameters to their needs and requirements, enhancing the
 relevance of solved matchings.
 Matching scoring mechanism includes evaluation of blood group and HLA compatibi
lity.
 Although, TX Matching is used primarily by the Czech Institute of Clinical
 and Experimental Medicine, efforts have been made to expand to other transplant
ation centers, but unfortunately, they were unsuccessful so far.
\end_layout

\begin_layout Subsection
KidneyMatch
\end_layout

\begin_layout Standard
KidneyMatch is the software developed and used by The Alliance for Paired
 Kidney Donation (APKD).
 They highly emphasize the security of their app: multifactor authentication,
 role-based authorization, and audit trails on every action in the software.
 KidneyMatch allows specifying the level of the match run: internal, regional,
 or national.
 The matches may be specified even further with various matching criteria,
 blood group, HLA, and discretionary exclusion criteria (DEC).
 They also allow specifying the expected solution: from a single match to
 chains and loops of different sizes.
\end_layout

\begin_layout Standard
To ensure the best possible matches, KidneyMatch collects more than 80 data
 points about each donor and more than 90 data points about each recipient.
 Such extensive data collection not only ensures good matching but also
 contributes to the refinement and development of matching methods.
 Furthermore, KidneyMatching supports additional medical data such as scans
 and charts, enhancing user experience and allowing for more well-informed
 decision-making 
\begin_inset CommandInset citation
LatexCommand cite
key "key-45"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
KPDGUI
\end_layout

\begin_layout Standard
KPDGUI (Kidney Paired Donation Graphical User Interface) is an interactive
 open-source software designed for the optimization and visualization of
 KPD programs.
 It addresses the aforementioned problem of donor withdrawal from KPD by
 arranging fallback options in case someone indeed left KPD and the exchange
 cannot proceed.
 What distinguishes it from the rest is that the application provides interactiv
e visualization of the pool of incompatible pairs, non-directed donors,
 and the suggested loops and chains 
\begin_inset CommandInset citation
LatexCommand cite
key "key-44"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/kpdgui.jpeg
	scale 70

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
KPDGUI interface 
\begin_inset CommandInset citation
LatexCommand cite
key "key-44"
literal "false"

\end_inset

.
 On the image you can see a chain.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
KidneyLife
\end_layout

\begin_layout Standard
Our supervisors originally suggested creating a module for Txmatching using
 the developed models, as is reflected in the bachelor project task.
 However, due to differences in project philosophy and the kind of data
 used in Txmatching, it was decided to create a separate application.
 In the following section, we will present the application, its architecture,
 how it works, the process of its development, and what can be done further.
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
We present KidneyLife, an inference application dedicated to the user-friendly
 use of machine learning models for survival analysis.
 The application aims to aid physicians in their decision-making process
 by providing a comprehensive estimate of a kidney transplant recipient's
 lifespan.
 Due to models' assumption of donor-recipient compatibility, it should be
 used in conjunction with the previously described KPD software or other
 compatibility matching software.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/KidneyLife.png
	scale 25

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kidney-Life-UI"

\end_inset

Kidney Life UI
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Kidney-Life-UI"
plural "false"
caps "false"
noprefix "false"

\end_inset

 features KidneyLife's user interface.
 On the left, you can see the form for the input of the data.
 On its top, you can notice a dropdown menu for the model selection.
 Categorical features can be selected through dropdowns, while numerical
 data can be manually entered into text inputs.
 On the bottom of the form, you can see the Submit button, which sends the
 data to the server to make a prediction and performs checks for correct
 input and the absence of missing values.
 For more details on the validation and the front-end application, please
 refer to Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the ”Front End” subsection.
 Further insights into the UI you can get in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Functionality-and-UX"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Details regarding the prediction methods can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the 
\begin_inset Quotes sld
\end_inset

Back End
\begin_inset Quotes srd
\end_inset

 subsection.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
two identical sentences.
 make it to not repeat
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Under the Submit button, you can find a button fetching synthetical data
 from the server.
 It allows for quick model illustration without the need for manual data
 entry.
 The details of the synthetic data generation can be found in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

 under the 
\begin_inset Quotes sld
\end_inset

Back End
\begin_inset Quotes srd
\end_inset

 subsection.
\end_layout

\begin_layout Standard
It is important to note that KidneyLife should be perceived as a minimum
 viable product (MVP), offering a minimum usable set of features for user
 feedback.
 This approach was adopted to address the rapidly growing development complexity
 and uncertainty of user needs and preferences.
 Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Future-Work"
plural "false"
caps "false"
noprefix "false"

\end_inset

 outlines initial conceptualization of the application and the potential
 avenues for further development.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
may not be right description
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Architecture 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Architecture"

\end_inset


\end_layout

\begin_layout Standard
The application was built with Docker.
 Docker is a software platform that allows developers to package their applicati
ons in containers, a standard unit of software that bundles code and its
 dependencies to ensure fast and reliable execution across various computing
 environments 
\begin_inset CommandInset citation
LatexCommand cite
key "key-39"
literal "false"

\end_inset

.
 Docker compose is a utility for building and distributing multi-container
 applications.
 Each container within the Docker Compose application stack is called a
 service.
 KidneyLife consists of three services: front end, back end, and the reverse
 proxy.
\end_layout

\begin_layout Paragraph

\series bold
Front End
\end_layout

\begin_layout Standard
The front-end service contains an application developed with React, the
 popular JavaScript front-end framework developed by Facebook in 2013.
 The architecture of the front-end application is rather simple, as it consists
 only of a few key components.
 A component is an independent and reusable piece of code that takes the
 form of a JavaScript function that returns HTML.
 The main page component contains all other components and all logic related
 to the temporary data storage and sending requests.
\end_layout

\begin_layout Standard
The InputField component renders the input type text.
 Any characters apart from numbers and a dot are replaced with an empty
 string.
 It was done so, as the input type number provided inconsistent behavior:
 it worked as expected in browsers based on Chromium, such as Brave and
 Chrome, but allowed other characters in other browsers, such as Firefox
 and Safari.
 Moreover, the input type number allows the character ”e”, as it is a part
 of scientific notation.
 Furthermore, it contained unappealing arrows for increasing and decreasing
 an input number whose styles could not be altered.
\end_layout

\begin_layout Standard
The Select component renders the dropdown menu that allows one to select
 values from the provided list, which is particularly useful for categorical
 values.
 The ModalContainer serves as a wrapper and a background for the form.
 Finally, the PlotComponent renders the interactive plot from the Plotly.js
 package.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
Rename the components to normal names and put them here
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The front-end validation includes checks for the provided values.
 First and foremost, it checks if the value was indeed provided.
 It also includes min/max value validation, ensuring that the value falls
 into a specific range.
 Moreover, there is float/integer validation, depending on the data type
 the field requires.
 Finally, it verifies that the selected categorical value is indeed from
 the provided list of values, preventing users from adding their own category
 using the browser's built-in developer tools and sending the request to
 the server.
 
\end_layout

\begin_layout Standard
The styling was done with Tailwind, a CSS library that significantly simplifies
 the process of transforming the design into code compared to vanilla CSS,
 which can be cumbersome and time-consuming.
\end_layout

\begin_layout Standard
As a part of the front-end service build process, React code is compiled
 into production-ready optimized code, which is then served with the Nginx
 server.
\end_layout

\begin_layout Paragraph

\series bold
Back End
\end_layout

\begin_layout Standard
The Django Rest Framework, a popular Python web framework for building RESTful
 APIs, was used for the back end.
 An application program interface (API) is a set of guidelines that dictate
 the procedures for connectivity and data exchange between applications
 or devices, while a REST API specifically adheres to the principles of
 the representational state transfer (REST) architectural style.
 Essentially, API serves as a mechanism for accessing a resource in one
 application or service from another.
 The application or service that is accessing the resource is called the
 client.
 The application or service providing the resource is referred to as a server.
 The resources accessed here are the machine learning models trained previously,
 which were serialized or ”pickled” with Python package pickle and are deseriali
zed when we need to make a prediction.
 
\end_layout

\begin_layout Standard
The back-end REST API consists of two endpoints: one for prediction and
 the other for synthetic data generation.
 The prediction endpoint expects the request body with two parameters: the
 model name and the data dictionary.
 It then validates the data, passes data through the data pipeline to make
 the data usable by the model, loads data into the model, makes a prediction,
 and sends the results to the front end as a response.
\end_layout

\begin_layout Standard
The synthetic data generation endpoint has only one parameter: the model
 name.
 When a request is sent, it loads the proper model description JSON document
 and looks at its statistical data, particularly the 10th and 90th quantiles
 for numerical data and the frequencies for categorical data.
 Then, under the assumption of normal distribution, we generate data for
 each numerical feature and, under given value frequencies, the values for
 categorical features.
 These quantiles are used to avoid peculiar situations when minimal values
 are generated along with the maximal (e.g.
 it could generate a pair of 6-year-old donor and 86-year-old recipient).
\end_layout

\begin_layout Paragraph*

\series bold
Reverse proxy
\end_layout

\begin_layout Standard
Even though the front-end code is hosted in the docker container on the
 server, it is executed in the user's browser.
 Cross-Origin Resource Sharing (CORS) is a mechanism that allows an application
 on one URL to request data from another URL.
 The browser implements the Same-origin policy as a part of its security
 measures, allowing requests from its own URL and blocking requests from
 other URLs unless certain conditions are met.
 When a browser sends a request to the server, it has an Origin property
 in its header.
 The server adds the header Access-Control-Allow-Origin to its response.
 If Origin and Access-Control-Allow-Origin are not the same, the browser
 will prevent the server from sharing the contents of the response with
 the client.
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
reference FireShip
\end_layout

\end_inset


\end_layout

\begin_layout Standard
It is not possible, however, to identify or specify the URL of each user,
 nor is it secure to allow calling the request from all sources.
 To solve this problem, we use the reverse proxy.
 A 
\emph on
reverse proxy
\emph default
 is an intermediary server that directs the client request to the appropriate
 backend server 
\begin_inset CommandInset citation
LatexCommand cite
key "key-41-reverse-proxy"
literal "false"

\end_inset

.
 When we access the applications's URL we get redirected to the front end.
 When we want to make a prediction, we send a request to the proxy, it redirects
 the request to the back end, and we get our prediction and render it on
 the front end.
 This schema is illustrated on the Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Application-architecture"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
In addition to the CORS handling, reverse proxy increases security, as it
 serves as a barrier between the internet and the backend.
 It also increases the application's scalability, as we can create multiple
 backends on different servers and balance the load between them.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Images/AppDiagram.png
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Application-architecture"

\end_inset

Application architecture.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Deployment
\end_layout

\begin_layout Standard
To deploy the app, we first need a place to deploy it to.
 We rent a virtual private server (VPS) or a virtual machine on the cloud.
 Initially, we used Google Cloud on the free trial, but it proved to be
 too expensive, so we later migrated to another VPS hosting provider.
\end_layout

\begin_layout Standard
After securing the VPS, we bought the domain name ”kidneylife.site” from
 the domain registrar Namecheap.
 Then, we registered the domain in the domain name system (DNS), associating
 the VPS IP address with the acquired domain.
\end_layout

\begin_layout Standard
The application was then deployed to the VPS using a Shell script.
 It goes through the following steps:
\end_layout

\begin_layout Enumerate
Check if the connection with the VPS can be established
\end_layout

\begin_layout Enumerate
Zip the application folder
\end_layout

\begin_layout Enumerate
Send the zip to the VPS
\end_layout

\begin_layout Enumerate
Unzip the folder
\end_layout

\begin_layout Enumerate
Build and run the docker compose stack with the production environment variables.
\end_layout

\begin_layout Subsection
Functionality and UX
\begin_inset CommandInset label
LatexCommand label
name "subsec:Functionality-and-UX"

\end_inset


\end_layout

\begin_layout Standard
KidneyLife is a standard machine learning inference application.
 Inference application stands for an application whose sole purpose is user-frie
ndly access to the machine learning model.
 As such, it is a one-page application that has select input with two models,
 the inputs are rendered based on the selected model.
 And the interactive graph made with Plotly.js.
 Finally, it features a button ”Generate synthetic data” made for testing
 and illustration purposes.
\end_layout

\begin_layout Standard
The design of the application was crafted with consideration of several
 design principles.
 Shades of blue were chosen because of their calming effects and their associati
on with reliability 
\begin_inset CommandInset citation
LatexCommand cite
key "key-42"
literal "false"

\end_inset

.
 Moreover, blue is generally a safe color, as it appeals to a larger audience.
 Simplicity, being one of the main principles of modern web design, was
 naturally prioritized as inference applications inherently have little
 complexity.
 Furthermore, the larger margins and paddings were employed to provide 'breathin
g room' within the interface, preventing it from feeling overcrowded.
 Lastly, the design exhibits responsiveness, accommodating a wide range
 of devices, from tablets to large 2k monitors, delivering a consistent
 and comfortable user experience across different screen sizes.
\end_layout

\begin_layout Subsection
Future Work 
\begin_inset CommandInset label
LatexCommand label
name "subsec:Future-Work"

\end_inset


\end_layout

\begin_layout Standard
There are several avenues for enhancing the application's functionality
 and user experience.
 First, integrating additional machine learning models is a promising avenue
 for providing more comprehensive and accurate predictions.
 While models mostly from scikit survival were used in this work, other
 packages, such as lifelines, can offer a wider range of survival analysis
 models.
\end_layout

\begin_layout Standard
Secondly, implementing a robust login system is essential to ensure secure
 access to the application's features and confidential data.
 It is especially important if we aim to implement the features outlined
 in the subsequent paragraphs, as they imply the storage of sensitive informatio
n.
 User authentication mechanisms, such as username-password authentication
 or OAuth, can restrict access to unauthorized individuals, protecting sensitive
 data associated with subsequent features.
\end_layout

\begin_layout Standard
Moreover, introducing patient (recipient and donor) and pair management
 systems can improve user experience by enabling the saving of the predictions
 for the provided pair of patients and models in the database.
 The feature will allow users to look at the predicted survival curve later
 only with a couple of clicks, eliminating the need to enter the data manually
 each time.
 The UI concept of this system can be seen in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Patient-MS"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/KL_Patient_MS.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Patient-MS"

\end_inset

Patient and pair management system concept
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another feature worth implementing is an option of estimating the cumulative
 hazards.
 It is relatively easy to implement, as it will only include minor changes
 to the front end and creating a new endpoint, which will access the method
 of already trained model and return the results.
 This might be useful if a user wants to see the graph of cumulative risk
 in time, rather than the probability of survival.
 
\end_layout

\begin_layout Standard
Furthermore, integrating the HL7 FHIR standard would enhance interoperability
 and data exchange between the application and the hospital's ecosystem.
 FHIR, which stands for 
\emph on
Fast Healthcare Interoperability Resources
\emph default
 (FHIR) is a 
\emph on
Health Level Seven
\emph default
 (HL7) standard for electronic healthcare data exchange.
 Although only a few hospital systems currently support HL7 FHIR, its adoption
 is viewed as the future of healthcare data exchange.
 
\end_layout

\begin_layout Standard
If any of these changes are to take place, we need to add continuous integration
 (CI) and continuous delivery (CD).
 CI/CD automates testing and deployment processes, improving efficiency
 and reducing the risk of errors.
 While the deployment has already been automated, the testing has not.
 Git Hub Actions can be utilized to test the application automatically on
 the server every time the changes are pushed to Git Hub, ensuring that
 pull requests cannot be merged into the main branch if the tests fail.
 While the tests for the backend already exist, automating them with GitHub
 actions would be the next step.
\end_layout

\begin_layout Standard
Cross-browser compatibility is another avenue for enhancing the application.
 It is crucial to ensure that the application looks the same across different
 operation systems and browsers.
 It can be achieved by designing custom scrollbars, improving the select
 fields by creating custom dropdown menus, and ensuring they look consistent
 across all browsers.
 However, it is possible that we may have missed something, so further testing
 is needed.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Graphics
	filename Images/KidneyLifeDraft.png
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Kidney-Life-Dashboard"

\end_inset

Kidney Life Dashboard page concept
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Before implementing any of the mentioned points, we need to reach out to
 institutions to see if this application is what actually transplant centers
 need.
 In case it is, the further development of the application can take place,
 making sure that it aligns with the needs of any given institution.
\end_layout

\begin_layout Standard
During the later stages of development, it was discovered that a nonprofit
 organization with the name KidneyLife already exists.
 If someone is interested in the application, we would need to rename and
 rebrand it to prevent any confusion and potential legal issues.
 
\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The study involved training six models to predict the survival outcome of
 kidney transplantation, three for each type of transplantation – living
 donor transplantation (LDT) and deceased donor transplantation (DDT).
 The study used regularised Cox (Coxnet) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-54"
literal "false"

\end_inset

, gradient boosted survival analysis (GBSA) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-56"
literal "false"

\end_inset

, and random survival forest (RSF) 
\begin_inset CommandInset citation
LatexCommand cite
key "key-55"
literal "false"

\end_inset

 algorithms from the scikit-survival package.
 
\end_layout

\begin_layout Standard
The performance of the models was evaluated using the Uno concordance index
 (Uno c-index), Integrated Brier Score (IBS), and the mean cumulative/dynamic
 area under the curve (mean AUC) as numerical performance metrics.
 The Brier score and the cumulative/dynamic area under curve (AUC) were
 used as time-dependent measures.
 
\end_layout

\begin_layout Standard
The DDT models achieved an Uno c-index of 0.691 - 0.699, while the LDT models
 achieved a 0.722-0.723.
 However, the results are comparable to other papers that trained models
 without any separation.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="8" columns="4">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Living
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Uno C-index
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
IBS
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mean AUC
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Coxnet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.136
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.743
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
GBSA
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.722
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.136
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.742
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RSF
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.723
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.139
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.744
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Deceased
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.695
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.163
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.729
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
GBSA
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.699
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.162
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
0.734
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RSF
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.691
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.164
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.724
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Model performance for DDT and LDT.
 The best model is highlighted.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Hyperparameter tuning was performed with GridSearchCV and some manual tweaking.
 Parameters from Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab:The-best-hyperparameters"
plural "false"
caps "false"
noprefix "false"

\end_inset

 were found to be the best, however, some further fine-tuning might be needed,
 especially with the number of estimators in the random survival forest.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Hyperparamer Set
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
RSF
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
{num_estimators=50, max_depth=12, min_samples_split=16}
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
GBSA
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
{learning_rate=0.2, num_estimators=94(living), 110(deceased), max_depth=6
 }
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Coxnet
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
{l1_ratio=0.9, alpha=*calculated*}
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
The best hyperparameters for trained models
\begin_inset CommandInset label
LatexCommand label
name "tab:The-best-hyperparameters"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The feature selection was performed by evaluating the importance of the
 feature with the importance of permutation and the iterative exclusion
 of features with negative and zero importance.
 More than 70 features were considered and engineered, only for most of
 them to be excluded.
 The following features showed the highest importance across models: DIAB,
 AGE, ETHNCAT, HCV_SEROSTATUS, ON_DIALYSIS, and GENDER.
 Table 6.1 provides a description and type for each of these features.
 
\end_layout

\begin_layout Standard
An application named KidneyLife has been developed to assist physicians
 in their decision-making process by offering a comprehensive estimate of
 the expected lifespan of a recipient of a kidney transplant.
 The application employs Coxnet models for LDT and DDT.
 Users can enter values for the selected features, click ”Submit,” and obtain
 a prediction in the form of a survival function.
 Synthetic data generation for testing purposes is also available.
 The application is considered a minimum viable product (MVP), and its functiona
lity may be significantly expanded according to the needs of interested
 institutions.
 For further details, please refer to the 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Future-Work"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
It is worth noting that the data have the assumption of well-matched pairs,
 despite the fact that HLA features were excluded in the end.
 Consequently, models and the application should be used only in conjunction
 with KPD software and prior negative crossmatch results, otherwise, the
 results might be inaccurate.
\end_layout

\begin_layout Chapter*
Data Source and Source Code
\end_layout

\begin_layout Standard
The data reported here have been supplied by the United Network for Organ
 Sharing as the contractor for the Organ Procurement and Transplantation
 Network.
 The interpretation and reporting of these data are the responsibility of
 the author(s) and in no way should be seen as an official policy of or
 interpretation by the OPTN or the U.S.
 Government.
\end_layout

\begin_layout Standard
Source code for both application and trained models can be found in the
 following GitHub repository: ...
 Data and trained models are of cource exluded.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1-kidney-transplantation-book"
literal "false"

\end_inset

Knechtle, S.
 J., Marson, L.
 P., & Morris, P.
 (2019).
 Kidney transplantation - principles and practice: Expert consult - online
 and print (8th ed.).
 Elsevier - Health Sciences Division
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2-nobel-prize-medicine"
literal "false"

\end_inset

Nobel prize in physiology or medicine (2022) Our Scientists.
 Available at: https://www.rockefeller.edu/our-scientists/alexis-carrel/2565-nobel
-prize/ (Accessed: February 6, 2023).
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3-barker"
literal "false"

\end_inset

Barker, C.
 F., & Markmann, J.
 F.
 (2013).
 Historical Overview of Transplantation.
 Cold Spring Harbor Perspectives in Medicine, 3(4).
 https://doi.org/10.1101/cshperspect.a014977
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4-matevossian"
literal "false"

\end_inset

Matevossian, Edouard, et al.
 "Surgeon Yurii Voronoy (1895-1961)-a pioneer in the history of clinical
 transplantation: in memoriam at the 75th anniversary of the first human
 kidney transplantation." Transplant International 22.12 (2009): 1132.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5-punt"
literal "false"

\end_inset

PUNT, Jenni et al.
 Kuby immunology.
 Eight.
 vyd.
 New York: Macmillan Education, 2019.
 ISBN 9781319114701;1319114709;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6-abbas"
literal "false"

\end_inset

ABBAS, Abul K., Andrew H.
 LICHTMAN a Shiv PILLAI.
 Basic immunology: functions and disorders of the immune system.
 Sixth.
 vyd.
 Philadelphia: Elsevier, 2020.
 ISBN 9780323549431;0323549438;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"
literal "false"

\end_inset

NCI Dictionary of Cancer terms (no date) National Cancer Institute.
 Available at: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/a
bo-blood-group-system (Accessed: March 6, 2023).
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"
literal "false"

\end_inset

Dean L.
 Blood Groups and Red Cell Antigens [Internet].
 Bethesda (MD): National Center for Biotechnology Information (US); 2005.
 Chapter 2, Blood group antigens are surface markers on the red blood cell
 membrane.
 Available from: https://www.ncbi.nlm.nih.gov/books/NBK2264/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"
literal "false"

\end_inset

Aurélien Géron.
 Hands-on Machine Learning with Scikit-Learn and TensorFlow Concepts, Tools,
 and Techniques to Build Intelligent Systems.
 O’Reilly Media, Inc., Sept.
 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"
literal "false"

\end_inset

Andriy Burkov.
 THE HUNDRED-PAGE MACHINE LEARNING BOOK.
 Andriy Burkov, 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"
literal "false"

\end_inset

Gradient Boosted Models — scikit-survival 0.22.2.
 (2015).
 Readthedocs.io.
 https://scikit-survival.readthedocs.io/en/stable/user_guide/boosting.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"
literal "false"

\end_inset

Bruce, P., Bruce, A., & Gedeck, P.
 (2020).
 Practical statistics for data scientists: 50+ Essential concepts using
 R and python (2nd ed.).
 O’Reilly Media.
 p.
 141
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13-SA"
literal "false"

\end_inset

Kleinbaum, D.
 G., & Klein, M.
 (2011).
 Survival analysis: A self-learning text, third edition (3rd ed.).
 Springer.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-14"
literal "false"

\end_inset

 Ostan R, Monti D, Gueresi P, Bussolotto M, Franceschi C, Baggio G.
 Gender, aging and longevity in humans: an update of an intriguing/neglected
 scenario paving the way to a gender-specific medicine.
 Clin Sci (Lond).
 2016 Oct 1;130(19):1711-25.
 doi: 10.1042/CS20160004.
 PMID: 27555614; PMCID: PMC4994139.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-15"
literal "false"

\end_inset

vom Steeg LG, Klein SL.
 SeXX Matters in Infectious Disease Pathogenesis.
 PLoS Pathog.
 2016 Feb 18;12(2):e1005374.
 doi: 10.1371/journal.ppat.1005374.
 PMID: 26891052; PMCID: PMC4759457.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-16"
literal "false"

\end_inset

Rodrigues S, Escoli R, Eusébio C, Dias L, Almeida M, Martins LS, Pedroso
 S, Henriques AC, Cabrita A.
 A Survival Analysis of Living Donor Kidney Transplant.
 Transplant Proc.
 2019 Jun;51(5):1575-1578.
 doi: 10.1016/j.transproceed.2019.01.047.
 Epub 2019 Jan 21.
 PMID: 31155195.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-17"
literal "false"

\end_inset

Nemati E, Einollahi B, Lesan Pezeshki M, Porfarziani V, Fattahi MR.
 Does kidney transplantation with deceased or living donor affect graft
 survival? Nephrourol Mon.
 2014 Jul 5;6(4):e12182.
 doi: 10.5812/numonthly.12182.
 PMID: 25695017; PMCID: PMC4317718.
\begin_inset Note Note
status open

\begin_layout Plain Layout
up untill this one are damaged.
 some of the following might be as well
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-18"
literal "false"

\end_inset

Pisavadia B, Arshad A, Chappelow I, Nightingale P, Anderson B, Nath J, Sharif
 A.
 Ethnicity matching and outcomes after kidney transplantation in the United
 Kingdom.
 PLoS One.
 2018 Apr 13;13(4):e0195038.
 doi: 10.1371/journal.pone.0195038.
 PMID: 29652887; PMCID: PMC5898720.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-19"
literal "false"

\end_inset

Guillermo García García, Arpana Iyengar, François Kaze, Ciara Kierans, Cesar
 Padilla-Altamira, Valerie A.
 Luyckx, Sex and gender differences in chronic kidney disease and access
 to care around the globe, Seminars in Nephrology, Volume 42, Issue 2, 2022,
 Pages 101-113, ISSN 0270-9295, https://doi.org/10.1016/j.semnephrol.2022.04.001.
 (https://www.sciencedirect.com/science/article/pii/S0270929522000092)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-20"
literal "false"

\end_inset

Mange KC, Joffe MM, Feldman HI.
 Effect of the use or nonuse of long-term dialysis on the subsequent survival
 of renal transplants from living donors.
 N Engl J Med.
 2001 Mar 8;344(10):726-31.
 doi: 10.1056/NEJM200103083441004.
 PMID: 11236776.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-21-rahman"
literal "false"

\end_inset

Rahman MS, Ambler G, Choodari-Oskooei B, Omar RZ.
 Review and evaluation of performance measures for survival prediction models
 in external validation settings.
 BMC Med Res Methodol.
 2017 Apr 18;17(1):60.
 doi: 10.1186/s12874-017-0336-2.
 PMID: 28420338; PMCID: PMC5395888.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-22-docs"
literal "false"

\end_inset

Evaluating survival models — scikit-survival 0.21.0.
 (n.d.).
 Readthedocs.Io.
 Retrieved July 18, 2023, from https://scikit-survival.readthedocs.io/en/stable/us
er_guide/evaluating-survival-models.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-23-wang"
literal "false"

\end_inset

Ping Wang, Yan Li, and Chandan k.
 Reddy.
 2019.
 Machine Learning for Survival Analysis: A Survey.
 ACM Comput.
 Surv.
 51, 6, Article 110 (February 2019), 36 pages.
\begin_inset Flex URL
status open

\begin_layout Plain Layout

 https://doi.org/10.1145/3214306
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-24-docs"
literal "false"

\end_inset

Definitions:, 1.
 1.
 (n.d.).
 Survival distributions, hazard functions, cumulative hazards.
 Stanford.edu.
 Retrieved October 23, 2023, from 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-25-docs"
literal "false"

\end_inset

Penalized Cox Models — scikit-survival 0.22.1.
 (2015).
 Readthedocs.io.
 
\begin_inset Flex URL
status open

\begin_layout Plain Layout

https://scikit-survival.readthedocs.io/en/stable/user_guide/coxnet.html
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-26-Ishwaran"
literal "false"

\end_inset

‌Ishwaran, H., Kogalur, U.
 B., Blackstone, E.
 H., & Lauer, M.
 S.
 (2008).
 Random survival forests.
\begin_inset Note Note
status open

\begin_layout Plain Layout
these 3 citations cause the glyph problem 26,27,28
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-27-wang"
literal "false"

\end_inset

Wang, H., & Li, G.
 (2017).
 A selective review on random survival forests for high dimensional data.
 Quantitative bio-science, 36(2), 85.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-28-breiman"
literal "false"

\end_inset

Breiman, L.
 (2001).
 Random forests.
 Machine learning, 45, 5-32.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-29"
literal "false"

\end_inset

Gönen M, Heller G.
 Concordance probability and discriminatory power in proportional hazards
 regression.
 Biometrika 2005;92(4):1799–09.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-30"
literal "false"

\end_inset

Enrico Longato, Vettoretti, M., & Barbara Di Camillo.
 (2020).
 A practical perspective on the concordance index for the evaluation and
 selection of prognostic time-to-event models.
 Journal of Biomedical Informatics, 108, 103496–103496.
 https://doi.org/10.1016/j.jbi.2020.103496
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-31"
literal "false"

\end_inset

‌Uno H, Cai T, Pencina MJ, D'Agostino RB, Wei LJ.
 On the C-statistics for evaluating overall adequacy of risk prediction
 procedures with censored survival data.
 Stat Med.
 2011 May 10;30(10):1105-17.
 doi: 10.1002/sim.4154.
 Epub 2011 Jan 13.
 PMID: 21484848; PMCID: PMC3079915.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-32"
literal "false"

\end_inset

 Kindt TJ Goldsby RA Osborne BA Kuby J.
 Kuby Immunology.
 6th ed.
 New York: W.H.
 Freeman; 2007.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-33"
literal "false"

\end_inset

Health.
 (2022).
 Kidneys.
 Vic.gov.au.
 https://www.betterhealth.vic.gov.au/health/conditionsandtreatments/kidneys
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-34"
literal "false"

\end_inset

‌Facts About Chronic Kidney Disease.
 (2020, May 15).
 National Kidney Foundation; https://www.kidney.org/atoz/content/about-chronic-kid
ney-disease
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-35"
literal "false"

\end_inset

‌Kamyar Kalantar-Zadeh, Jafar, T.
 H., Nitsch, D., Neuen, B.
 L., & Perkovic, V.
 (2021).
 Chronic kidney disease.
 The Lancet, 398(10302), 786–802.
 https://doi.org/10.1016/s0140-6736(21)00519-5
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-36"
literal "false"

\end_inset

‌Holland, K.
 (2019, May 23).
 Everything You Need to Know About Kidney Failure.
 Healthline; Healthline Media.
 https://www.healthline.com/health/kidney-failure#outlook
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-37"
literal "false"

\end_inset

‌Causes of chronic kidney disease.
 (2022, August 29).
 National Institute of Diabetes and Digestive and Kidney Diseases; NIDDK
 - National Institute of Diabetes and Digestive and Kidney Diseases.
 https://www.niddk.nih.gov/health-information/kidney-disease/chronic-kidney-disease
-ckd/causes
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-38"
literal "false"

\end_inset

Estimated Glomerular Filtration Rate (eGFR).
 (2015, December 24).
 National Kidney Foundation; https://www.kidney.org/atoz/content/gfr
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-39"
literal "false"

\end_inset

What is a Container? | Docker.
 (2023, October 26).
 Docker.
 https://www.docker.com/resources/what-container/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-40"
literal "false"

\end_inset

‌Use Docker Compose.
 (2024).
 Docker Documentation.
 https://docs.docker.com/get-started/08_using_compose/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-41-reverse-proxy"
literal "false"

\end_inset

What is a Reverse Proxy Server? | NGINX.
 (2023, June).
 NGINX.
 https://www.nginx.com/resources/glossary/reverse-proxy-server/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-42"
literal "false"

\end_inset

Kendra Cherry, Mse.
 (2022, November 22).
 How the color blue impacts moods, feelings, and behaviors.
 Verywell Mind.
 https://www.verywellmind.com/the-color-psychology-of-blue-2795815 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-43"
literal "false"

\end_inset

Kidney Paired Donation (KPD) Program.
 (2024, January 3).
 Professional Education.
 https://professionaleducation.blood.ca/en/organs-and-tissues/programs/kidney-pair
ed-donation-kpd-program 
\begin_inset Note Note
status open

\begin_layout Plain Layout
maybe images should be referenced in some other way.
 check it!
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-44"
literal "false"

\end_inset

‌Bray, M., Wang, W., Rees, M.
 A., Peter X-K.
 Song, Leichtman, A.
 B., Ashby, V.
 B., & Kalbfleisch, J.
 D.
 (2019).
 KPDGUI: An interactive application for optimization and management of a
 virtual kidney paired donation program.
 Computers in Biology and Medicine, 108, 345–353.
 https://doi.org/10.1016/j.compbiomed.2019.03.013
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-45"
literal "false"

\end_inset

‌4 Key Features of the APKD Software Program | Alliance for Paired Kidney
 Donation.
 (2022, December 9).
 Alliance for Paired Kidney Donation |.
 https://paireddonation.org/4-key-features-of-the-apkd-software-program/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-46"
literal "false"

\end_inset

Aufhauser, D.
 D., Peng, A.
 W., Murken, D.
 R., Concors, S.
 J., Abt, P.
 L., Sawinski, D., Bloom, R.
 D., Reese, P.
 P., & Levine, M.
 H.
 (2018).
 Impact of prolonged dialysis prior to renal transplantation.
 Clinical Transplantation, 32(6).
 https://doi.org/10.1111/ctr.13260
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-47-naqvi"
literal "false"

\end_inset

‌Naqvi SAA, Tennankore K, Vinson A, Roy PC, Abidi SSR.
 Predicting Kidney Graft Survival Using Machine Learning Methods: Prediction
 Model Development and Feature Significance Analysis Study.
 J Med Internet Res.
 2021 Aug 27;23(8):e26843.
 doi: 10.2196/26843.
 PMID: 34448704; PMCID: PMC8433864.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-48-aufhauser"
literal "false"

\end_inset

Aufhauser DD Jr, Peng AW, Murken DR, Concors SJ, Abt PL, Sawinski D, Bloom
 RD, Reese PP, Levine MH.
 Impact of prolonged dialysis prior to renal transplantation.
 Clin Transplant.
 2018 Jun;32(6):e13260.
 doi: 10.1111/ctr.13260.
 Epub 2018 Jun 25.
 PMID: 29656398; PMCID: PMC6023748.
\begin_inset Note Note
status open

\begin_layout Plain Layout
2nd time this reference
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-49"
literal "false"

\end_inset

Mark E, Goldsman D, Gurbaxani B, Keskinocak P, Sokol J.
 Using machine learning and an ensemble of methods to predict kidney transplant
 survival.
 PLoS One 2019; 14.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-50"
literal "false"

\end_inset

Paquette FX, Ghassemi A, Bukhtiyarova O, Cisse M, Gagnon N, Della Vecchia
 Aet al..
 Machine learning support for decision-making in kidney transplantation:
 step-by-step development of a technological solution.
 JMIR Med Inform 2022;10:e34554
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-51"
literal "false"

\end_inset

Senanayake S, Kularatna S, Healy H, Graves N, Baboolal K, Sypek MPet al..
 Development and validation of a risk index to predict kidney graft survival:
 the kidney transplant risk index.
 BMC Med Res Methodol 2021;21:1–11.
 (0.60-0.63)
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-52"
literal "false"

\end_inset

Ravindhran B, Chandak P, Schafer N, Kundalia K, Hwang W, Antoniadis S, Haroon
 U, Zakri RH.
 Machine learning models in predicting graft survival in kidney transplantation:
 meta-analysis.
 BJS Open.
 2023 Mar 7;7(2):zrad011.
 doi: 10.1093/bjsopen/zrad011.
 PMID: 36987687; PMCID: PMC10050937.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-53"
literal "false"

\end_inset

Yoo, K.D., Noh, J., Lee, H.
 et al.
 A Machine Learning Approach Using Survival Statistics to Predict Graft
 Survival in Kidney Transplant Recipients: A Multicenter Cohort Study.
 Sci Rep 7, 8904 (2017).
 https://doi.org/10.1038/s41598-017-08008-8
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-54"
literal "false"

\end_inset

sksurv.linear_model.CoxnetSurvivalAnalysis — scikit-survival 0.22.2.
 (2015).
 Readthedocs.io.
 https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.linear_model.
CoxnetSurvivalAnalysis.html#sksurv.linear_model.CoxnetSurvivalAnalysis
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-55"
literal "false"

\end_inset

‌sksurv.ensemble.RandomSurvivalForest — scikit-survival 0.22.2.
 (2015).
 Readthedocs.io.
 https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.ensemble.Rand
omSurvivalForest.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-56"
literal "false"

\end_inset

‌sksurv.ensemble.GradientBoostingSurvivalAnalysis — scikit-survival 0.22.2.
 (2015).
 Readthedocs.io.
 https://scikit-survival.readthedocs.io/en/stable/api/generated/sksurv.ensemble.Grad
ientBoostingSurvivalAnalysis.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-57"
literal "false"

\end_inset

‌Ott, H.
 C., Matthiesen, T.
 S., Saik Kia Goh, Black, L.
 D., Kren, S.
 M., Netoff, T.
 I., & Taylor, D.
 A.
 (2008).
 Perfusion-decellularized matrix: using nature’s platform to engineer a
 bioartificial heart.
 Nature Medicine, 14(2), 213–221.
 https://doi.org/10.1038/nm1684
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-58"
literal "false"

\end_inset

‌Sánchez, P.
 L., M.
 Eugenia Fernández-Santos, Costanza, S., Climent, A.
 M., Moscoso, I., M.
 Angeles Gonzalez-Nicolas, Sanz-Ruiz, R., Rodríguez, H., Kren, S.
 M., Garrido, G., Escalante, J.
 L., Bermejo, J., Elizaga, J., Menarguez, J., Yotti, R., Villar, del, M.
 Angeles Espinosa, Guillem, M.
 S., Willerson, J.
 T., & Bernad, A.
 (2015).
 Acellular human heart matrix: A critical step toward whole heart grafts.
 Biomaterials, 61, 279–289.
 https://doi.org/10.1016/j.biomaterials.2015.04.056
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-59"
literal "false"

\end_inset

‌Jin, Z., Li, Y., Yu, K., Liu, L., Fu, J., Yao, X., Zhang, A., & He, Y.
 (2021).
 3D Printing of Physical Organ Models: Recent Developments and Challenges.
 Advanced Science, 8(17).
 https://doi.org/10.1002/advs.202101394
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-60"
literal "false"

\end_inset

‌Nagashima, H., & Hitomi Matsunari.
 (2016).
 Growing human organs in pigs—A dream or reality? Theriogenology, 86(1),
 422–426.
 https://doi.org/10.1016/j.theriogenology.2016.04.056
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-61"
literal "false"

\end_inset

‌A Guide to Calculating and Interpreting the Estimated Post- ..., optn.transplant.hrs
a.gov/media/1511/guide_to_calculating_interpreting_epts.pdf.
 Accessed 16 Apr.
 2024.
 
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-62"
literal "false"

\end_inset

Baskin-Bey ES, Kremers W, Nyberg SL.
 A recipient risk score for deceased donor renal allocation.
 Am J Kidney Dis.
 2007 Feb;49(2):284-93.
 doi: 10.1053/j.ajkd.2006.10.018.
 PMID: 17261431.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-63"
literal "false"

\end_inset

Wolfe RA, McCullough KP, Leichtman AB.
 Predictability of survival models for waiting list and transplant patients:
 calculating LYFT.
 Am J Transplant.
 2009 Jul;9(7):1523-7.
 doi: 10.1111/j.1600-6143.2009.02708.x.
 PMID: 19656143.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-64"
literal "false"

\end_inset

Mark E, Goldsman D, Gurbaxani B, Keskinocak P, Sokol J.
 Using machine learning and an ensemble of methods to predict kidney transplant
 survival.
 PLoS One.
 2019 Jan 9;14(1):e0209068.
 doi: 10.1371/journal.pone.0209068.
 PMID: 30625130; PMCID: PMC6326487.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-65"
literal "false"

\end_inset

Hamet, P., & Tremblay, J.
 (2017).
 Artificial intelligence in medicine.
 Metabolism, Clinical and Experimental, 69, S36–S40.
 https://doi.org/10.1016/j.metabol.2017.01.011
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-66"
literal "false"

\end_inset

‌Chen, T.
 K., Knicely, D.
 H., & Grams, M.
 E.
 (2019).
 Chronic Kidney Disease Diagnosis and Management.
 JAMA, 322(13), 1294–1294.
 https://doi.org/10.1001/jama.2019.14745
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-67"
literal "false"

\end_inset

‌Organ Donation Statistics | organdonor.gov.
 (2024, March 29).
 Organdonor.gov.
 https://www.organdonor.gov/learn/organ-donation-statistics
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-68"
literal "false"

\end_inset

‌USAFacts.
 (2024, June 15).
 US population by year, race, age, ethnicity, & more.
 USAFacts; USAFacts.
 https://usafacts.org/data/topics/people-society/population-and-demographics/our-
changing-population/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-69"
literal "false"

\end_inset

Friedman, J.
 (1999, March).
 Stochastic gradient boosting.
 Technical report, Stanford University, Statistics Department.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-70"
literal "false"

\end_inset

G.
 Ridgeway, “The state of boosting,” Computing Science and Statistics, 172–181,
 1999.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-71"
literal "false"

\end_inset

Leblanc, M., & Crowley, J.
 (1993).
 Survival Trees by Goodness of Split.
 Journal of the American Statistical Association, 88(422), 457–467.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-72"
literal "false"

\end_inset

scikit-survival/sksurv/tree/tree.py at v0.23.0 · sebp/scikit-survival.
 (2024).
 GitHub.
 https://github.com/sebp/scikit-survival/blob/v0.23.0/sksurv/tree/tree.py#L38-L676
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-73"
literal "false"

\end_inset

scikit-learn/sklearn/tree/_tree.pyx at main · scikit-learn/scikit-learn.
 (2024).
 GitHub.
 https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/tree/_tree.pyx#L14
0
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-74"
literal "false"

\end_inset

DecisionTreeRegressor.
 (2024).
 Scikit-Learn.
 https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegress
or.html
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-75"
literal "false"

\end_inset

‌
\end_layout

\end_body
\end_document
