#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
%% Font setup: please leave the LyX font settings all set to 'default'
%% if you want to use any of these packages:

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}

%% Use Utopia text with Fourier-GUTenberg math
%\usepackage{fourier}

%% Bitstream Charter text with Math Design math
%\usepackage[charter]{mathdesign}

%%---------------------------------------------------------------------

%% Make the multiline figure/table captions indent so that the second
%% line "hangs" right below the first one.
%\usepackage[format=hang]{caption}

%% Indent even the first paragraph in each section
\usepackage{indentfirst}

%%---------------------------------------------------------------------

%% Disable page numbers in the TOC. LOF, LOT (TOC automatically
%% adds \thispagestyle{chapter} if not overriden
%\addtocontents{toc}{\protect\thispagestyle{empty}}
%\addtocontents{lof}{\protect\thispagestyle{empty}}
%\addtocontents{lot}{\protect\thispagestyle{empty}}

%% Shifts the top line of the TOC (not the title) 1cm upwards
%% so that the whole TOC fits on 1 page. Additional page size
%% adjustment is performed at the point where the TOC
%% is inserted.
%\addtocontents{toc}{\protect\vspace{-1cm}}

%%---------------------------------------------------------------------

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}
\end_preamble
\use_default_options false
\begin_modules
theorems-ams
\end_modules
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 11
\spacing single
\use_hyperref false
\papersize a4paper
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 3cm
\topmargin 4cm
\rightmargin 2cm
\bottommargin 3cm
\headheight 0.8cm
\headsep 1cm
\footskip 0.5cm
\secnumdepth 3
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style swedish
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle headings
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
def
\backslash
documentdate{August 2, 2023}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

%%
\backslash
def
\backslash
documentdate{
\backslash
today}
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{empty}
\end_layout

\begin_layout Plain Layout

{
\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent
\align block
\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/cvut.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "60line%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\align center

\shape smallcaps
\size large
Czech Technical University in Prague
\shape default

\begin_inset Newline newline
\end_inset

Faculty of Nuclear Sciences and Physical Engineering
\end_layout

\end_inset


\begin_inset Box Frameless
position "c"
hor_pos "c"
has_inner_box 1
inner_pos "c"
use_parbox 0
use_makebox 0
width "3cm"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status collapsed

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename Images/TITLE/fjfi.pdf
	display false
	width 3cm
	height 3cm
	keepAspectRatio

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 3cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 1cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\align block

\size large
Bachelor's Degree Project
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
ends the centered part (the required new paragraph before "}" is inserted
 by \SpecialChar LyX
 as "}" is on a separate line.)
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_layout Standard
\align block
\begin_inset VSpace vfill
\end_inset


\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Author:
\series bold
Kyrylo Stadniuk
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Supervisor:
\series bold
Ing.
 Tomáš Kouřim
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Consultant:
\series bold
Ing.
 Pavel Strachota, Ph.D.
\end_layout

\begin_layout Labeling
\labelwidthstring MMMMMMMMM
Language
\begin_inset space ~
\end_inset

advisor:
\series bold
PaedDr.
 Eliška Rafajová
\end_layout

\begin_layout Labeling
\paragraph_spacing single
\labelwidthstring MMMMMMMMM
Academic
\begin_inset space ~
\end_inset

year: 2022/2023
\end_layout

\begin_layout Standard
\begin_inset External
	template PDFPages
	filename zadani_cele.pdf
	extra LaTeX "pages={1,2}"

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Acknowledgment:
\end_layout

\begin_layout Standard
\noindent
I am grateful to Ing.
 Tomáš Kouřim for his expert guidance and to Dr.
 Pavel Strachota for his invaluable support and insightful feedback throughout
 this project.
 I would also like to extend my sincerest appreciation to PaedDr Eliška
 Rafajová for her language assistance.
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\noindent

\size larger
\emph on
Author's declaration:
\end_layout

\begin_layout Standard
\noindent
I declare that this Bachelor's Degree Project is entirely my own work and
 I have listed all the used sources in the bibliography.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent
Prague,
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
documentdate
\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset

Kyrylo Stadniuk
\end_layout

\begin_layout Standard
\begin_inset VSpace 2cm
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
\lang czech
Název práce:
\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
\lang czech
Odhad délky života pacienta po úspěšné transplantaci ledviny pomocí metod
 strojového učení
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Autor:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Obor:
\emph default
 Aplikovaná Informatika
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Druh práce:
\emph default
 Bakalářská práce
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Vedoucí práce:
\emph default
 Ing.
 Tomás Kourim Mild Blue, s.r.o., Plzenská 27, Praha 5
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Konzultant:
\emph default
Ing.
 Pavel Strachota, Ph.D.
 Katedra matematiky, Fakulta jaderna a fyzikálne inzenyrska, Ceské vysoké
 udeni technické v Praze, Trojanova 13, 120 00 Praha 2
\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Abstrakt:
\emph default
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.
 Abstrakt max.
 na 10 řádků.

\end_layout

\begin_layout Standard

\lang czech
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
\lang czech
Klíčová slova:
\emph default
 klíčová slova (nebo výrazy) seřazená podle abecedy a oddělená čárkou
\end_layout

\begin_layout Standard
\begin_inset VSpace vfill
\end_inset


\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
\paragraph_spacing onehalf
\noindent

\emph on
Title:
\end_layout

\begin_layout Standard
\align block

\series bold
\size huge
Estimating patient's life expectancy after a successful kidney transplant
 using machine learning methods
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Author:
\emph default
 Kyrylo Stadniuk
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Abstract:
\emph default
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
 Max.
 10 lines of English abstract text.
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Standard
\noindent

\emph on
Key words:
\emph default
 keywords in alphabetical order separated by commas
\end_layout

\begin_layout Standard
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The goal of this paper is to explore fields of kidney transplantation and
 machine learning, create and apply machine learning model in real-world
 application.

\end_layout

\begin_layout Chapter
Medical Background
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{headings}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Why kidney fail
\end_layout

\begin_layout Section
The history of kidney transplantation.
\end_layout

\begin_layout Subsection
Early animal experiments
\end_layout

\begin_layout Standard
Advancements in surgical methods and techniques at the beginning of the
 20th century eventually led to experiments with organ transplantation.
 On March 1st, 1902, Emerich Ullman, a physician at the Vienna Medical School,
 performed the first recorded organ transplantation.
 He performed an autograft, meaning the transplantation where the donor
 and the recipient are the same individual.
 Ullmann utilized the method of vascular suturing developed by Ervin Payr,
 to connect the dog's kidney to the vessels of its neck.
 The transplant was successful - the kidney produced urine.
 The dog was presented the same day to Vienna medical society eliciting
 significant interest and discussion.
\end_layout

\begin_layout Standard
The same year other similar transplantations were made.
 Another physician, Alfred von Decastello, performed a dog-to-dog kidney
 allograft at the Institute of Experimental Pathology in Vienna.
 The kidney produced urine for a while but then stopped working.
 Later Ullman performed a dog-to-goat kidney xenograft (cross-species transplant
), and to his surprise kidney produced some urine, but later stopped.
\end_layout

\begin_layout Standard
In Lyon in the department headed by Mathieu Jabourday, his assistants Carrel,
 Briau and Villard were working on new methods of vascular suturing.
 In 1902, Alex Carrel published the method of vessel anastomosis now referred
 to as Carrel's seam.
 This technique represented a significant improvement over existing methods
 and effectively addressed the common issues of thrombosis, hemorrhage,
 stricture, and embolism
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Later Carrel moved to the United States where he continued his research
 on vessel suturing and organ transplantations at The Rockefeller Institute
 for Medical Research.
 There he perfected his method and while performing autografts and allografts
 documented what later would be recognized as ”rejection”.
 For his works in 1912, he got the Nobel Prize in Medicine.
 By this time, his method of suturing had been widely adopted in human surgeries
\begin_inset CommandInset citation
LatexCommand cite
key "key-6"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Early human transplantation
\end_layout

\begin_layout Standard
The first recorded human renal xenograft
\begin_inset Formula $ $
\end_inset

 was performed by Mathieu Jaboulay in 1906.
 He chose a pig and a goat as donor animals and performed two xenografts.
 One kidney was transported to the arm and the second to the thigh.
 Each kidney functioned for one hour
\begin_inset CommandInset citation
LatexCommand cite
key "key-1,key-4"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The second and third human transplants performed by Ernst Unger were far
 more known.
 On December 10, 1909, he performed a kidney transplant from a stillborn
 baby to a baboon.
 Even though the kidney produced no urine, the postmortem showed that vascular
 anastomosis (connection of vessels) was performed successfully.
 This inspired Unger to perform another transplantation that same month,
 but this time monkey-to-human xenograft.
 The kidney was transplanted from an ape to dying from renal failure young
 woman.
 The kidney never worked.

\end_layout

\begin_layout Standard
These early experiments demonstrated that technically kidney transplantation
 was possible, but the mechanism of rejection was not yet fully understood.
 Carrel in his famous lecture about the future of transplantation (1914)
 to the International Surgical Society mentioned that the works of his colleague
 at the Rockefeller center J.B.
 Murphy might seriously impact the development of the field.
 Murphy found that irradiation and benzol treatment increased the graft
 survival of cancer in mice.
 This observation inspired Carrel to conduct his own experiments, wherein
 he irradiated recipients and found prolonged graft survival, but these
 experiments were never formally published
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The period of the 1930s and 1940s was rather stagnant compared to the beginning
 of the century.
 European surgical centers that studied transplantology before were in decline.
 Moyo Clinic in the US was conducting some cautious experiments without
 considering Carrol's works and attempts at immunosuppression.
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset

 However, there was a notable event during this period - the first human-to-huma
n transplantation.
 It was performed by Yurii Voronyi (in literature for some reason he is
 referred to as Voronoy) on March 3, 1933, in Kherson, Ukraine.
 The recipient was a 26-year-old woman admitted to the hospital on March
 3, 1933, with mercury chloride poisoning induced by a suicide attempt the
 previous day that resulted in acute renal failure.
 Transplantation seemed the only viable option.
 It was known from previous experiments by other scientists that no xenograft
 ever was successful so human-to-human transplantation was the only feasible
 choice.
 The option of injuring a living person by organ removal was not even considered.
 It was known from the physiology that kidneys save their function a couple
 of hours after the reperfusion with ringer-solution and that organs keep
 some sterility a couple of hours after the host's death.
 So temporary cadaver transplantation until the woman's own kidneys would
 regenerate seemed to be a reasonable option.
 The transplantation was performed to the thigh's artery and vein using
 Carrel's seam with some modifications.
 After some time the kidney started to produce urine for a while but then
 eventually the allograft failed and 48 hours after the surgery the patient
 dies.
 The reason for graft failure was blood group mismatch and too long warm
 ischemia time - 6 hours, so the kidney began to degrade, resulting in an
 immune reaction to dead kidney cells and kidney blood cells.
 Voronyi performed another 5 such transplantations, which he considered
 as a bridge therapy until the recipient's own kidneys would recover.
 Kidneys produced urine for different durations from 1 to 7 days with 2
 patients eventually recovering and living normally thereafter
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
First successes
\end_layout

\begin_layout Standard
In 1946, at the Peter Bent Brigham Hospital in Boston, a group of surgeons:
 Hufnagel, Hume, and Landsteinerhuman performed kidney transplantation under
 local anesthetic on the arm vessels.
 The short period of kidney functioning may have helped the patient to recover
 from acute renal failure.
 It ignited the hospital's interest in renal transplantation.

\end_layout

\begin_layout Standard
Simonsen in Denmark, Dempster in London, and Küss in Paris concluded that
 it is preferable to place the kidney in the pelvis.
 Further, both Simonsen and Dempster deduced that the immune response was
 responsible for graft failure and both hypothesized that the humoral mechanism
 of rejection was probable.

\end_layout

\begin_layout Standard
In the early 1950s, two groups of surgeons based in Paris and Chicago performed
 pelvic kidney transplants without immunosuppression.
 In Paris, Jean Hamburger reported the first live-related kidney transplant
 between a mother and her child.
 he transplanted kidney began to function immediately.
 It functioned for 22 days until it was rejected.
\end_layout

\begin_layout Standard
A series of nine transplantations with the thigh position of the allograft
 was closely studied in Boston and the first usage of hemodialysis for the
 preparation was recorded in Boston by David Hume in 1953.
 In some of these cases, mild successes were achieved using the adrenocorticotro
pic hormone (more known as cortisone).
 It was hypothesized that the endogenous immunosuppression of uremia was
 responsible for the results rather than the drug regimen.
 Hume's findings were substantial as he concluded that prior blood transfusions,
 blood group matching between the donor and recipient, and host bilateral
 nephrectomy could be beneficial for the success of the transplant.
 These conclusions were later confirmed by subsequent studies.

\end_layout

\begin_layout Standard
These attempts in the early 1950s taught technical aspects of kidney transplanta
tion and with increased confidence on December 23, 1954 in Boston Joseph
 Murray performed kidney allograft from one identical twin to another, bypassing
 the rejection barrier.
 From that time many similar surgeries were performed in Boston.
 This caused a lot of talks and predictions but all of them were negated
 when one of such recipients got pregnant and gave birth to a completely
 normal infant.
 However, in retrospective, it didn't bring anything new scientifically,
 because the technical possibilty of kidney transplantation was evident
 and the cases of successful skin allografts between identical twins were
 known for decades, but nonetheless it was an important milestone that aroused
 the interest in further experiments
\begin_inset CommandInset citation
LatexCommand cite
key "key-1,key-4"
literal "false"

\end_inset

.
\end_layout

\begin_layout Subsection
Attempts in immunosuppression
\end_layout

\begin_layout Standard
In 1948 at Mayo Clinic patients handicapped by rheumatoid arthrytis were
 given already mentioned cortisone, adrenal cortical hormone with mild immunosup
pressive properties, that relieved their condition.
 This popularized the research on adrenal cortical hormones, but later it
 was concluded that the steroid effect was clinically insignifficant for
 transplantation.
 After that, the experiments with irradiation, abandoned by Carrel and Murphy,
 were revitalized.
 Joan Main and Richmond Prehn showed that weakening of the immune system
 of adult mice by radiation and consequent skin and bone marrow transplantation
 from the same donor resulted in skin transplant acceptance.
 This encouraged teams in Boston and Paris to pursue the similar approach
 in humans.
\end_layout

\begin_layout Standard
In 1958, Murray’s team transplantation on humans utilizing the Main-Prehn
 method conducted lethal total body irradiation (TBI) on two patients with
 additional bone marrow transplant.
 Ten more recipients were irradiated with sub-lethal TBI, but without donor
 bone marrow transplant.
 As a result 11 patients passed away within a month, the only survivor had
 sub-lethal TBI without transplanted bone marrow and he got kidney from
 his non-identical twin brother.
 This was rather revolutionary - for the first time kidney was not rejected
 from non-identical twin.
 The kidney functioned for 20 years.
 Jean Hamburger and his team performed another fraternal twin transplant
 utilizing the same irradiation technique.
 The transplant functioned for 26 years finishing with the recipient's death
 for rejection-unrelated reason.
\end_layout

\begin_layout Standard
Between 1960 and 1962 Kuss and Hamburger performed four successful transplantati
ons between non-twin patients with following TBI.
 This gave promise that the transplantations could be done in non-twins
 and potentially between anybody.
 The research continued.
\end_layout

\begin_layout Standard
It was obvious that TBI is not the best choice and that it is necessary
 to find a substitution.
 In 1959, Schwarz and Damesehek from Tufts University published paper that
 described how an anticancer drug 6-mercaptopurine (6-MP) lowered immune
 response to foreign proteins in rabbits.
 Roy Calne, a training surgeon at Royal Free Hospital, London, dissapointed
 with TBI in prolongation of kidney allograft survival in dogs, noticed
 Schwarz and Dameshek's paper and performed his own experiment in dogs and
 found that it signifficantly prolonged dog's survival.
 Charles Zukoski and David Hume found the same outcomes.

\end_layout

\begin_layout Standard
6-MP was used in three tranplantations at Royal Free Hospital, but without
 success, However Kuss and assosiates reported one prolonged graft survival
 from a nonrelated donor.
 The TBI was main agent and intermittent usage of azathioprine and prednisone
 was used as an additional therapy.
\end_layout

\begin_layout Standard
Gertrude Elion and George Hitchings provided Roy Calne with the 6-MP derivative
 - azathioprine.
 Calne showed even longer graft survival with azathioprine.
 Both Elion and Hitchings were awarded with the Nobel Prize for the development
 of 6-MP and azathioprine.
 In 1961 Azathioprine became available for human use.
\end_layout

\begin_layout Subsection
Gloom then revolution
\end_layout

\begin_layout Standard
In 1963 National Research Council organized a small conference constisting
 of 25 transplant clinitians and scientists to review the status of kidney
 transplantation at the moment.`the discussion was quite depresive.
 Clinitians presented their results, that were rather discouraging: less
 that 10% of hundreds performed transplantations survived for more than
 three months, from patients with TBI only six got to the one year mark.
 Murray reported that from his first ten patients on 6-MP one survived for
 a year, others passed away within 6 months, so it was concluded that drugs
 were not more effective than radiation.
\end_layout

\begin_layout Standard
The gloom continued untill Tom Starzl, until then unknown, did his presentation
 where he described his protocol that allowed graft survival for more than
 one year in 70% of cases.
 He was not believed at first, but then he showed medical records of his
 patients and he was eventually believed.
 The only thing that differed from other protocols with 6-MP was that addition
 of prednison.
 This was a sensation.
 In the first year after the presentation, 50 new tranplantation programs
 were founded in US alone.
 And his protocol became medical world standard for the next 20 years.
\end_layout

\begin_layout Subsection
Plateau
\end_layout

\begin_layout Standard
During the period from 1964 to 1980 nothing groundbreaking had happened,
 although the steady development was seen.
 Dialysis become available and thanks to the acumulated exprerience the
 dosages became more precise.
 The brain death was accepted and the body was supported for a while to
 save organs for transplantation.
\end_layout

\begin_layout Standard
Hemodialysis for renal failure was created by Willem Kolff from Holland
 during WWII.
 But it couldn't be used for chronic renal failure untill 1960 when was
 invented Teflon arteriovenous conduits for long-term vascular access.
\end_layout

\begin_layout Standard
Acceptance of brain death as a real death.
 Before the mid 60s the kadaver transplantation was limited by the ischemic
 damage.
 Now the additional organs were available from
\begin_inset Quotes sld
\end_inset

heartbeating kadavers
\begin_inset Quotes srd
\end_inset

.
\end_layout

\begin_layout Standard
Cold for organ preservation.
 This was suggested in 1905 by Carrel's colleague Charles Guthrerie.
 Initially, Starzl used total body hypothermia to protect donor organs,
 but by 1960 switched to infusing cold solution into the portal vein to
 protect donor livers.
 In 1963 the infusion of cold solution intravenous in the transplanted kidney
 has become a standard.
\end_layout

\begin_layout Standard
As the organ preservation for more than 6 hours was achieved in mid 60s
 the exchange of organs between centers has became practical.
 Initially sharing was local and informal, that roused the worry that the
 organs could be distributed unequally and that they could be transported
 outside of the US.
 This led to Congress passing the National Transplant Act in 1984.
 The Southeastern Organ Procurement Foundation (SEOPF), founded in 1969
 and eventually composed of 12 hospitals in several cities, served as the
 template for the United Network of Organ Sharing (UNOS) that controls organ
 allocation and placement, monitors performance of transplant centers and
 organ procurement organizations, collects data, and controls quality.
 They kindly provided us with data for this paper.
\end_layout

\begin_layout Subsection
Tissue typing
\end_layout

\begin_layout Standard
Although tissue typing was suggested by Alexis Carrel in the beginning of
 20th century it could not be proven and used until 1958 when Jean Dausset
 discovered the first human leukocyte antigen (HLA).
 Testing for antibodies was not reliable until 1964 when Paul Terasaki invented
 a microxytotoxicity assay.
 Test included mixing donor's lymphocytes and recipient's serum and quickly
 has became the standard and was name crossmatch.
 For a couple of years Terasaki performed typing for most of U.S.
 transplant centers and found a couple of observations: 1) Positive cross-match
 test predicts hyperacute rejection.
 2) matching can reliably identify optimal donor within a family.
 It was assumed that the same would work for non-related recipients.

\end_layout

\begin_layout Standard
However, when in 1970 Terasaki reviewed his large database of cadaver renal
 allografts he found no correlation with the typing.
 This raised a lot of agitation in tissue typing community and his grant
 even was temporalily suspended until others didn't report the same.
 Now it is conclded that the
\end_layout

\begin_layout Subsection
Antilymphocyte serum
\end_layout

\begin_layout Standard
Next mark was cyclosporine, a fungal derivative with immunosuppressive propertie
s discovered in 1976 by Jean-François Borel.
 It revolutionized the renal and extrarenal transplants, proving to be much
 better than the previous drug azathioprine.
 However it also had to be combined with prednisone to gain those results.
 It was used until 1989 when even more potent drug was discovered - Tacrolimus.
 It helps even when the cyclosporine with prednisone has no effect.
\end_layout

\begin_layout Standard
Tom Starzl discovered that donor leukocyte chimerism was present in patients
 who had maintained successful kidney or liver grafts for up to three decades.
\end_layout

\begin_layout Standard
chimerism is an important cause (not the consequence) of successful transplantat
ion,
\end_layout

\begin_layout Standard
successful engraftment is the result of the responses of coexisting donor
 and recipient cells each to the other causing reciprocal clonal exhaustion
 followed by peripheral clonal deletion
\end_layout

\begin_layout Subsection
Conclusion and challenges of the field
\end_layout

\begin_layout Standard
The ultimate goal is immunosuppression without drugs because drugs are often
 toxic and the proper dosing might be tricky.
\end_layout

\begin_layout Section
Immunology
\end_layout

\begin_layout Standard
The immune system is a sophisticated defense mechanism that evolved to protect
 multicellular organisms from pathogens such as bacteria, fungi, viruses,
 and parasites.
 It consists of many cells and tissues that compose a complex system that
 detects, evaluates, and responds to the invader.
 The immune system is divided into humoral and cell-mediated immunity.
 Humoral is mediated by soluble immunoglobulin proteins referred to as antibodie
s, while cell-mediated involves pathogen-specific T Lymphocytes that either
 destroy the invader or assist other cells in doing so.
 Both are essential for a complete immune response.
\end_layout

\begin_layout Standard
Lymphocyte is a type of white blood cell that is responsible for both humoral
 and cell-mediated immune responses.
 There are two types of lymphocytes: T lymphocytes (T cells) and B lymphocytes
 (B cells).
 B cells mediate humoral response by producing antigen-specific antibodies.
 An antigen is any molecular structure that binds to an antibody or specific
 surface T cell receptor, triggering an immune response.
 Once B-cell encountered an antigen it starts to produce antibodies specific
 to it, antibodies then bind to it, marking the invader for destruction.
 T cells when encountering an antigen start to proliferate forming an army
 of T cells that will eliminate the invader and will form long-term memory
 about the pathogen.
\end_layout

\begin_layout Standard
Physical barriers: epithelia and mucous membranes constitute the first line
 of defense.
 To activate the immune system the pathogen must first breach physical barriers.
 The immune system categorizes pathogens by common characteristics and designs
 its response accordingly.
 Pathogen detection and categorization rely on the interaction between pathogen
 and T-cell receptors, as well as soluble antibodies.
 Binders for T cell receptors and antibodies can be the whole pathogen’s
 body, its part, or molecules excreted by it.
\end_layout

\begin_layout Standard
Pathogens are recognized and categorized by molecular patterns that are
 associated with a particular pathogen and are referred to as pathogen-associate
d molecular patterns (PAMPs).
 Pathogen recognition receptors (PRRs), which are excreted by white blood
 cells, bind to PAMPs initiating the cascade of events that will mark a
 pathogen for destruction.
\end_layout

\begin_layout Standard
Pathogen-host interaction is a continuous arms race, as pathogens usually
 have a short life cycle and can modify their DNA to elude the host's recognitio
n systems.
 The generation of diversity in developing cells is designed to combat this.
 When lymphocytes are developing in bone marrow random PRRs are generated,
 then cells are tested on non-reactivity to host cells.
 If the test is passed the cell is released into circulation.
 The principle of recognizing self vs.
 non-self is called tolerance.
\end_layout

\begin_layout Standard
There are two interconnected systems of response: innate and adaptive.
 Innate includes primitive built-in cellular and molecular mechanisms aimed
 at preventing infections and quickly demolishing common pathogens.
 It consists of physical and molecular barriers as well as PRRs that are
 encoded in DNA and therefore are inherited.
 Innate immunity provides a fast and effective response which however is
 not very specific and cannot differentiate small differences.
 Adaptive immunity is constituted by both humoral, where antibodies neutralize
 and eradicate extracellular microbes and toxins, and cell-mediated immunity,
 where T lymphocytes exterminate intracellular invaders.
\end_layout

\begin_layout Standard
Adaptive immunity is much slower but more able to recognize small differences.
 It typically starts to act within 5 to 6 days after initial exposure.
 Because it takes time to create an army of cells with specific receptors.
 After pathogen extermination, some of the lymphocytes with the specific
 receptor become memory cells, making it easier to fight this type of pathogen.
\begin_inset CommandInset citation
LatexCommand cite
key "key-13"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
In conclusion, the immune system is a complex network of molecules, cells,
 tissues, and organs that cooperate in protecting the organism from pathogens.
 The system can be divided into two main branches: innate and adaptive,
 which cooperate in protecting the host from infections while developing
 long-term immunity to specific pathogens.
 Understanding the mechanisms of the immune system is essential to understanding
 the domain of kidney transplantation.
\end_layout

\begin_layout Section
Immunology of kidney transplant
\end_layout

\begin_layout Standard
The process of transplantation inevitably includes termination of blood
 flow, and, as a result, oxygenation.
 Therefore cell is unable to generate sufficient amount of energy to maintain
 homeostasis, leading to damage or death.
 Damage or death is associated with DAMP release that might be detected
 by both innate and adaptive immunity.
\end_layout

\begin_layout Subsection
Immune system activation Peritransplant
\end_layout

\begin_layout Standard
The process of transplantation inevitably includes termination of blood
 flow, and, as a result, oxygenation.
 Therefore cell is unable to generate sufficient amount of energy to maintain
 homeostasis, leading to damage or death.
 Damage or death is associated with DAMP release that might be detected
 by both innate and adaptive immunity.
 Mostly it is the ancient innate immunity that is activated with its soluble
 arm - complement system.
\end_layout

\begin_layout Paragraph*
Damage signals
\end_layout

\begin_layout Standard
Many DAMPS are recognized by the same PRRs that mediate response to PAMPs.
 These DAMPS include molecules that are normally hidden from the immune
 system and are produced during ischemia, such as extrecellular ATP, heat
 shock proteins (HSPs), uric acid, etc.
 Likewise, oxidative stress and decline in intracellular potassium may act
 as intracellular damage signals.

\end_layout

\begin_layout Paragraph
Complement
\end_layout

\begin_layout Standard
Complement system is comprised of series of proteine kinases that are sequential
ly activated resulting in membrane attack complex (MAC) formation.
 MAC include complement components C5 to C9, which are inserted into pathogen
 cell membrane resulting in compromising cell integrity leading to cell
 death.
\end_layout

\begin_layout Standard
There are three pathways of complement system activation: the classical
 pathway, the alternative pathway, and the mannose-binding lectin (MBL)
 pathway.
 The classical pathway is activated by IgM and IgG antibodies and participates
 in antibody-mediated rejection, that will be discussed further.
 Alternative complement is always active and therefore must be controlled
 by a regulatory proteins, to prevent inadequate responses.
 The MBL pathway is activated by damaged endothelium, a cell tissue that
 covers organs and vessels, and carbohydrates present on pathogens.
 Either pathway results in C3 convertase that cleaves C3.
 This cleavage leads to a cascade of reactions that culminate in MAC formation.
\end_layout

\begin_layout Standard
Long ischemia time results in endothelial cell damage that is acossiated
 with ischemia-reperfusion injury (IRI).
 IRI activates MBL and alternative complement pathways.
\end_layout

\begin_layout Standard
Gene silencing using small interfering RNA (siRNA) might be a promising
 instrument in organ transplantation, because it can be appliend to an allograft
 during cold reperfusion and it has been shown to mitigate IRI in animal
 models.
 Other strategies of supressing local complement activation would also be
 useful.
\end_layout

\begin_layout Subsection
Stimulation of Adaptive Alloimmunity
\end_layout

\begin_layout Standard
Immune response to a graft occurs in two main stages: afferent and efferent
 arms.
 In afferent stage, recipient lymphocytes are stimulated by donor antigens
 and start to proliterate and send signals to other cells.
 In efferent arm, leukocytes migrate to the transplanted organ and donor
 specific antibodies are produced.

\end_layout

\begin_layout Standard
For the immune system to be activated graft must express antigens that will
 be considered by the host's immune system as foreign.
 These include ABO antigens, human leukocyte antigens (HLA), and polymorphic
 non-HLA
\begin_inset Quotes sld
\end_inset

auto-antigens
\begin_inset Quotes srd
\end_inset

.

\end_layout

\begin_layout Subsubsection*
ABO blood group antigens
\end_layout

\begin_layout Standard
ABO system is used to group blood into groups, based on presence or absense
 of antigens on a blood cell surface.
 There are four major blood groups: A, B, O and AB.
\begin_inset CommandInset citation
LatexCommand cite
key "key-7"
literal "false"

\end_inset


\end_layout

\begin_layout Standard
When allocating an organ to transplant the first thing that is considered
 is ABO blood group antigens compatibility.
 ABO antigens are expressed almost by any cell in the allograft, and if
 the transplantation to be carried out in ABO-incompatible donor and recipient
 it would result in a hyperacute antibody-mediated rejection.
\end_layout

\begin_layout Standard
Donors with blood group O are so called
\begin_inset Quotes sld
\end_inset

universal donors
\begin_inset Quotes srd
\end_inset

.
 Organs from them can be safely transplanted to recipients with any ABO
 blood group.
 Whereas, recipient with AB group can safely receive organ from recipient
 with any ABO blood group and is called a
\begin_inset Quotes sld
\end_inset

universal recipient
\begin_inset Quotes srd
\end_inset

.
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset


\end_layout

\begin_layout Subsubsection*
HLA
\end_layout

\begin_layout Standard
Histocompatibility antigens are genetically encoded antigens that cover
 cell surfaces.
 They differ between individuals of the same species and therefore trigger
 an immune response in case of allograft.
 In all vertebraes histocompatibility antigens are divided into single major
 histocompatibility complex (MHC) and numerous minor histocompatibility
 (miH) systems.
 In case of either MHC or miH incompatibility the result is an immune response
 to the graft, more severe in case of MHC than miH.
 Rejection in MHC-compatible donor-recipient pair is usually delayed, in
 some cases forever.
 Although, sometimes miH mismatch might be so severe that it would be comparable
 to full MHC mismatch.

\end_layout

\begin_layout Standard
MHC antigens are proteins that cover cell surfaces to help the immune system
 to recognize self vs.
 non-self.
 Major histocompatibility comples is divided into MHC class I and MHC class
 II.
 MHC class I cover surfaces of most cells and are liable for activation
 of cytotoxic CD8 cells, that help to find and destroy infected cells.
 MHC class II are found on certain immune cells and play crucial role in
 immune response coordination.
 In humans MHC class 1 are divided into three subgroups each, as can be
 seen on table
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
MHC class division
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class I
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MHC class II
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DR
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DP
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
HLA-DQ
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In clinical practice, clinicians asses and try to match donors and recipient
 according to the number of HLA-A, -B, and -DR mismatches, ranging from
 zero mismatches (0-0-0) to a maximum of 6 mismatches (2-2-2).
 Generally more emphasis is placed on DR loci due to capability of CD4 T
 cell activation, which might trigger both humoral and cellular adaptive
 immune responses.
\end_layout

\begin_layout Standard
Minor histocompatibility proteins can act as antigens, although weaker than
 MHC.
 However if prior sensitisation exists it could result in severe immune
 response that might result in graft loss.

\end_layout

\begin_layout Subsection
T Cell-mediated rejection
\end_layout

\begin_layout Standard
T cell-mediated rejection or TCMR is the most common type of allograft rejection
, as it still happens in 20% of transplantations mostly within first 6 months
 posttrasplant.
 Immune system cells migrate through vessels to the graft, become activated
 and start to attack the organ.
 Complement may also play role in it.
\end_layout

\begin_layout Subsection
B Cell-mediated rejection
\end_layout

\begin_layout Standard
B cells are immune system cells that produce antibodies.
 Alloantibodies are antibodies that react to donor-specific HLA antigens
 and might cause hyperacute rejection, acute antibody-mediated rejection
 (ABMR), and chronic ABMR.
 About 30% of patients have sensitivities and have certain HLA antibodies.
 It might cease transplantation or require antibody suppression strategy.
 Even low amount of antibodies below crossmatch cutoff doubles the risk
 of ABMR and increases the risk of graft failure by 76%.
 Additionally, donor specific antibodies might develop posttransplant and
 cause an actue ABMR.

\end_layout

\begin_layout Standard
Acute AMBR is rarely seen in patients withour prior sensitization and is
 highly difficult to treat.
 AMBR is characterized by decline in allograft function, presense of DSA
 and signs of acute vascular injury.
 A progressive reduction in graft function over time is observed almost
 universally.
\end_layout

\begin_layout Subsection
Transplant tolerance
\end_layout

\begin_layout Standard
Taking into account the detrimental effect of long-term immunosuppression
 one of the primary objectives in transplantation is the induction of immunologi
c non-responsiveness (tolerance) to an allograft.
 The are a couple a pathways of immune non-responsivenes generation described
 in literature, however it haven't gone further animal models yet.
\end_layout

\begin_layout Subsection
Factors Influencing rejection beyond the graft - microbiome
\end_layout

\begin_layout Standard
Human body is very complex system where every subsystem influences other
 subsystems and the whole system in general.
 It is clear that gut microbiome has a profound influence on immune system.
 It is possible microflora on the allograft might cause rejection.
 Immunosupression, profilactory antibbiotics, diet changes and other restriction
s asossiated with organ transplantation result in decrease in gut microbiome
 diversity that result in systematic inflammation, that might contribute
 to alloimunnity, as well as autoimunnity.
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Chapter
Machine Learning Background
\end_layout

\begin_layout Standard
Machine learning is a subfield of computer science that consists of building
 algorithms that are capable of processing large amounts of data, finding
 patterns and making some actions such as predictions or even generating
 new data.
 Machine learning is an intersection of many fields of science such as statistic
s, theory of probability, linear algebra, calculus and, of cource, computer
 science.
\end_layout

\begin_layout Standard
Machine learning excels in problems that are either overly complex or have
 no known algorithm.
\begin_inset CommandInset citation
LatexCommand cite
key "key-9"
literal "false"

\end_inset

 It can help us learn.
 We can extract previously unknown correclations from the data and create
 knowledge.
 It might have less error in decision making than humans.
\end_layout

\begin_layout Standard
Based on the problem and, therefore, on our approach to building a dataset(t
\series bold
ell somewhere the difference between the dataset and raw data
\series default
) and the model, machine learning can be divided into four sub-fields: supervise
d, semi-supervised, unsupervised and reinforcement learning.
 Supervised learning means that data are labeled and we want to predict
 labels from the unlabeled data.
 (
\series bold
explain labeled data
\series default
!) Unsupervised learning deals with unlabeled data.
\end_layout

\begin_layout Standard

\series bold
Semi-supervised learning:
\series default
 means that only a part of our data is labeled.

\end_layout

\begin_layout Standard
In
\series bold
reinforcement learning:
\series default
 we create an environment, set up rewards for doing certain things and punishmen
t for other things, and let the machine (actor) to perform actions that
 produce the highest reward.
 A separate subsection is dedicated to each subfield.
\end_layout

\begin_layout Standard
Every field suffers from human errors, and medicine is no exception.
 (people get tired, wrong institutions, etc...) According to Makary et.
 al., medical error is the third death cause in the United States // hele to je odvazne, v tom clanku to sice nejak je, ale ja bych se takto ostremu a kontroverznimu tvrzeni vyhnul
\begin_inset CommandInset citation
LatexCommand cite
key "key-11"
literal "false"

\end_inset

.
 Machine learning also makes mistakes, but if we manage to get at least
 1% less error than human error this will be a success.
 Human body is a complex system, where other humans (doctors) can't possibly
 comprehend everything that is going on and how it is related to each other.
 Also it can help us gain insights from data that were already accumulated
 and possibly make some discoveries.
\end_layout

\begin_layout Standard
It is important to note that machine learning is rather marketing term.
 Machines can't learn.
 If we were to at least slightly distort inputs, machine wouldn't be able
 to
\begin_inset Quotes sld
\end_inset

learn
\begin_inset Quotes srd
\end_inset

.
 This is very different from learning in humans and animals.
\begin_inset CommandInset citation
LatexCommand cite
key "key-10"
literal "false"

\end_inset

 It is debatable that statistics and mathematics can really produce consciousnes
s.
 Luckily the debate lies beyond this paper. // tenhle odstavec je take dost odvazny...
\end_layout

\begin_layout Standard
In this chapter we will cover all theoretical background that might prove
 useful for solving our problem, this includes classical machine learning,
 deep learning, statistical survival analysis, basic steps that are required
 to create machine learining systems and how to preprocess data.
 We will begin by exploring what is supervised learning.
\end_layout

\begin_layout Section
Supervised Learning
\end_layout

\begin_layout Standard
Supervised learning is the process of training a model on data where the
 outcome is known to make predictions for data where the outcome is not
 known
\begin_inset CommandInset citation
LatexCommand cite
key "key-12"
literal "false"

\end_inset

.

\emph on
Classification
\emph default
 and
\emph on
regression
\emph default
 are common supervised learning tasks.
 In this section we will define these problems, necessary terminology and
 describe commonly used algorithms that are used to solve these types of
 problems.
\end_layout

\begin_layout Standard
In supervised learning the
\emph on
dataset
\emph default
 is the collection of labeled examples
\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, where each individual
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is called a
\emph on
feature vector
\emph default
.
 A feature vector is a vector that in each its dimension
\begin_inset Formula $j=1,...,D$
\end_inset

 contains a value that describes an example in some way.
 This value is called a
\emph on
feature
\emph default
 and is denoted as
\begin_inset Formula $x^{(j)}$
\end_inset

.
 The
\emph on
label
\emph default

\begin_inset Formula $y^{i}$ // tohle je divne, v predchozim odstavci je $y_{i}$, tady $y^{i}$
\end_inset

 might be either a finite set of classes
\begin_inset Formula $\{1,2,...,C\}$
\end_inset

, in case of classification task, or a real number, a vector, a matrix or
 graph, in case of a regression.
 The goal of supervised learning algorithm is to create a model using the
 dataset that will take the feature vector as input and produce label or
 more complex structre as a output.
\end_layout

\begin_layout Standard
Classification is a problem of assigning a label to unlabeled example.
 This problem is solved by a classification learning algorithm that takes
 a labeled set of examples as an input and produces a model that takes unlabeled
 example as input and outputs a label, a number associated with it or a
 probability of belonging to a certain class out of which it is easy to
 deduce the class.
 If the set of labels has only two classes we talk about
\emph on
binary classification
\emph default
.
 Consequentlhy, if the set of labels has three or more classes it is a
\emph on
multiclass classification
\emph default
.
 Some algorithms are binary classifiers by definition, while others are
 multiclass classifiers.
 It is possible to create an
\emph on
ensemble
\emph default
 out of binary classifiers that will be able to perform multiclass classificatio
n.
 Ensemble is a combination of algorithms that are connected together to
 perform one task.
 More on that in subsection 3.1.5 talking about random forests.
\end_layout

\begin_layout Standard
Regression is a problem of predicting a
\emph on
target value
\emph default
 given an unlabeled example.
 The problem is solved by regression learning algorithm that takes a set
 of labeled examples as inputs and produces a model that takes unlabeled
 example as input and outputs a target value.

\end_layout

\begin_layout Standard
In the following subsections we are going to explore some techniques for
 supervised learning.
 Classification and regression tasks are similar in many ways and often
 for each classifier there is an equivalent regressor, and vice versa.
 More on that in the following sections.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
mean square error (MSE)
\end_layout

\begin_layout Standard
confusion matrix
\end_layout

\begin_layout Standard
precision/recall
\end_layout

\begin_layout Standard
area under the ROC curve
\end_layout

\begin_layout Subsection
Linear Regression
\end_layout

\begin_layout Standard
Linear regression is a popular regression learning algorithm.
 The model it produces is a linear combination of all features.
 (
\series bold
show what is a linear combinarion)
\end_layout

\begin_layout Standard
The problem we are trying to solve is formulated as follows:
\series bold

\series default
Given a collection of labeled examples
\series bold

\series default

\begin_inset Formula $\{(\overline{x}_{i},y_{i})\}_{i=1}^{N}$
\end_inset

, create a model
\begin_inset Formula
\begin{equation}
f_{\bar{w},b}(\overline{x})=\overline{w}\overline{x}+b\label{eq:lin_reg}
\end{equation}

\end_inset

.
 Where N is the size of the collection,
\begin_inset Formula $\overline{x}_{i}$
\end_inset

 is a
\emph on
feature vector
\emph default
of D dimensions of example
\begin_inset Formula $i=1,…,N$
\end_inset

, every feature
\begin_inset Formula $x_{i}^{(J)}\epsilon\mathbb{R},$
\end_inset


\begin_inset Formula $y_{i}\epsilon\mathbb{R}$
\end_inset

 is target value.

\begin_inset Formula $\overline{w}$
\end_inset

 is D-dimensional vector of parameters and
\begin_inset Formula $b\epsilon\mathbb{R}$
\end_inset

.
 Notation
\begin_inset Formula $f_{\bar{w},b}(\overline{x})$
\end_inset

 means that
\begin_inset Formula $f$
\end_inset

 is parametrized by
\begin_inset Formula $\overline{w}$
\end_inset

 and
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
To train the linear regression means to find optimal values
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters
\begin_inset Formula $\overline{w}$
\end_inset

 and
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 In graphical terms it means finding such a hyperplane that fits datapoints
 from the training set as good, as possible, as it can be seen on the image

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{lin_reg_img}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.6
\backslash
textwidth]{Images/linear_regression}
\end_layout

\begin_layout Plain Layout


\backslash
caption{Linear regression for two-dimensional data}
\end_layout

\begin_layout Plain Layout


\backslash
label{lin_reg_img}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
To find optimal parameters we need to minimize the following expression:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
\frac{1}{N}\mathop{\underset{i=1...N}{\sum}(}f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}.\label{eq:mse}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
It iscalled
\emph on
mean squared error (MSE),
\emph default
the
\emph on
loss function
\emph default
that comprises of
\emph on
squared error loss
\emph default

\begin_inset Formula $(f_{\bar{w},b}(\overline{x_{i}})-y_{i})^{2}$
\end_inset

, another loss function
\emph on

\emph default
that
\emph on

\emph default
evaluates individual predictions
\emph on
.

\emph default
Loss function is a measure of how well a model performs overall (MSE) or
 for any individual prediction (square error loss).

\emph on

\end_layout

\begin_layout Standard
There is a
\emph on
closed-form solution
\emph default
 for finding the optimal values
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

.
 Closed-form solution means simple algrebraic expression that gives the
 result directly.
 In case of linear regression it is the
\emph on
normal equation
\emph default
, and it looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
\mathbf{\overline{\mathbf{w}}^{*}=(x^{T}x)x^{T}y}.\label{eq:norm_eq}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where
\begin_inset Formula $x^{T}$
\end_inset

means transposed feature vector
\begin_inset Formula $x$
\end_inset

.
\end_layout

\begin_layout Standard
We could select other loss function, but according to Andriy Burkov it would
 be a different algorithm.
 For example we could take the absolute difference of
\begin_inset Formula $f(x_{i})$
\end_inset

 and
\begin_inset Formula $y_{i}$
\end_inset

.
 But that would create us problems as the derivative of absolute value is
 not continuous, and therefore the function is not smooth.
 Non-smooth functions create unnecessary complications during the optimisation
 process.

\end_layout

\begin_layout Standard
Linear models are usually resilient to overfitting because they are simple.
 Overfitting is a situation when the model learns the intricacies of the
 training dataset so well that it remembers actual values rather than learn
 underlying pattern.
 Overfit model is unable to make accurate predictions when confronted with
 the unseen data.
 More on overfitting in section 3.6.
\end_layout

\begin_layout Subsection
Logistic Regression
\end_layout

\begin_layout Standard

\emph on
Logistic regression
\emph default
 is a binary classifier that estimates the probability of an example belonging
 to a particular class.
 If the predicted probability of example belonging to a certain class is
 greater than 50% then the model concludes that it does belong to the class
 (referred to as positive class and is labeled as
\begin_inset Formula $1$
\end_inset

), otherwise it predicts that example does not belong to that class (but
 belongs to the negative class, labeled
\begin_inset Formula $0$
\end_inset

).
 Logistic regression comes from statistics, where its mathematical formulation
 is indeed similar to regression.
 Multiclass classification is implemented in softmax regression, multiclass
 vartiant of logistic regression.
\end_layout

\begin_layout Standard
As with the linear regression, in logistic regression we want to model
\begin_inset Formula $y$
\end_inset

 as a linear combination of
\begin_inset Formula $\overline{x}$
\end_inset

, but in this case it is not that straightforward.
 (
\series bold
think how to make this paragraph better)
\end_layout

\begin_layout Standard
The logistic regression model looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
f_{\bar{w},b}(\overline{x})\stackrel{def}{=}\frac{1}{1+e^{-(wx+b)}}.\label{eq:log_reg}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Just like with the linear regression, our task is to find optimal values

\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for parameters
\begin_inset Formula $\overline{w}$
\end_inset

 and
\begin_inset Formula $b$
\end_inset

.
\end_layout

\begin_layout Standard
Once we found
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 for the
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

, in other words we trained the model, we can apply the model
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log_reg"
plural "false"
caps "false"
noprefix "false"

\end_inset

 on features
\begin_inset Formula $x_{i}$
\end_inset

 from an example
\begin_inset Formula $(x_{i},y_{i})$
\end_inset

.
 The output value lies in range
\begin_inset Formula $0<p<1$
\end_inset

 .
 If
\begin_inset Formula $y_{i}$
\end_inset

 is a positive class
\series bold

\series default
the probability of being a positive class is given by
\begin_inset Formula $p$
\end_inset

, if it is negative, the probability is given by
\begin_inset Formula $1-p$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.6
\backslash
textwidth]{Images/logistic_regression}
\end_layout

\begin_layout Plain Layout


\backslash
caption{Logistic function}
\end_layout

\begin_layout Plain Layout


\backslash
label{log_reg_img}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
By looking at the graph we can see that indeed if the
\begin_inset Formula $y$
\end_inset

 has the value less than
\begin_inset Formula $\frac{1}{2}$
\end_inset

 it has negative
\begin_inset Formula $x$
\end_inset

 values and will be marked as negative, and positive if it is larger than

\begin_inset Formula $\frac{1}{2}$
\end_inset

 .
 Although the threshold may differ, depending on the context.
\end_layout

\begin_layout Standard
Instead of minimizing MSE, in logistic regression we are trying to
\emph on
maximize
\emph default
 the
\emph on
likelihood function
\emph default
.
 In statistics likelihood function tells how likely the example is according
 to our model.
 The objective function in logistic regression is called
\emph on
maximum likelihood.

\emph default
It looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
L_{\bar{w},b}\stackrel{def}{=}\underset{i=1...N}{\prod}f_{\bar{w},b}(\overline{x}_{i})^{y_{i}}(1-f_{\bar{w},b}(\overline{x}_{i}))^{(1-y_{i})}.\label{eq:max_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Because of the exponential function in
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:max_likelihood"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for easier calculations it is better to use the
\emph on
log-likelihood
\emph default
instead.
 As
\begin_inset Formula $Log$
\end_inset

 is strictly increasing function maximizing it is the same as maximizing
 its argument.
 The solution to this optimization problem is the same as the solution to
 the original problem.

\emph on
Log-likelihood
\emph default
function looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
LogL_{\bar{w},b}\stackrel{def}{=}ln(L_{\overline{w},b}(\overline{x}))\stackrel[i=1]{N}{\sum}y_{i}lnf_{\bar{w},b}(\overline{x})+(1-y_{i})ln(1-f_{\bar{w},b}(\overline{x})).\label{eq:log_likelihood}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Unfortunately, there is no closed-form solution for this optimisation problem.
 But the function is convex, so
\emph on
gradient descent
\emph default
 (or any onther optimisation algorithm) more or less guarantees the finding
 of the global minimum, provided not too large learning rate and given enough
 time.
 More on gradient descent in section ...
 dedicated to deep learning.
\end_layout

\begin_layout Subsection
Support Vector Machines
\end_layout

\begin_layout Standard

\series bold
Description:
\series default
\emph on
Support vector machine (SVM)
\emph default
 is a very popular a and powerful machine learning algorithm that can perform
 wide range of tasks including linear and nonlinear classification, regression
 and outlier detection on small- to medium-sized datasets.
 (
\series bold
positive class +1, negative -1)
\end_layout

\begin_layout Paragraph*

\series bold
LinearSVM
\end_layout

\begin_layout Standard
The concept behind support vector machines is demonstrated on Figure
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{svm1_img}
\end_layout

\end_inset

.
 The image is comprised of two classes represented by the red and blue dots,
 divided by solid line termed the
\emph on
dicision boundary
\emph default

\begin_inset Formula $\overline{w}\overline{x}-b=0$
\end_inset

 with two dashed lines by its sides known as
\emph on
support vectors
\emph default

\begin_inset Formula $\overline{w}\overline{x}-b=1$
\end_inset

 and
\begin_inset Formula $\overline{w}\overline{x}-b=-1$
\end_inset

.
 Support vectors are determined by closest instances of a class to the decision
 boundary, which can be seen in the figure.

\end_layout

\begin_layout Standard
The distance between the closest instances of two classes is called
\emph on
margin
\emph default
and is equal to
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, where
\begin_inset Formula $||\overline{w}||$
\end_inset

 is the Equclidian norm and
\begin_inset Formula $\overline{w}$
\end_inset

 is a parameter vector of the same dimensionality as the feature vector.
 So the smaller the norm, the larger the margin is.
 The larger the margin, the better model generalizes.
 The primary objective of the model is to find the largest margin
\begin_inset Formula $\frac{2}{||\overline{w}||}$
\end_inset

, so, to do that we need to
\emph on
minimize
\emph default
the Euclidian norm defined by the expression
\end_layout

\begin_layout Standard
\begin_inset Formula
\[
||\overline{w}||=\sqrt{\stackrel[j=1]{D}{\sum}(w^{(j)})^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
The fundamental assumption is that classes are
\emph on
linearly separable
\emph default
, implying they can be separated by a hyperplane (decision boundary) with
 no instances of one class lying among instances of the opposite class.
 It is illustrated on the figure
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{svm2_img}
\end_layout

\end_inset

.
 In this case the algorithm won't be able to find optimal solution with
 no instances lying between the support vectors and the decision boundary.
 (Consequentlt, the model is highly sensitive to outliers.
 This type of SVM is called
\emph on
hard margin classification.
)
\end_layout

\begin_layout Standard
Now let's look at this from the mathematical point of view.
 As was mentioned before, the decision boundary is defined by the
following
 expression
:
\begin_inset Formula $\overline{w}\overline{x}-b=0$
\end_inset

, where
\begin_inset Formula $\overline{w}\overline{x}$
\end_inset

 is a
\emph on
linear combination
\emph default
 that looks like this:
\begin_inset Formula $w_{1}x_{1}+w_{2}x_{2}+....+w_{D}x_{D}$
\end_inset

, where D is the number of dimensions.
 The supporting vectors are defined by the following equations
\begin_inset Formula $\overline{w}\overline{x}-b=1$
\end_inset

, and
\begin_inset Formula $\overline{w}\overline{x}-b=-1$
\end_inset

, these two equations can be reduced to one
\begin_inset Formula $y_{i}(\overline{w}\overline{x}-b)\geqslant1$
\end_inset

 (as
\begin_inset Formula $y_{i}=(1or-1)$
\end_inset

).
\end_layout

\begin_layout Standard
The model is described by the equation
\begin_inset Formula
\[
f(x)=sign(\overline{w}\overline{x}-b).
\]

\end_inset


\end_layout

\begin_layout Standard
Sign returns +1 if the input is positive, and -1 if it is negative, that
 corresponds to the positive and negative classes defined earlier.
\end_layout

\begin_layout Standard

\series bold
What is margin:
\series default
Margin is the distance between the closest instances of two classes.
 Large margin contributes to a better generalization.
 Margin is equal to
\begin_inset Formula $\frac{2}{||\overline{w}|}$
\end_inset

.
 So the smaller is the Euklidian norm
\begin_inset Formula $||\overline{w}||$
\end_inset

, the larger the margin is, the better is generalization.
\end_layout

\begin_layout Standard

\series bold
What we are minimizing:
\series default

\begin_inset Formula $||\overline{w}||=\sqrt{\stackrel[j=1]{D}{\sum}(w^{(j)})^{2}}$
\end_inset

 - this is called
\emph on
Euclidian norm
\emph default
.
\end_layout

\begin_layout Standard

\series bold
Optimisation Constraints:
\end_layout

\begin_layout Enumerate
\begin_inset Formula $\overline{w}\overline{x_{i}}-b\geqslant1$
\end_inset

 if
\begin_inset Formula $y_{i}=+1$
\end_inset


\begin_inset Formula $\overline{w}\overline{x_{i}}-b\leqslant-1$
\end_inset

 if
\begin_inset Formula $y_{i}=-1$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Objective:
\series default
Minimize
\begin_inset Formula $||\overline{w}||$
\end_inset

 subject to constraint
\begin_inset Formula $y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1$
\end_inset

 for
\begin_inset Formula $i=1,...,N$
\end_inset

, where N is the number of features.
 Minimizing
\begin_inset Formula $||\overline{w}||$
\end_inset

 is equal to minimizing
\begin_inset Formula $\frac{1}{2}||\overline{w}||^{2}$
\end_inset

 so we can use quadratic programming optimisation.
 So the resulting optimistaion problem looks like this:
\end_layout

\begin_layout Standard
\begin_inset Formula
\[
min\frac{1}{2}||\overline{w}||^{2}\text{such that }y_{i}(\overline{w}\overline{x_{i}}-b)\geqslant1,i=1,...,N
\]

\end_inset


\end_layout

\begin_layout Standard
To train the SVM means to find optimal values
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 of parameters
\begin_inset Formula $\overline{w}$
\end_inset

 and
\begin_inset Formula $b$
\end_inset

 so that the model makes as accurate predictions as possible.
 The process of finding
\begin_inset Formula $(\overline{w}^{*},b^{*})$
\end_inset

 is called training.
\end_layout

\begin_layout Standard

\series bold
Dealing with noise:
\series default
 To introduce the ability of SVM to handle nonlinearly separable data we
 define the
\emph on
hinge loss
\emph default
function:
\begin_inset Formula $max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)).$
\end_inset

 It is zero if the constraits 1 and 2 are satisfied.
 If it is not zero it means that the datapoint does not lie on the right
 side of the decision boundary.
 The function value is proportional to the distance from the decision boundary.
 The resulting cost function looks like the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
C||\overline{w}||^{2}+\frac{1}{N}\stackrel[j=1]{N}{\sum}max(0,1-y_{i}(\overline{w}\overline{x_{i}}-b)),\label{eq:svm_loss}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where C is the hyperparameter that determines the tradeoff between increasing
 the size of the decision boundary and ensuring that each
\begin_inset Formula $x_{i}$
\end_inset

 lies on the correct side of the decision boundary.
 Its value is chosen experimentally.
 C handles the tradeoff between classifying the training data well and classifyi
ng future examples well (generalization).
 For higher values of C the misclassification error will be almost negligible
 so the algorithm will try to find the highest margin without considering
 it.
 For lower values of C, the algorithm will try to make fewer mistakes by
 sacrificing the margin size.
 The larger margin is better for the generalization.
\end_layout

\begin_layout Standard
SVM with the hinge loss function is called
\emph on
softmargin SVM
\emph default
, while original formulation that optimizes the Euclidian norm is referred
 to as
\emph on
hard-margin SVM.
\end_layout

\begin_layout Standard

\series bold
Soft margin classification:
\series default

\emph on
Soft margin classification
\emph default
tries to mitigate the downsides of the hard margin classification by trying
 to find a balance between keeping the
\begin_inset Quotes sld
\end_inset

road
\begin_inset Quotes srd
\end_inset

 as large as possible and mitigating the margin outliers (instances that
 lie on the
\begin_inset Quotes sld
\end_inset

road
\begin_inset Quotes srd
\end_inset

 or on the opposite side).
 In Scikit-Learn's SVM this balance is controlled by the
\begin_inset Formula $C$
\end_inset

 hyperparameter.
 Smaller values lead to wider street and more margin violations, larger
 values lead to narrower street and less margin violations.
\end_layout

\begin_layout Standard

\series bold
Kernels and dealing with nonlinearity:
\series default
 We can adapt SVM to work with nonlinearly separable datasets by applying
 the kernel trick.
 Kernel trick means transfroming the original space to a higher dimensional
 one during the cost function optimisation with the hope that in higher
 dimensional space it will become linearly separable.
 In mathematical language: kernel trick is mapping
\begin_inset Formula $\varphi:\overline{x}\rightarrow\varphi(\overline{x})$
\end_inset

, where
\begin_inset Formula $\varphi(\overline{x})$
\end_inset

 is vector of higher dimensionality than
\begin_inset Formula $\overline{x}.$
\end_inset

 The kernel trick allows us to save a lot of non-necessary computations.
\end_layout

\begin_layout Standard
There are multiple kernel functions.
 The most videly used are linear, polynomial, radial basis function (RBF),
\end_layout

\begin_layout Standard

\series bold
SVM regression
\series default
:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.6
\backslash
textwidth]{Images/svm1}
\end_layout

\begin_layout Plain Layout


\backslash
caption{SVM demonstration for two-dimensional dataset}
\end_layout

\begin_layout Plain Layout


\backslash
label{svm1_img}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.6
\backslash
textwidth]{Images/svm2}
\end_layout

\begin_layout Plain Layout


\backslash
caption{Linearly inseparable dataset}
\end_layout

\begin_layout Plain Layout


\backslash
label{svm2_img}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Decision Trees and Random Forests
\end_layout

\begin_layout Standard
highly interpretable model
\end_layout

\begin_layout Standard
builds trees that can be visualized and show the model's reasoning
\end_layout

\begin_layout Standard
The concept of an ensemble:
\end_layout

\begin_layout Standard
Stacking:
\end_layout

\begin_layout Standard
(some other technique)
\end_layout

\begin_layout Standard
Base learner of the random forest:
\end_layout

\begin_layout Standard
How it all works together:
\end_layout

\begin_layout Standard
Benefits: potentially interpretable model.
\end_layout

\begin_layout Section
Unsupervised Learning
\end_layout

\begin_layout Standard

\emph on
Unsupervised learning
\emph default
 operates with the dataset without lables.
 It proves useful during
\emph on
exploratory data analysis (EDA)
\emph default
 and
\emph on
dimensionality reduction
\emph default
, but otherwise it is not very useful in practical applications, as unlabeled
 dataset means no feedback about how well your model performs. // no to neni taky uplne pravda, uzitecne to je a tohle sem nepis. Annomaly detection na tom stoji komplet

\end_layout

\begin_layout Standard
We will cover this part very briefly, as it is not closely related to our
// uz se pouziva I will cover... A tohle je taky trochu nestastne receno. Kdyz to neni dulezite, tak proc o tom vubec neco psat? Spis zdurazni ty veci, ktere treba pouzit chces
 task.
 Albeit something will be used as a data preprocessing step, such as dimensional
ity reduction, or during exploratory data analysis.
\end_layout

\begin_layout Standard
There are more approaches but we are going to cover only three, that might
 prove useful to our task.

\emph on
Clustering
\emph default
 is identifying similar instances and assigning them to groups of similar
 instances - clusters.
 It can be used for data analysis, customer segmentation, as a dimensionality
 reduction technique, anomaly detection.
 Clustering might be
\emph on
soft
\emph default
 and
\emph on
hard.

\emph default
Hard clustering means that an instance might belong to only one class.
 In soft clustering instance has a score of belonging to a particular cluster.
 The score might be the distance from the cluster centroid or an affinity
 (similarity score).

\emph on

\end_layout

\begin_layout Standard

\emph on
Dimensionality reduction
\emph default
 is useful for visualization and for acceleration of learning.
 Datasets often have a lot of redundant data or it might be the case that
 the task requires a lot of features.
 A lot of algorithms, such as linear model, SVMs, decision trees, do not
 handle high dimensional data well.
 So called
\emph on
curse of dimensionality
\emph default
 states that high dimensional data can cause slow learning and prevent us
 from getting an optimal model.
 So the reduction of the data dimensionality might be a good idea.
 Its worth noting though that dimensionality reduction algorithm might loose
 some useful information, while decreasing training time.
 A lot of modern algorithms, such as neural networks or ensemle algorithms,
 handle high dimensional data very well, so dimensionality reduction techniques
 are used less than in the past.
 They are still used however for data visualisation and for cases when we
 need to build an interpretable model and we are limited in number of algorithms
 we can use.
\end_layout

\begin_layout Standard

\emph on
Anomaly
\emph default
 (
\emph on
outlier
\emph default
)
\emph on
detection
\emph default
 involves detection of instances strongly deviating from the norm.
 These instances are called
\emph on
outliers
\emph default
 or
\emph on
anomalies
\emph default
, normal instances are referred to as
\emph on
inliers.

\emph default
It is useful in many applications.
 It can be used as a data preprocessing step - to remove outliers from the
 dataset, which might improve the performance of the resulting model.
 Also, it can be used in
\emph on
fraud detection
\emph default
task and in detection of faulty products in manufacturing facility.

\end_layout

\begin_layout Standard

\emph on
Novelty detectio
\emph default
n is closely related task to the anomaly detection.
 The only thing different about them is that the novelty detection makes
 an assumption that the dataset the model was trained on was not contaminated
 by outliers, while the anomaly detection does not make this assumption.

\end_layout

\begin_layout Subsection
KMeans
\end_layout

\begin_layout Standard

\series bold
Finding optimal numbers of clusters
\series default
:
\end_layout

\begin_layout Standard
Advantages of kmeans
\end_layout

\begin_layout Standard

\series bold
Limitations of KMeans
\series default
:
\end_layout

\begin_layout Subsection
Principal Component Analysis (PCA)
\end_layout

\begin_layout Standard

\emph on
Principal components
\emph default
 are vectors that define new coordinate system.
 First vector goes in the direction of the highest variance.
 Second vector is orthogonal to the first one and goes in the direction
 of the second highest variance, and so on.
 If we were to reduce dimensionality to
\begin_inset Formula $D_{new}<D$
\end_inset

, we would pick
\begin_inset Formula $D_{new}$
\end_inset

 largest principal components and
\emph on
project
\emph default
 instances onto them.

\end_layout

\begin_layout Standard
(Create images)
\end_layout

\begin_layout Standard
It is not adviced to choose the number of dimensions arbitrarily.
 Usually such number of dimensions is chosen that preserves large amount
 of variance (e.g.
 95%), or in case of visualization we reduce number of dimensions down to
 2 or 3.
\end_layout

\begin_layout Standard
There are different versions of PCA; kernel PCA, Incremental PCA (online
 or batch PCA), Randomized PCA.
 But covering those lies beyond this paper.
\end_layout

\begin_layout Subsection
Gaussian Mixtures
\end_layout

\begin_layout Standard
Gaussian mixtures is common algorithm that can be used for anomaly detection.
 Gaussian Mixtures assume that the dataset is generated by several Gaussian
 distributions.
 Any instance lying in the region of low density is an anomaly.
 The density threshold has be specified.
 If one gets too many
\emph on
false positives
\emph default
 (good products labeles as faulty) them need to decrease the the threshold,
 consequently if we get too many
\emph on
false negatives (
\emph default
faulty products labeled as good) the threshold has to be increased.
 Gaussian mixtures belong to
\emph on
soft clustering.
\end_layout

\begin_layout Standard
Gaussian mixtures require the number of clusters to be specified.
 It needs to be run a couple of times to avoid suboptimal solutions.
\end_layout

\begin_layout Section
Data Preparation
\end_layout

\begin_layout Standard
Due to the curse of dimensionality and inherent noise, we cannot load raw
 data to an algorithm and expect it to perform well - most of the times
 it has way too many features, and most of them have very low predictive
 power.
 We need to build a dataset first.
 Feature engineering is responsible for transforming raw data into a dataset.
 It is a labor-demanding process that requires creativity and, most inportantly,
 domain knowledge.
\end_layout

\begin_layout Standard
Our goal in this step is to create
\emph on
informative
\emph default
 features or features with high
\emph on
predictive power
\emph default
.
 For example, in our task of predicting survival time, donor-recipient blood
 group compatibility or recipient age have much higher predictive power
 than donor's or recipient's citizenship.
\end_layout

\begin_layout Standard
We can create new features with higher predictive power out of those with
 low predictive power.
 For example, calculation of eGFR, metric of kidney function estimated on
 patient's age, gender and serum creatinine level, could potentially give
 more information to the learning algorithm, than all those features separately.
\end_layout

\begin_layout Standard
In the following subsections we will cover some popular feature engineering
 techniques.
\end_layout

\begin_layout Subsection

\series bold
Handling Categorical Features
\end_layout

\begin_layout Standard
Most of the machine learning algorithms work with numerical features.
 To handle categorical features, such as age or blood group, we can use

\emph on
one-hot encoding
\emph default
 to convert it to a several binary ones.
 Let's take blood groups as an example.
 There are four main blood groups: A, B, AB and O.
 We can convert single feature into a vector of four numerical values:
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{align*}
\end_layout

\begin_layout Plain Layout

A&=[1,0,0,0]
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

B&=[0,1,0,0]
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

AB&=[0,0,1,0]
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

O&=[0,0,0,1]
\end_layout

\begin_layout Plain Layout


\backslash
end{align*}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
By doing so we increase the dimensionality of our dataset, but this is a
 necessary evil, because if we were to assign a number to each group (1
 to A, 2 to B, and so on) in order to reduce amount of dimensions, that
 would imply that there is some gradation between these values, when there
 is none.
\end_layout

\begin_layout Standard
However, if the categorical feature implies some gradation (e.g.

\begin_inset Quotes sld
\end_inset

bad
\begin_inset Quotes srd
\end_inset

,
\begin_inset Quotes sld
\end_inset

average
\begin_inset Quotes srd
\end_inset

,
\begin_inset Quotes sld
\end_inset

good
\begin_inset Quotes srd
\end_inset

,
\begin_inset Quotes sld
\end_inset

excellent
\begin_inset Quotes srd
\end_inset

) numerating each value would be a right practice.
 Assigning a number to values that have ranking is called
\emph on
ordinal encoding
\emph default
.
\end_layout

\begin_layout Standard

\emph on
Binning
\emph default
 (or
\emph on
bucketing
\emph default
) is the process of converting numerical values into multiple binary features,
 called
\emph on
bins
\emph default
 or
\emph on
buckets
\emph default
.
 For example, we could transform patient's age into age range bins: 0 to
 18 years old, 18 to 25 y.o., 25 to 40 years old, and so on.
 This might help an algorithm to learn better on smaller datasets.

\end_layout

\begin_layout Subsection

\series bold
Feature Scaling
\end_layout

\begin_layout Standard
Different ranges of feature values might pose a problem to some machine
 learning algorithms, as they don't handle them very well.
 It might result in slowed training time or worse performance.
 This problem is solved by
\emph on
normalization
\emph default
 and
\emph on
standartization
\emph default
 techniques.
\end_layout

\begin_layout Standard

\emph on
Normalization
\emph default
 (also known as
\emph on
min-max scaling
\emph default
) is a technique of converting an actual range of numerical feature values
 into a standard range of values:
\begin_inset Formula $[-1,1]$
\end_inset

 or
\begin_inset Formula $[0,1]$
\end_inset

 without loosing any information.
 The normalization formula for value
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature
\begin_inset Formula $j$
\end_inset

, looks like following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\[
\bar{x}^{(j)}=\frac{x^{(j)}-min(j)}{max(j)-min(j)},
\]

\end_inset

where
\begin_inset Formula $min(j)$
\end_inset

 and
\begin_inset Formula $max(j)$
\end_inset

 are minimal and maximal values of feature
\begin_inset Formula $j$
\end_inset

.
\end_layout

\begin_layout Standard

\emph on
Standartization
\emph default
 is a scaling technique that scales numerical data in such a way that after
 scaling it has properties of
\emph on
standard normal distribution
\emph default
 with the mean µ (average value) equal to zero and the standard deviation
 from the mean
\begin_inset Formula $\sigma$
\end_inset

 equal to 1.
 The standartization formula for value
\begin_inset Formula $x^{(j)}$
\end_inset

 for feature
\begin_inset Formula $j$
\end_inset

, looks like following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\[
\hat{x}^{(j)}=\frac{x^{(j)}-\mu^{(j)}}{\sigma^{(j)}}.
\]

\end_inset


\end_layout

\begin_layout Standard
Typically, standartization is used for supervised learning (it works better
 with standartization), in case, feature values are formed by standard distribut
ion (bell-curve) or feature has outliers.
 In other cases normalization is preferred.
\end_layout

\begin_layout Subsection

\series bold
Handling missing feature values
\end_layout

\begin_layout Standard
Often datasets have missing values and to handle them we have one of the
 following options:
\end_layout

\begin_layout Itemize
Dropping rows with missing values.
 The most obvious and bruteforce way of dealing with them.
 If there are not so much missing values, or we have a dataset that is large
 enough, the usage of this technique would be appropriate.
\end_layout

\begin_layout Itemize
Dropping a feature.
 If there are way too many missing values compared to the size of the dataset,
 it is better to remove the feature altogether.
\end_layout

\begin_layout Itemize
Regression Imputation.
 Imputation means filling in the missing value.
 Using machine learning regression algorithm to predict missing feature
 values.
\end_layout

\begin_layout Itemize
Mean/median impitation.
 Filling missing values with mean or median.
\end_layout

\begin_layout Itemize
Constant value imputation.
 Filling missing values with clearly too high or too low value for the algorithm
 to understand that it is an outlier and this particular value should not
 be considered, while other features might be.
 This method is not recommended, as it can introduce bias.
\end_layout

\begin_layout Standard
It is often impossible to tell which data imputation method would work the
 best and it should be checked experimentally.
\end_layout

\begin_layout Section
Model Training and Hyperparameter Tuning
\end_layout

\begin_layout Standard
It is a common practice to divide the data set into three parts
\end_layout

\begin_layout Itemize
Training set (about 70% of the whole dataset)
\end_layout

\begin_layout Itemize
Validation set (15% of the whole dataset)
\end_layout

\begin_layout Itemize
Test set (15% of the whole dataset)
\end_layout

\begin_layout Standard
Training set is the largest of them and is used to train the machine learning
 model.
 Validation and test sets are of the same size and are called
\emph on
hold-out sets.

\emph default
They are used in subsequent stages of model evaluation.

\end_layout

\begin_layout Standard
The reason behind the use of separate training and validation sets is to
 prevent overfitting - situation when the model performs well on training
 data but poorly on unseen data.
 Overfitting might occur if the model is tested and evaluated on the same
 dataset, and as a result it memorizes the training examples and fails to
 make accurate predictions on unseen data.
 To mitigate this we use the validation set to fine-tune the model and the
 test set to asess its performance before deploying it to production.
\end_layout

\begin_layout Standard

\series bold
Underfitting:
\end_layout

\begin_layout Standard
The workflow is following: we train the model on the training set, then
 validate it on the validation set using the chosen metric, then adjust
 the model's parameters so that it perfoms better.
 This process is repeated until the significant improvement is no longer
 observed.
 Lastly, we asses model's perfomance on the test set.
 This iterative process is called hyperparameter tuning.
\end_layout

\begin_layout Standard
An alternative to the three sets technique is
\emph on
k-fold cross validation
\emph default
.
 It works as follows: the dataset is divided into
\begin_inset Formula $k$
\end_inset

 subsets, or
\emph on
folds,
\emph default
of equal size.
 One fold is put aside as a validation set while other
\begin_inset Formula $k-1$
\end_inset

 are used as a training set.
 The model is trained exactly
\emph on
k
\emph default
 times with each fold being used as a validation set only once.
 The only downside is that it is highly computationally expensive with high

\emph on
k
\emph default
 value and larger datasets as the model will be trained multiple times.
\end_layout

\begin_layout Standard
A
\emph on
hyperparameter
\emph default
 is a parameter that is set up before model training (regular model parameters
 are calculated during training).
 Each model has a different set of hyperparameters and they profoundly influence
 model's performance.
 (
\series bold
Add hyperparameter examples from already covered models
\series default
) The process of finding optimal combination of hyperparameters is called

\emph on
hyperparameter tuning
\emph default
.
 One solution would be manually select hyperparameters and see how they
 influence performance.
 But there is a better way - grid search.

\end_layout

\begin_layout Standard

\emph on
Grid search
\emph default
 is a traditional way of perfroming the hyperparameter fine-tuning.
 We define hyperparameters we want to experiment with, provide values we
 want to try and the algorithm will train a model for each possible combination
 of hyperparameters, asses their performance using k-fold cross validation
 and return the best combination of hyperparameters (at least it works that
 way in scikit-learn's implementation GridSearchCV).
\end_layout

\begin_layout Standard
Grid search is great when we try out relatively few hyperparameter combinations,
 but when there are a lot of values it is preferrable to use RandomizedSearch
 (RandomizedSearchCV in scikit-learn) instead.
 It works more or less the same way as grid search, does but instead of
 trying every possible combination from provided values it tries only given
 number of hyperparameter combinations selected randomly.
 Main reason it is better than the grid search is that it gives you more
 control over how much computational power and time you want to spend on
 the hyperparameter tuning.
\end_layout

\begin_layout Section
Survival Analysis
\end_layout

\begin_layout Standard

\series bold
Definition
\series default
:
\emph on
Survival analysis,
\emph default
 also known as
\emph on
time-to-event analysis
\emph default
, is a statistical method used to analyze the time until an event of interest
 occurs.
 Its name originates from the clinical and biological research, where these
 methods are used mostly to analyze survival time, hence the name.
 These methods, however, found their uses in areas far beyond clinical setting:
 in business to analyze the time until the customer
\begin_inset Quotes sld
\end_inset

churns
\begin_inset Quotes srd
\end_inset

 from a subscription, in engineering, to estimate the longevity of certain
 products, or its parts.
 In social sciences, estimate the longevity of a marriage or a student dropout
 rate in academic setting.
\end_layout

\begin_layout Standard

\series bold
Censoring:
\series default
The most pronounced feature of these methods is the ability to handle censored
 data.
 Censoring is a situation when the information about the survival time is
 known only partially.
 For example, in the dataset used in this work there are
\series bold
(number of patients)
\series default
 that were still alive until last date of observation
\series bold
(date of observation
\series default
).
 And we do not know what happened to them after that date — they are censored.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.6
\backslash
textwidth]{Images/censoring_ill}
\end_layout

\begin_layout Plain Layout


\backslash
caption{caption}
\end_layout

\begin_layout Plain Layout


\backslash
label{fig1}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Look at the figure
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig1}
\end_layout

\end_inset

.
 On the y axis we can see individual patients.
 x axis corresponds to timeline of the study (right side is the end of the
 study.
 Cross (X) denotes an occurance of the event, circle (O) corresponds to
 subject's exit from the study.

\end_layout

\begin_layout Standard

\series bold
Left and right censoring:
\series default
There are two types of censoring: left and right censoring.
 Right censoring, the most common one, happens when we know that event did
 not happen up to a certain point — the patient didn't attend follow-ups
 after a certain time or the study ended when he was still alive.
 Lets look at the figure 3.1
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig1}
\end_layout

\end_inset

, E and F are obviously right censored, as the event didn't happen during
 study and B is also right censored, as the subject dropped out of study.
 Left censoring, being much less common, describes the situation when the
 subject's survival time is unkonown, but we know that it is less than a
 certain time.
 For example, in epidimiology studies we may not know when exactly the patient
 was infected.
 but we know that he is infected.
\end_layout

\begin_layout Standard

\series bold
Censoring assumptions:
\series default
They are very similar, however, they have very subtle differences that we
 are going cover in the following paragraphs.
\end_layout

\begin_layout Standard

\series bold
-Random
\series default
: censored subjects are representative of the remaining subjects provided
 the same survival experience.
\end_layout

\begin_layout Standard

\series bold
-Independent
\series default
: censoring is independent if it is random within any subgroup
\end_layout

\begin_layout Standard
Random an independent censoring assume that those who left the study are
 no different from those who stayed and therefore the percentage of survival
 is the same in both groups with respect to their survival experience.
 They are the same if assumed in one group.
 Censoring is
\emph on
independent
\emph default
 if the censoring percentage in one group differs from the censoring percentage
 in the other group.
 In contrast, if the censoring percentages are the same, the censoring is

\emph on
random
\emph default
.
\end_layout

\begin_layout Standard

\series bold
-Non-informative censoring
\series default
: distribution of survival times T provides no information about the distributio
n of censorship times C.
\end_layout

\begin_layout Standard

\series bold
Relation to machine learning
\series default
: Because we are predicting a numerical value(rather values), from the machine
 learning point of view, survival analysis might be considered a regression,
 but instead of one numerical value we predict a continuous value - survival
 function or hazard function, and, obviously, the dataset is censored.

\emph on
Survival function (also survivor function)
\emph default

\begin_inset Formula $S(t)$
\end_inset

 shows us the probability of patient
\emph on
surviving (event doesn't happen)
\emph default
 at a given time
\begin_inset Formula $t$
\end_inset

 and can be denoted as
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
S(t)=P(T>t).\label{eq:f_surv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where
\begin_inset Formula $t$
\end_inset

 is any specific
\emph on
time
\emph default
 of interest,
\begin_inset Formula $T$
\end_inset

 is random variable for subject's survival time.
 For instance, if we want to know if a patient is going to live for more
 than 5 years after kidney transplant,
\begin_inset Formula $t$
\end_inset

 is equal to 5 and we ask whether
\begin_inset Formula $T$
\end_inset

 is greater than
\begin_inset Formula $t$
\end_inset

 (probability question).
 The function is declining in the range from 0 to infinity.
 As it is a probability, the function value ranges only from 0 to 1.
 Theoretically, the graph of the survival function must be smooth, but in
 reality it is represented by a step function.
\end_layout

\begin_layout Standard
Hazard function
\begin_inset Formula $h(t)$
\end_inset

 tells us the probability of given event
\emph on
happening
\emph default
 at a given point of time
\begin_inset Formula $t,$
\end_inset

 provided the event did not happen before time
\begin_inset Formula $t$
\end_inset

, and is denoted as
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
h(t)=\underset{\Delta t\rightarrow o}{lim}\frac{P(t\leq T<t+\Delta t|T\geq t)}{\Delta t}.\label{eq:f_hazard}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Subject's survival time
\begin_inset Formula $T$
\end_inset

 lies between
\begin_inset Formula $t$
\end_inset

 and
\begin_inset Formula $t+\Delta t$
\end_inset

 provided that survival time
\begin_inset Formula $T$
\end_inset

 is greater or equal than
\begin_inset Formula $t$
\end_inset

.
 Sometimes, the hazard function is called a
\emph on
conditional failure rate.

\emph default
 It is a rate because it is a conditional probability per unit of time
\begin_inset Formula $\Delta t$
\end_inset

.
 As it is not a probability, but a rate, the scale for this ratio is from
 0 to infinity — depends on the measure of time in days, weeks or years.
 When we consider the limit of the expression as the time interval approaches
 zero we basically get the instantenious potential of failing at time
\begin_inset Formula $t$
\end_inset

 per unit time, given survival up to time
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Cumulative hazard function
\series default
: This is basically area under the hazard function that allows to say which
 group has a greater risk.
\end_layout

\begin_layout Standard

\series bold
The relationsip between the two
\series default
: There is a clear relationship between the survival function
\begin_inset Formula $S(t)$
\end_inset

 and the hazard function
\begin_inset Formula $h(t)$
\end_inset

 – if we know one, we can determine the other.
 The relationships are the following:
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
S(t)=exp\left[-\intop_{0}^{t}h(u)du\right]\label{eq:haz_to_surv}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation ...
 tells that the survival function
\begin_inset Formula $S(t)$
\end_inset

 is equal to the exponential of the negative integral of the hazard function
 from zero to
\begin_inset Formula $t$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula
\begin{equation}
h(t)=-\left[\frac{dS(t)/dt}{S(t)}\right]\label{eq:surv_to_haz}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Equation ...
 tells us that the hazard function is equal to the negative derivative of
 the survival function
\begin_inset Formula $S(t)$
\end_inset

 with respect to
\begin_inset Formula $t$
\end_inset

 divided by
\begin_inset Formula $S(t).$
\end_inset


\end_layout

\begin_layout Standard
Considering the facts that survival function describes the probability of
 patient surviving to a given point of time
\begin_inset Formula $t$
\end_inset

 and hazard function shows us the probability of person dying at any given
 point of time
\begin_inset Formula $t$
\end_inset

, we can say that they provide complementary information about survival
 and risk over time.
 Of the two discussed functions the survival function is used much more
 often as it is more appealing in the context of survival analysis and in
 the practical part of this paper(bachelor project/thesis) I am going to
 estimate exactly the survival function.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{figure}
\end_layout

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\begin_layout Plain Layout


\backslash
includegraphics[width=0.9
\backslash
textwidth]{Images/survival_and_hazard_funcs}
\end_layout

\begin_layout Plain Layout


\backslash
caption{(Don't forget to provide example images both for the survival and
 hazard functions for some random patient from the dataset)}
\end_layout

\begin_layout Plain Layout


\backslash
label{fig2}
\end_layout

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\begin_layout Plain Layout


\backslash
end{figure}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Take a look at figure
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig2}
\end_layout

\end_inset

.

\emph on
a)
\emph default
 shows us a graph of the estimated survival function and b) shows us a graph
 of the estimated hazard function for a random patient from the dataset
 used.
 As we can see the survival function is declining over time, while the hazard
 function increases.
\end_layout

\begin_layout Standard

\series bold
Kaplan-Meier Survival Curves
\series default
:
\end_layout

\begin_layout Standard
it is one of the ways to create a survival function.

\end_layout

\begin_layout Standard
non-parametric - does not take covariates into account.
\end_layout

\begin_layout Standard
\begin_inset Quotes sld
\end_inset

The main assumption of the Kaplan-Meier estimator is that censored data
 has the same probability of survival as uncensored data.

\begin_inset Quotes sld
\end_inset


\end_layout

\begin_layout Standard

\series bold
Log-Rank Test
\series default
: It is a way to compare two survival functions.
 Often used in studies, where there is a target group and a placebo (control)
 group to assess the efficacy of the studied thing by comparing the survival
 curves of the two groups.
\end_layout

\begin_layout Subsection
Performance Metrics
\end_layout

\begin_layout Standard
documentation scikit-survival
\end_layout

\begin_layout Standard
c_index: Begins to be biased at high levels of censoring
\end_layout

\begin_layout Standard
Uno's c_index: Handles high levels of censoring (provide a number) very
 well
\end_layout

\begin_layout Standard
Time-dependent Area under the ROC:
\end_layout

\begin_layout Standard
Time-dependent Brier Score:
\end_layout

\begin_layout Subsection
Survival Gradient Boosting
\end_layout

\begin_layout Standard
mention it is an ensemble model and the way it is implemented in the scikit-surv
ival.
\end_layout

\begin_layout Standard
—
\end_layout

\begin_layout Standard
Survival analysis methods
\end_layout

\begin_layout Standard
- CoxPH
\end_layout

\begin_layout Standard
- Regularized Cox
\end_layout

\begin_layout Subsection
Cox Proportional hazards method
\end_layout

\begin_layout Standard
Survival Trees
\end_layout

\begin_layout Subsection
Random Survival Forests
\end_layout

\begin_layout Standard
useless with large amounts of data
\end_layout

\begin_layout Section
Deep Learning
\end_layout

\begin_layout Standard
neuron
\end_layout

\begin_layout Standard
neuron's parts
\end_layout

\begin_layout Standard
neuron activation function
\end_layout

\begin_layout Standard
layers
\end_layout

\begin_layout Standard
dropout
\end_layout

\begin_layout Standard
backpropagation
\end_layout

\begin_layout Subsection
gradient descent
\end_layout

\begin_layout Standard
stochastic gradient descent
\end_layout

\begin_layout Standard
feed forward
\end_layout

\begin_layout Standard
different architectures
\end_layout

\begin_layout Standard
transformers
\end_layout

\begin_layout Standard
convolutional neural networks
\end_layout

\begin_layout Standard
natural language processing
\end_layout

\begin_layout Standard
neural networks for survival estimation
\end_layout

\begin_layout Standard
survival neural networks
\end_layout

\begin_layout Standard
deepSurv
\end_layout

\begin_layout Section
Machine learning workflow
\end_layout

\begin_layout Standard
0.
 define problem type and what objectives you want to achieve.
\end_layout

\begin_layout Standard
1.
 gather data
\end_layout

\begin_layout Standard
2.
 Analyse data
\end_layout

\begin_layout Standard
3.
 Create dataset: feature engineering
\end_layout

\begin_layout Standard
4.
 Train a lot of models for the given problem type on the reduced dataset,
 shortlist a couple of the most promising ones.
\end_layout

\begin_layout Standard
5.
 Fine tune the most promising ones using the validation set until no signiffican
t improvements are seen.

\end_layout

\begin_layout Standard
6.
 Asses on the test set
\end_layout

\begin_layout Standard
7.
 Run on the whole dataset and/or modify the dataset
\end_layout

\begin_layout Standard
7.
 Deploy
\end_layout

\begin_layout Section
Challenges of machine learning
\end_layout

\begin_layout Standard
Poor quality data
\end_layout

\begin_layout Standard
Over
\end_layout

\begin_layout Section
Overview of Machine Learning Libraries
\end_layout

\begin_layout Subsection
Sci-kit learn
\end_layout

\begin_layout Standard
- pros
\end_layout

\begin_layout Standard
- cons
\end_layout

\begin_layout Standard
- where it is used
\end_layout

\begin_layout Subsection
Keras
\end_layout

\begin_layout Subsection
Tensorflow
\end_layout

\begin_layout Standard
- pros
\end_layout

\begin_layout Standard
- cons
\end_layout

\begin_layout Standard
- where it can be used
\end_layout

\begin_layout Standard
it is more corporate driven
\end_layout

\begin_layout Subsection
PyTorch
\end_layout

\begin_layout Standard
- pros
\end_layout

\begin_layout Standard
- cons
\end_layout

\begin_layout Standard
- where it can be used
\end_layout

\begin_layout Standard
it is more research driven
\end_layout

\begin_layout Subsection
Comparison
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Chapter
Data Preparation and Analysis
\end_layout

\begin_layout Section
Exploratory Data Analysis
\end_layout

\begin_layout Standard
- how many transplantations we have
\end_layout

\begin_layout Standard
- median age
\end_layout

\begin_layout Standard
- age distribution
\end_layout

\begin_layout Standard
- survival time distribution
\end_layout

\begin_layout Standard
- box plots
\end_layout

\begin_layout Subsection
...
\end_layout

\begin_layout Section
Data preparation
\end_layout

\begin_layout Subsection
Feature selection
\end_layout

\begin_layout Subsection
Handling Numerical Values
\end_layout

\begin_layout Subsection
Handling Categorical Values
\end_layout

\begin_layout Chapter
Machine Learning Model
\end_layout

\begin_layout Section
Problem Formulation
\end_layout

\begin_layout Standard
We can formulate the problem of predicting the survival time after a successful
 kidney transplant in three ways: as a regression, classification and survival
 analysis.
\end_layout

\begin_layout Standard
We are predicting survival time, some number, and the obvious choice would
 be to use some
\emph on
regression
\emph default
 model, but it is not the best choice for the following reasons:
\end_layout

\begin_layout Standard
1.

\series bold
The censored dataset
\series default
.
 The dataset has a high level of censoring – (
\series bold
provide a number
\series default
).
 The dataset contains a number of days survived and whether the person is
 alive or not.
 If we were to consider people who are still alive with the ones who died
 we would introduce too much noise to the model, making it highly inaccurate.
 It is impossible to predict a number of days survived with regression methods
 based on dataset where both alive and died patients are mixed.

\end_layout

\begin_layout Standard
2.

\series bold
Censoring removal would produce bias
\series default

\series bold
and signifficantly reduce the dataset
\series default
.
 We could get rid of all censored instances, but that would reduce the dataset
 from
\series bold
500 000
\series default
 to about
\series bold
120 000
\series default
 instances and introduce signifficant bias: we would only consider people
 who passed away, and most of them have done so before the indroduction
 of modern techniques of treating the rejection.
 So the model created from such a dataset would be highly biased and therefore
 inaccurate.
\end_layout

\begin_layout Standard
3.

\series bold
Regression predicts only one single number.

\series default
 This is a problem, because on longer time frames there are too much factors
 that we can't account for this would produce incorrect predictions.
\end_layout

\begin_layout Standard
Another way of formulating the problem is
\emph on
classification
\emph default
.
 We can theoretically divide the dataset into groups:
\begin_inset Quotes sld
\end_inset

less than one year
\begin_inset Quotes srd
\end_inset

,
\begin_inset Quotes sld
\end_inset

one to five years
\begin_inset Quotes srd
\end_inset

,
\begin_inset Quotes sld
\end_inset

five and more
\begin_inset Quotes srd
\end_inset

, or even more groups, as it was done by ....
 et al., and train some classifier based on them.
 And again we would face problems of censoring and bias mentioned above.
 So the classification is also not the best option.
\end_layout

\begin_layout Standard
Superior way of problem formulation is in terms of
\emph on
survival analysis
\emph default
.
 Survival analysis methods handle censoring and provide better form of predictio
n: survival function or hazard function, i.e.
 survival probability or failing rate at each moment in time.
 (
\series bold
add more reasons why it is superior way)
\end_layout

\begin_layout Section
Model selection
\end_layout

\begin_layout Standard
bottleneck in a number of instances each model can train on.
 +
\end_layout

\begin_layout Standard
different groups of people have different expected life expectancy -> separate
 models for different groups of people for the result maximization.
\end_layout

\begin_layout Standard
–
\end_layout

\begin_layout Standard
Create an ensemble of a couple of learners.
\end_layout

\begin_layout Section
Model comparison
\end_layout

\begin_layout Section
Final Model
\end_layout

\begin_layout Section
Scoring algorithm
\end_layout

\begin_layout Standard
the cumulative hazard suits the place of score very well
\end_layout

\begin_layout Section
Limitations
\end_layout

\begin_layout Section
Further work
\end_layout

\begin_layout Chapter
Applications
\end_layout

\begin_layout Standard
txmatching is something totally different, so it was decided to create separate
 application for accesing the model.
\end_layout

\begin_layout Section
Existing Solutions
\end_layout

\begin_layout Subsection
Txmatching
\end_layout

\begin_layout Section
KidneyLife
\end_layout

\begin_layout Subsection
Frontend
\end_layout

\begin_layout Subsection
Backend
\end_layout

\begin_layout Subsection
MLOps
\end_layout

\begin_layout Chapter*
Conclusion
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
pagestyle{plain}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addcontentsline{toc}{chapter}{Conclusion}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Text of the conclusion\SpecialChar ldots

\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset

Knechtle, S.
 J., Marson, L.
 P., & Morris, P.
 (2019).
 Kidney transplantation - principles and practice: Expert consult - online
 and print (8th ed.).
 Elsevier - Health Sciences Division
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-2"
literal "false"

\end_inset

Nobel prize in physiology or medicine (2022) Our Scientists.
 Available at: https://www.rockefeller.edu/our-scientists/alexis-carrel/2565-nobel
-prize/ (Accessed: February 6, 2023).

\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-3"
literal "false"

\end_inset

Barker, C.
 F., & Markmann, J.
 F.
 (2013).
 Historical Overview of Transplantation.
 Cold Spring Harbor Perspectives in Medicine, 3(4).
 https://doi.org/10.1101/cshperspect.a014977
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-4"
literal "false"

\end_inset

Matevossian, Edouard, et al.
 "Surgeon Yurii Voronoy (1895-1961)-a pioneer in the history of clinical
 transplantation: in memoriam at the 75th anniversary of the first human
 kidney transplantation." Transplant International 22.12 (2009): 1132.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-5"
literal "false"

\end_inset

PUNT, Jenni et al.
 Kuby immunology.
 Eight.
 vyd.
 New York: Macmillan Education, 2019.
 ISBN 9781319114701;1319114709;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-6"
literal "false"

\end_inset

ABBAS, Abul K., Andrew H.
 LICHTMAN a Shiv PILLAI.
 Basic immunology: functions and disorders of the immune system.
 Sixth.
 vyd.
 Philadelphia: Elsevier, 2020.
 ISBN 9780323549431;0323549438;
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-7"
literal "false"

\end_inset

NCI Dictionary of Cancer terms (no date) National Cancer Institute.
 Available at: https://www.cancer.gov/publications/dictionaries/cancer-terms/def/a
bo-blood-group-system (Accessed: March 6, 2023).

\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-8"
literal "false"

\end_inset

Dean L.
 Blood Groups and Red Cell Antigens [Internet].
 Bethesda (MD): National Center for Biotechnology Information (US); 2005.
 Chapter 2, Blood group antigens are surface markers on the red blood cell
 membrane.
 Available from: https://www.ncbi.nlm.nih.gov/books/NBK2264/
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-9"
literal "false"

\end_inset

Aurélien Géron.
 Hands-on Machine Learning with Scikit-Learn and TensorFlow Concepts, Tools,
 and Techniques to Build Intelligent Systems.
 O’Reilly Media, Inc., Sept.
 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-10"
literal "false"

\end_inset

Andriy Burkov.
 THE HUNDRED-PAGE MACHINE LEARNING BOOK.
 Andriy Burkov, 2019.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-11"
literal "false"

\end_inset

Makary M A, Daniel M.
 Medical error—the third leading cause of death in the US BMJ 2016; 353
 :i2139 doi:10.1136/bmj.i2139
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-12"
literal "false"

\end_inset

Bruce, P., Bruce, A., & Gedeck, P.
 (2020).
 Practical statistics for data scientists: 50+ Essential concepts using
 R and python (2nd ed.).
 O’Reilly Media.
 p.
 141
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-13"
literal "false"

\end_inset

Kleinbaum, D.
 G., & Klein, M.
 (2011).
 Survival analysis: A self-learning text, third edition (3rd ed.).
 Springer.
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
key "key-1"

\end_inset


\end_layout

\end_body
\end_document
